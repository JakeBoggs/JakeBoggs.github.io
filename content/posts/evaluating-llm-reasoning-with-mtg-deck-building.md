---
title: "Evaluating Reasoning in LLMs Through MTG Deck Building"
date: "2025-05-09"
draft: false
---

<div style="background: #e0f7fa; color: #006064; padding: 12px 18px; border-radius: 6px; margin-bottom: 18px; font-size: 1.08em; font-weight: 500;">
  <strong>Update (2025-08-05):</strong> Kimi K2, GPT OSS 120B (low), and GPT OSS 120B (high) added.
</div>
<details style="margin-bottom: 18px;">
  <summary style="cursor: pointer; font-weight: 500;">View Older Updates</summary>
  <div style="padding-top: 10px;">
    <div style="background: #e0f7fa; color: #006064; padding: 12px 18px; border-radius: 6px; margin-bottom: 18px; font-size: 1.08em; font-weight: 500;">
      <strong>Update (2025-07-13):</strong> Grok 4, Grok 3, Gemini 2.5 Flash, Claude Sonnet 4 (thinking), and Command A added.
    </div>
    <div style="background: #e0f7fa; color: #006064; padding: 12px 18px; border-radius: 6px; margin-bottom: 18px; font-size: 1.08em; font-weight: 500;">
      <strong>Update (2025-06-11):</strong> o3 (high) added after API cost reduction.
    </div>
    <div style="background: #e0f7fa; color: #006064; padding: 12px 18px; border-radius: 6px; margin-bottom: 18px; font-size: 1.08em; font-weight: 500;">
      <strong>Update (2025-06-06):</strong> Deepseek R1 05-28 and Gemini 2.5 Pro 06-05 added.
    </div>
    <div style="background: #e0f7fa; color: #006064; padding: 12px 18px; border-radius: 6px; margin-bottom: 18px; font-size: 1.08em; font-weight: 500;">
      <strong>Update (2025-05-22):</strong> Claude Sonnet 4 and Opus 4 added.
    </div>
    <div style="background: #e0f7fa; color: #006064; padding: 12px 18px; border-radius: 6px; margin-bottom: 0; font-size: 1.08em; font-weight: 500;">
      <strong>Update (2025-05-14):</strong> Human Baseline, Gemini 2.5 Pro 03-25, Gemini 1.5 Flash, Deepseek V3 03-24, Qwen3 30B 3A added.
    </div>
  </div>
</details>

## Introduction

Evaluating the advanced reasoning capabilities of Large Language Models (LLMs) requires specialized benchmarks that move beyond surface-level NLP tasks. Building upon my obsession applying AI models to my favorite card game, I've created ManaBench, a benchmark designed to probe an LLM's capacity for reasoning using the collectible card game Magic: The Gathering (MTG) as a proxy. MTG, with its intricate interactions and deep strategic layer, serves as an ideal micro-world to test an LLM's ability to process extensive contextual information, identify sophisticated patterns, and make judgments that align with expert human strategic choices. This post provides a technical overview of the benchmark's construction and the methodology used for evaluating LLMs, not as a test of MTG-specific card knowledge, but as a measure of their broader reasoning and problem-solving faculties when faced with a constrained, strategic challenge.

## The Deck Completion Task

The core task in ManaBench is as follows: Given a 59-card main deck from a specific MTG constructed format (e.g., Modern, Legacy) - a deck originally constructed by a human player and sourced from tournament results - the LLM must choose the most suitable 60th card from a list of six options. One of these options is the "golden" card - the card that was originally in that slot in the human-designed decklist - while the other five are plausible alternatives generated by [Manamorphosis](https://github.com/JakeBoggs/Manamorphosis), a diffusion model I trained specifically for completing MTG decks.

This task is non-trivial for an LLM because it demands more than just factual recall about individual cards; it requires:
*   **Strategic Coherence Evaluation:** The chosen card must align with the deck's overall strategy (e.g., aggro, control, combo), a judgment that requires understanding the interplay of the existing 59 cards.
*   **System-Wide Optimization:** The card should fit the deck's mana curve and resource development plan, demonstrating an understanding of resource management within the game system.
*   **Complex Interaction Analysis (Card Synergies):** Effective MTG play relies heavily on card interactions. The LLM needs to identify cards that synergize well with the existing 59 cards, showcasing an ability to reason about emergent properties of combined elements.
*   **Contextual Awareness (Format Knowledge):** Different MTG formats have distinct card pools and power levels. The choice must be legal and relevant within the specified format, testing the LLM's ability to operate within defined constraints.
*   **Discernment Against Plausible Alternatives:** The alternatives are not entire random but are generated by a model trained for the task, making them potentially attractive but incorrect choices. Successfully identifying the golden card requires fine-grained distinction based on strategic fit.

## Benchmark Leaderboard

<div style="width: 90%; margin: 20px auto;">
    <canvas id="leaderboardChart"></canvas>
</div>

<script src="https://cdn.jsdelivr.net/npm/chart.js@3.9.1/dist/chart.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2.2.0/dist/chartjs-plugin-datalabels.min.js"></script>
<script>
    document.addEventListener('DOMContentLoaded', () => {
        const canvasElement = document.getElementById('leaderboardChart'); // Get the canvas element itself
        const ctxLeaderboard = canvasElement.getContext('2d');
        
        const labels = [
            'Human Baseline',                       // 68
            'o3 (high)',                            // 65
            'o3 (low)',                             // 63
            'Gemini 2.5 Pro 06-05',                 // 57.5
            'Gemini 2.5 Pro 03-25',                 // 53
            'Grok 4',                               // 52.5
            'Gemini 2.5 Flash',                     // 50
            'Claude 3.7 Sonnet (no thinking)',      // 49.5
            'o4 Mini (low)',                        // 45
            'Claude Sonnet 4 (thinking)',           // 44
            'Deepseek R1',                          // 43.5
            'Deepseek R1 05-28',                    // 43
            'Grok 3',                               // 41.5
            'GPT-4o 08-06',                         // 41
            'Kimi K2',                              // 41
            'Claude Opus 4 (no thinking)',          // 40.5
            'GPT OSS 120B (high)',                  // 40
            'Claude Sonnet 4 (no thinking)',        // 38
            'Deepseek V3 03-24',                    // 37.5
            'Qwen3 235B A22B (thinking)',           // 37
            'Grok 3 Mini (low)',                    // 37
            'GPT OSS 120B (low)',                   // 35.5
            'Gemini 2.0 Flash',                     // 35
            'Mistral 3 Medium',                     // 31.5
            'Qwen3 30B A3B (thinking)',             // 28.5
            'Llama 4 Maverick',                     // 26.5
            'Command A',                            // 25
            'Gemini 1.5 Flash',                     // 22.5
            'GPT-4.1 Nano',                         // 19.5
            'Llama 3.3 70B',                        // 19.5
            'Random Guessing'                       // 16.67
        ];

        // Calculate dynamic height for the chart
        const numBars = labels.length;
        const pixelsPerBar = 15; // Adjust this value as needed for spacing
        const chartTitleAndXAxisHeight = 100; // Estimated height for title, X-axis, padding
        const newCanvasHeight = (numBars * pixelsPerBar) + chartTitleAndXAxisHeight;
        
        canvasElement.style.height = newCanvasHeight + 'px';

        const dataValues = [
            68, 65, 63, 57.5, 53, 52.5, 50, 49.5, 45, 44, 43.5, 43, 41.5, 41, 41, 40.5, 40, 38, 37.5, 37, 37, 35.5, 35, 31.5, 28.5, 26.5, 25, 22.5, 19.5, 19.5, 16.67
        ];

        const backgroundColors = [
            'rgba(34, 139, 34, 0.8)',  // Human Baseline
            'rgba(70, 130, 220, 0.8)', // o3 (high)
            'rgba(75, 192, 192, 0.8)', // o3 (low)
            'rgba(0, 220, 220, 0.8)', // Gemini 2.5 Pro 06-05
            'rgba(0, 200, 255, 0.8)', // Gemini 2.5 Pro 03-25
            'rgba(220, 20, 60, 0.8)',  // Grok 4
            'rgba(50, 205, 50, 0.8)',  // Gemini 2.5 Flash
            'rgba(54, 162, 235, 0.8)', // Claude 3.7 Sonnet (no thinking)
            'rgba(255, 206, 86, 0.8)', // o4 Mini (low)
            'rgba(138, 43, 226, 0.8)', // Claude Sonnet 4 (thinking)
            'rgba(255, 99, 132, 0.8)',  // Deepseek R1
            'rgba(255, 99, 100, 0.8)',  // Deepseek R1 05-28
            'rgba(255, 140, 0, 0.8)',  // Grok 3
            'rgba(153, 102, 255, 0.8)',// GPT-4o 08-06
            'rgba(0, 100, 0, 0.8)',    // Kimi K2
            'rgba(205, 133, 63, 0.8)', // Claude Opus 4 (no thinking)
            'rgba(255, 215, 0, 0.8)',  // GPT OSS 120B (high)
            'rgba(188, 143, 143, 0.8)',// Claude Sonnet 4 (no thinking)
            'rgba(128, 128, 0, 0.8)',   // Deepseek V3 03-24
            'rgba(255, 159, 64, 0.8)', // Qwen3 235B A22B (thinking)
            'rgba(101, 143, 74, 0.8)',  // Grok 3 Mini (low)
            'rgba(255, 192, 203, 0.8)',// GPT OSS 120B (low)
            'rgba(210, 105, 30, 0.8)',  // Gemini 2.0 Flash
            'rgba(0, 128, 128, 0.8)',   // Mistral 3 Medium
            'rgba(0, 0, 205, 0.8)',     // Qwen3 30B A3B (thinking)
            'rgba(165, 42, 42, 0.8)',   // Llama 4 Maverick
            'rgba(139, 69, 19, 0.8)',   // Command A
            'rgba(255, 105, 180, 0.8)', // Gemini 1.5 Flash
            'rgba(70, 130, 180, 0.8)',  // GPT-4.1 Nano
            'rgba(128, 0, 128, 0.8)',   // Llama 3.3 70B
            'rgba(150, 150, 150, 0.8)'  // Random Guessing
        ];

        const borderColors = backgroundColors.map(color => color.replace('0.8', '1'));

        new Chart(ctxLeaderboard, {
            type: 'bar',
            data: {
                labels: labels,
                datasets: [{
                    label: 'Accuracy',
                    data: dataValues,
                    backgroundColor: backgroundColors,
                    borderColor: borderColors,
                    borderWidth: 1
                }]
            },
            options: {
                indexAxis: 'y', // Horizontal bar chart
                responsive: true,
                maintainAspectRatio: false, // Key change: allow height to be independent of width
                plugins: {
                    legend: {
                        display: false // Hide legend as there's only one dataset
                    },
                    title: {
                        display: true,
                        text: 'ManaBench LLM Leaderboard (Accuracy %)',
                        font: {
                            size: 18
                        }
                    },
                    tooltip: {
                        callbacks: {
                            label: function(context) {
                                let label = context.dataset.label || '';
                                if (label) {
                                    label += ': ';
                                }
                                if (context.parsed.x !== null) {
                                    label += context.parsed.x + '%';
                                }
                                return label;
                            }
                        }
                    }
                },
                scales: {
                    x: {
                        beginAtZero: true,
                        title: {
                            display: true,
                            text: 'Accuracy (%)'
                        }
                    },
                    y: {
                        ticks: {
                            autoSkip: false // Ensure all labels are shown
                        }
                    }
                }
            }
        });
    });
</script>

The human baseline has an admittedly small sample size (just myself, it's difficult to find skilled MTG players who want to sit through a 200 question test), but still provides a valuable reference. As the creator of the benchmark and a player of the game for 10+ years, I scored 68% agreement with the other human deck builders. This could be a skill issue on my end, but I think it is more likely due to the somewhat subjective nature of question, along with the presentation format. To provide a fair comparison, I made a script that presents that questions to me exactly as they are shown to the LLMs, but I think I could do better using a deck editor. Despite my less than perfect score, I still beat most of the models by a wide margin. Among LLMs, o3 (high) comes closest with 65% accuracy, followed by o3 (low) at 63%, and Gemini 2.5 Pro 06-05 at 57.5%. This aligns with qualitative assessments and other benchmarks where these models often excel in real-world tasks. Their success here suggests that ManaBench is effectively capturing a similar type of reasoning aptitude.

The results also highlight a discernible gap in performance between the leading American models (o3, Gemini 2.5 Pro, Claude) and prominent Chinese models like Deepseek R1 (43.5%) and Qwen3 235B A22B (37%). While these models are undoubtedly powerful, their performance on ManaBench suggests that their reasoning capabilities may not be as developed as some of their US counterparts. This observation underscores the utility of specialized benchmarks like ManaBench in revealing subtle but significant differences in model capabilities that might be obscured by broader, more generalized benchmarks.

## Accuracy vs. Cost

This chart visualizes the model cost vs performance. The x-axis represents a blended cost per million tokens, calculated with a 3:1 weighting of input to output costs. The y-axis shows the accuracy on ManaBench. Models on the red line represent the Pareto frontier.

<div style="width: 90%; margin: 20px auto; min-height: 600px;">
    <canvas id="paretoChart"></canvas>
</div>

<script src="https://cdn.jsdelivr.net/npm/chart.js@3.9.1/dist/chart.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2.2.0/dist/chartjs-plugin-datalabels.min.js"></script>
<script>
    document.addEventListener('DOMContentLoaded', () => {
        const ctxPareto = document.getElementById('paretoChart').getContext('2d');
        
        const modelData = [
            { name: 'o3 (high)', accuracy: 65, inputCost: 2.00, outputCost: 8.00, color: 'rgba(70, 130, 220, 0.8)' },
            { name: 'o3 (low)', accuracy: 63, inputCost: 2.00, outputCost: 8.00, color: 'rgba(75, 192, 192, 0.8)' },
            { name: 'Gemini 2.5 Pro 06-05', accuracy: 57.5, inputCost: 2.50, outputCost: 15.00, color: 'rgba(0, 220, 220, 0.8)' },
            { name: 'Gemini 2.5 Pro 03-25', accuracy: 53, inputCost: 2.50, outputCost: 15.00, color: 'rgba(0, 200, 255, 0.8)' },
            { name: 'Grok 4', accuracy: 52.5, inputCost: 3.00, outputCost: 15.00, color: 'rgba(220, 20, 60, 0.8)' },
            { name: 'Gemini 2.5 Flash', accuracy: 50, inputCost: 0.30, outputCost: 2.50, color: 'rgba(50, 205, 50, 0.8)' },
            { name: 'Claude 3.7 Sonnet (no thinking)', accuracy: 49.5, inputCost: 3.00, outputCost: 15.00, color: 'rgba(54, 162, 235, 0.8)' },
            { name: 'o4 Mini (low)', accuracy: 45, inputCost: 1.10, outputCost: 4.40, color: 'rgba(255, 206, 86, 0.8)' },
            { name: 'Claude Sonnet 4 (thinking)', accuracy: 44, inputCost: 3.00, outputCost: 15.00, color: 'rgba(138, 43, 226, 0.8)' },
            { name: 'Deepseek R1', accuracy: 43.5, inputCost: 0.45, outputCost: 2.15, color: 'rgba(255, 99, 132, 0.8)' },
            { name: 'Deepseek R1 05-28', accuracy: 43, inputCost: 0.50, outputCost: 2.15, color: 'rgba(255, 99, 100, 0.8)' },
            { name: 'Grok 3', accuracy: 41.5, inputCost: 3.00, outputCost: 15.00, color: 'rgba(255, 140, 0, 0.8)' },
            { name: 'GPT-4o 08-06', accuracy: 41, inputCost: 2.50, outputCost: 10.00, color: 'rgba(153, 102, 255, 0.8)' },
            { name: 'Kimi K2', accuracy: 41, inputCost: 0.55, outputCost: 2.20, color: 'rgba(0, 100, 0, 0.8)' },
            { name: 'Claude Opus 4 (no thinking)', accuracy: 40.5, inputCost: 15.00, outputCost: 75.00, color: 'rgba(205, 133, 63, 0.8)' },
            { name: 'GPT OSS 120B (high)', accuracy: 40, inputCost: 0.15, outputCost: 0.6, color: 'rgba(255, 215, 0, 0.8)' },
            { name: 'Claude Sonnet 4 (no thinking)', accuracy: 38, inputCost: 3.00, outputCost: 15.00, color: 'rgba(188, 143, 143, 0.8)' },
            { name: 'Deepseek V3 03-24', accuracy: 37.5, inputCost: 0.27, outputCost: 1.10, color: 'rgba(128, 128, 0, 0.8)' },
            { name: 'Qwen3 235B A22B (thinking)', accuracy: 37, inputCost: 0.13, outputCost: 0.60, color: 'rgba(255, 159, 64, 0.8)' },
            { name: 'Grok 3 Mini (low)', accuracy: 37, inputCost: 0.30, outputCost: 0.50, color: 'rgba(101, 143, 74, 0.8)' },
            { name: 'GPT OSS 120B (low)', accuracy: 35.5, inputCost: 0.15, outputCost: 0.6, color: 'rgba(255, 192, 203, 0.8)' },
            { name: 'Gemini 2.0 Flash', accuracy: 35, inputCost: 0.10, outputCost: 0.40, color: 'rgba(210, 105, 30, 0.8)' },
            { name: 'Mistral 3 Medium', accuracy: 31.5, inputCost: 0.40, outputCost: 2.00, color: 'rgba(0, 128, 128, 0.8)' },
            { name: 'Qwen3 30B A3B (thinking)', accuracy: 28.5, inputCost: 0.08, outputCost: 0.29, color: 'rgba(0, 0, 205, 0.8)' },
            { name: 'Llama 4 Maverick', accuracy: 26.5, inputCost: 0.15, outputCost: 0.60, color: 'rgba(165, 42, 42, 0.8)' },
            { name: 'Command A', accuracy: 25, inputCost: 2.50, outputCost: 10.00, color: 'rgba(139, 69, 19, 0.8)' },
            { name: 'Gemini 1.5 Flash', accuracy: 22.5, inputCost: 0.075, outputCost: 0.30, color: 'rgba(255, 105, 180, 0.8)' },
            { name: 'GPT-4.1 Nano', accuracy: 19.5, inputCost: 0.10, outputCost: 0.40, color: 'rgba(70, 130, 180, 0.8)' },
            { name: 'Llama 3.3 70B', accuracy: 19.5, inputCost: 0.038, outputCost: 0.12, color: 'rgba(128, 0, 128, 0.8)' },
        ];

        const scatterDataPoints = modelData.map(model => {
            const blendedCost = (model.inputCost * 3 + model.outputCost * 1) / 4;
            return {
                x: blendedCost,
                y: model.accuracy,
                label: model.name,
                color: model.color
            };
        });

        // Identify Pareto Frontier
        scatterDataPoints.sort((a, b) => b.y - a.y || a.x - b.x); // Sort by accuracy (desc), then cost (asc)
        
        const paretoFrontier = [];
        let maxAccuracy = -1;
        let minCostForMaxAccuracy = Infinity;

        for (const point of scatterDataPoints) {
            // Start with the highest accuracy point
            if (paretoFrontier.length === 0) {
                paretoFrontier.push(point);
                minCostForMaxAccuracy = point.x;
                continue;
            }

            // A point is on the frontier if it has a lower cost than the current best point for a given or higher accuracy
            // Since we sorted by accuracy desc, we only need to check the cost
            if (point.x < minCostForMaxAccuracy) {
                paretoFrontier.push(point);
                minCostForMaxAccuracy = point.x;
            }
        }
        paretoFrontier.sort((a, b) => a.x - b.x); // Sort by cost for drawing the line

        const costs = scatterDataPoints.map(p => p.x);
        const minCost = Math.min(...costs);
        const maxCost = Math.max(...costs);
        const midCost = Math.sqrt(minCost * maxCost); // Geometric mean for log scale
        const scores = scatterDataPoints.map(p => p.y);
        const minScore = Math.min(...scores);
        const maxScore = Math.max(...scores);

        new Chart(ctxPareto, {
            type: 'scatter',
            plugins: [ChartDataLabels],
            data: {
                datasets: [{
                    label: 'Models',
                    data: scatterDataPoints,
                    backgroundColor: scatterDataPoints.map(p => p.color),
                    pointRadius: 6,
                    pointHoverRadius: 8,
                    datalabels: {
                        formatter: (value, context) => {
                            const point = context.dataset.data[context.dataIndex];
                            if (context.active) {
                                return `${point.label}: (${point.x.toFixed(2)}, ${point.y}%)`;
                            }
                            return point.label;
                        },
                        font: function(context) {
                            return {
                                size: 9,
                                weight: context.active ? 'bold' : '500'
                            };
                        },
                        backgroundColor: function(context) {
                            return context.active ? context.dataset.backgroundColor[context.dataIndex].replace('0.8', '1') : null;
                        },
                        borderColor: function(context) {
                            return context.active ? 'white' : null;
                        },
                        borderRadius: 4,
                        borderWidth: 1,
                        color: function(context) {
                            return context.active ? 'white' : '#333';
                        },
                        padding: 4,
                        align: (context) => context.dataset.data[context.dataIndex].x < midCost ? 'right' : 'left',
                        offset: 8,
                        anchor: 'center'
                    }
                },
                {
                    label: 'Pareto Frontier',
                    data: paretoFrontier,
                    type: 'line',
                    borderColor: 'rgba(255, 0, 0, 0.7)',
                    borderWidth: 2,
                    fill: false,
                    showLine: true,
                    pointRadius: 0,
                    tension: 0.1,
                    datalabels: {
                        display: false
                    }
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                plugins: {
                    title: {
                        display: true,
                        text: 'ManaBench Pareto Frontier (Accuracy vs. Blended Cost)',
                        font: { size: 18 }
                    },
                    tooltip: {
                        enabled: false
                    },
                    legend: {
                        display: false
                    }
                },
                scales: {
                    x: {
                        type: 'logarithmic',
                        min: minCost * 0.9,
                        max: maxCost * 1.1,
                        title: {
                            display: true,
                            text: 'Blended Cost ($ per 1M Tokens, 3:1 Input/Output Ratio) - Log Scale'
                        }
                    },
                    y: {
                        min: Math.max(minScore - 4, 0),
                        max: Math.min(maxScore + 4, 100),
                        title: {
                            display: true,
                            text: 'Accuracy (%)'
                        }
                    }
                }
            }
        });
    });
</script>

## Benchmark Construction Methodology

The creation of ManaBench involves several stages, from curating source decks to generating challenging multiple-choice questions. The process is designed to be rigorous and reproducible, with a focus on capturing instances of human expert judgment as reflected in competitive deck choices.

### Source Data & Deck Curation

The foundation of the benchmark is a large corpus of human-constructed MTG decklists scraped from [MTGTop8](https://www.mtgtop8.com/) (a public database of MTG tournament results and decklists). The curation process involves:

1.  **Format Filtering:** Decks are categorized by major constructed formats: Modern, Pioneer, Legacy, and Vintage. The benchmark currently focuses on these eternal formats. Standard is excluded due to its rotating nature, which does not align with the goal of measuring reasoning abilities and minimizing the impact of knowledge cutoff dates, thereby keeping the focus on reasoning skills rather than rapidly changing format knowledge.
2.  **Strict Validation:** Only decks containing exactly 60 main deck cards and 15 sideboard cards are considered. This ensures that they obey conventional deck-building wisdom and that the creator carefully considered the metagame when constructing the deck.
3.  **Format Legality Check:** Using the card database from [MTGJSON](https://mtgjson.com/), every card in a potential source deck is verified for legality within its designated format. This step is crucial for ensuring that the decks and subsequent questions are valid within the game's rules.
4.  **Sampling:** From the pool of validated decks, 25 decks are randomly sampled for each of the four target formats, resulting in a set of 100 unique, curated decklists for benchmark generation. These decks represent successful strategies designed and tested by human players in tournament play.

### Question Generation

For each of the 100 sampled human-constructed decks, two unique questions are generated. This involves selecting a "golden" card to remove and then using the Manamorphosis model to propose alternatives.

1.  **Golden Card Selection:**
    *   The "golden" card represents the correct answer for the deck completion task, reflecting a choice consistent with the original human-designed, tournament-sourced deck.
    *   It is chosen by randomly selecting one *unique card name* present in the original 60-card main deck. For instance, if a deck contains four copies of "Lightning Bolt," "Lightning Bolt" is one possible unique card name that could be selected as the golden card.
    *   To ensure variety in the questions derived from a single deck, the two golden cards selected from the same source deck must be different card names.

2.  **Partial Deck Creation:**
    *   Once a golden card name is selected, one instance of this card is removed from the original 60-card main deck list. This creates the 59-card partial deck that will be presented to the LLM.
    *   For example, if "Island" is chosen as the golden card from a deck containing 10 Islands, the partial deck will contain 9 Islands.

3.  **Generating Plausible Alternatives:**
    *   The five incorrect-but-plausible alternatives are generated using a Manamorphosis, a Transformer-based diffusion model custom-trained trained on a vast corpus of MTG decks. It learns to represent cards as high-dimensional embeddings and understands patterns of card co-occurrence and deck structure.
    *   For benchmark generation, this model takes the 59-card partial deck and, through a reverse diffusion process conditioned on these known cards, predicts embeddings for the missing card. These embeddings are then mapped back to specific card names. This process is designed to generate alternatives that are contextually relevant yet distinct from the golden card.
    *   For a detailed technical explanation of the diffusion model's architecture and training process, please refer to the [Manamorphosis repository](https://github.com/JakeBoggs/Manamorphosis) and the [accompanying blog post](https://boggs.tech/posts/manamorphosis).
    *   This generation process is repeated to obtain 5 unique card names that are different from the chosen golden card and from each other, serving as challenging distractors for the LLM.

4.  **Question Structure (JSON):**
    Each generated question is stored in a JSON object with the following structure:
    ```json
    {
      "id": "question_001",
      "deck": ["Card Name 1", "Card Name 2", "Card Name 3", "... list of 59 card names ..."],
      "golden": "Actual Missing Card Name",
      "alternatives": [
        "Alternative Card Name A",
        "Alternative Card Name B",
        "Alternative Card Name C",
        "Alternative Card Name D",
        "Alternative Card Name E"
      ],
      "format": "modern"
    }
    ```

## Evaluation Protocol

### Prompting Strategy

Careful prompt engineering is employed to provide the LLM with sufficient context to make a reasoned judgment, minimizing the need for memorized MTG card knowledge and emphasizing analytical skill:

1.  **Role Assigment:** The LLM is instructed: `"You are an expert Magic: The Gathering player."`
2.  **Task Definition:** The prompt explains that a decklist from a specific format is missing one card and asked to choose the answer that best completes the deck.
3.  **Decklist Presentation:** The 59-card partial deck is provided. Each card entry includes:
    *   Its count in the partial deck.
    *   Its full name.
    *   Its detailed rules text, including type line, mana cost, power/toughness (for creatures), loyalty (for planeswalkers), and Oracle text. This information is sourced from MTGJSON. *The explicit provision of full rules text for all cards in the deck and choices is a key design element, intended to reduce the task's reliance on the LLM's pre-existing knowledge of specific cards and instead focus on its ability to reason based on the provided information.*
    *   *Example Card Presentation in Prompt:*
        `2x Snapcaster Mage - Creature - Human Wizard - Cost: {1}{U} - P/T: 2/1 - Rules: Flash. When Snapcaster Mage enters the battlefield, target instant or sorcery card in your graveyard gains flashback until end of turn. The flashback cost is equal to its mana cost.`
4.  **Multiple-Choice Options:**
    *   The six choices (the golden card and the five alternatives) are presented, shuffled randomly to avoid positional bias, and labeled A through F.
    *   Each choice is also presented with its full name and detailed rules text (sourced from MTGJSON, same as deck cards).
    *   *Example Choice Presentation in Prompt:*
        `A) Brainstorm - Instant - Cost: {U} - Rules: Draw three cards, then put two cards from your hand on top of your library in any order.`

### Answer Format and Extraction

To standardize evaluation, LLMs are instructed to output their final answer in a specific format:
`"Respond with only the letter of your choice in the format: ANSWER: [LETTER]"`

The evaluation script parses this response using the following logic:
1.  **Primary Method:** Looks for the "ANSWER: [LETTER]" pattern (case-insensitive for "ANSWER:", extracts the letter).
2.  **Fallback 1 (Single Letter):** If the primary method fails, it checks if the entire response is a single letter from A to F (e.g., a response like "C").
3.  **Fallback 2 (Boxed Letter):** If both above fail, it looks for the pattern `$\boxed{LETTER}$` (e.g., "The answer is $\boxed{A}$") using a regular expression.

### Metrics

*   **Accuracy:** The primary metric is the percentage of questions the LLM answers correctly by selecting the golden card.

## Why ManaBench is a Strong Benchmark for Reasoning

The preliminary results and the design of ManaBench highlight several key strengths for evaluating an LLM's reasoning capabilities:

*   **Measures Alignment with Human Expert Judgment:** The "golden" answers are derived from decks designed by human players and sourced from MTGTop8, a database of tournament decklists. Success on this benchmark therefore indicates an LLM's ability to make choices that align with established human expertise and strategic consensus.

*   **Clear Performance Differentiation:** When comparing ManaBench scores to other established LLM evaluation metrics like LMArena ELO ratings, ManaBench demonstrates a significantly stronger ability to differentiate between models. While a general positive correlation is observed (as indicated by the trendline in the chart below), ManaBench provides a much wider relative spread in scores. For the models included in the comparison, ManaBench accuracies range from 19.5% to 65% (a spread of 45.5 percentage points, representing an increase of approximately 233% from the minimum observed score to the maximum). In contrast, their LMArena ELO scores range from 1257 to 1470 (a spread of 213 ELO points, representing an increase of approximately 17% from the minimum observed ELO score to the maximum). The significantly larger proportional range in ManaBench allows for a more granular distinction between models.

    <div style="width: 90%; margin: 20px auto;">
        <canvas id="correlationChart"></canvas>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const ctxCorrelation = document.getElementById('correlationChart').getContext('2d');
            
            const modelLabels = [
                'o3 (low)',
                'Gemini 2.5 Pro 03-25',
                'Gemini 2.5 Flash',
                'Claude 3.7 Sonnet (no thinking)',
                'o4 Mini (low)',
                'Deepseek R1',
                'Grok 3',
                'GPT-4o 08-06',
                'Qwen3 235B A22B (thinking)',
                'Grok 3 Mini (low)', // ELO Not available, will be filtered or handled
                'Gemini 2.0 Flash',
                'Mistral 3 Medium', // ELO Not available
                'Llama 4 Maverick',
                'Command A',
                'GPT-4.1 Nano',
                'Llama 3.3 70B',
                'Gemini 1.5 Flash',
                'Deepseek V3 03-24',
                'Gemini 2.5 Pro 06-05',
                'Deepseek R1 05-28',
                'Kimi K2'
            ];

            const manaBenchScores = [
                63, 53, 50, 49.5, 45, 43.5, 41.5, 41, 37, 37, 35, 31.5, 26.5, 25, 19.5, 19.5, 22.5, 37.5, 57.5, 43, 41
            ];

            const eloScores = {
                'Gemini 2.5 Pro 03-25': 1448,
                'Gemini 2.5 Flash': 1416,
                'o3 (low)': 1411,
                'Claude 3.7 Sonnet (no thinking)': 1291,
                'o4 Mini (low)': 1351,
                'Deepseek R1': 1359,
                'Grok 3': 1409,
                'GPT-4o 08-06': 1265,
                'Qwen3 235B A22B (thinking)': 1342,
                'Gemini 2.0 Flash': 1355,
                'Llama 4 Maverick': 1269,
                'Command A': 1346,
                'GPT-4.1 Nano': 1270,
                'Llama 3.3 70B': 1257,
                'Gemini 1.5 Flash': 1271,
                'Deepseek V3 03-24': 1372,
                'Gemini 2.5 Pro 06-05': 1470,
                'Kimi K2': 1420
            };

            // Moved color definitions here, before they are used
            const backgroundColors = [
                'rgba(75, 192, 192, 0.8)', 
                'rgba(0, 200, 255, 0.8)', // Gemini 2.5 Pro 03-25
                'rgba(50, 205, 50, 0.8)',  // Gemini 2.5 Flash
                'rgba(54, 162, 235, 0.8)', 
                'rgba(255, 206, 86, 0.8)', 
                'rgba(255, 99, 132, 0.8)',  
                'rgba(255, 140, 0, 0.8)',  // Grok 3
                'rgba(153, 102, 255, 0.8)',
                'rgba(255, 159, 64, 0.8)', 
                'rgba(101, 143, 74, 0.8)',  
                'rgba(210, 105, 30, 0.8)',  
                'rgba(0, 128, 128, 0.8)',   
                'rgba(165, 42, 42, 0.8)',   
                'rgba(139, 69, 19, 0.8)',   // Command A
                'rgba(70, 130, 180, 0.8)',  
                'rgba(128, 0, 128, 0.8)',
                'rgba(255, 105, 180, 0.8)', // Gemini 1.5 Flash
                'rgba(128, 128, 0, 0.8)',    // Deepseek V3 03-24
                'rgba(0, 220, 220, 0.8)', // Gemini 2.5 Pro 06-05
                'rgba(255, 99, 100, 0.8)',  // Deepseek R1 05-28
                'rgba(0, 100, 0, 0.8)'    // Kimi K2
            ];
            const borderColors = backgroundColors.map(color => color.replace('0.8', '1'));

            const scatterDataPoints = [];
            const scatterPointBackgroundColors = [];
            const scatterPointBorderColors = [];
            const regressionPoints = []; // For calculating trendline
            const chartLabelsForTooltip = []; // Filtered labels for tooltip

            modelLabels.forEach((label, index) => {
                if (eloScores[label] !== undefined) {
                    const point = { x: eloScores[label], y: manaBenchScores[index] };
                    scatterDataPoints.push(point);
                    regressionPoints.push(point);
                    chartLabelsForTooltip.push(label);
                    scatterPointBackgroundColors.push(backgroundColors[index % backgroundColors.length] || 'rgba(150, 150, 150, 0.8)');
                    scatterPointBorderColors.push(borderColors[index % borderColors.length] || 'rgba(150, 150, 150, 1)');
                }
            });

            // Calculate trendline
            let trendlineDataset = {};
            if (regressionPoints.length >= 2) {
                let sumX = 0, sumY = 0, sumXY = 0, sumX2 = 0;
                const n = regressionPoints.length;
                regressionPoints.forEach(p => {
                    sumX += p.x;
                    sumY += p.y;
                    sumXY += p.x * p.y;
                    sumX2 += p.x * p.x;
                });

                const slope = (n * sumXY - sumX * sumY) / (n * sumX2 - sumX * sumX);
                const intercept = (sumY - slope * sumX) / n;

                const minElo = Math.min(...regressionPoints.map(p => p.x));
                const maxElo = Math.max(...regressionPoints.map(p => p.x));

                trendlineDataset = {
                    type: 'line',
                    label: 'Trendline',
                    data: [
                        { x: minElo, y: slope * minElo + intercept },
                        { x: maxElo, y: slope * maxElo + intercept }
                    ],
                    borderColor: 'rgba(255, 0, 0, 0.7)',
                    borderWidth: 2,
                    fill: false,
                    pointRadius: 0,
                    tension: 0
                };
            }
            
            const finalDatasets = [
                {
                    label: 'Models',
                    type: 'scatter',
                    data: scatterDataPoints,
                    backgroundColor: scatterPointBackgroundColors,
                    borderColor: scatterPointBorderColors,
                    pointRadius: 8,
                    pointHoverRadius: 10
                }
            ];

            if (Object.keys(trendlineDataset).length > 0) {
                finalDatasets.push(trendlineDataset);
            }

            new Chart(ctxCorrelation, {
                type: 'scatter', // Base type, can be overridden by dataset type
                data: {
                    // labels: chartLabelsForTooltip, // Not needed directly here for scatter
                    datasets: finalDatasets
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: true,
                    plugins: {
                        legend: {
                            display: false 
                        },
                        title: {
                            display: true,
                            text: 'ManaBench Accuracy vs. LMArena ELO',
                            font: {
                                size: 18
                            }
                        },
                        tooltip: {
                            filter: function(tooltipItem) {
                                // Only show tooltips for the scatter plot dataset (index 0)
                                return tooltipItem.datasetIndex === 0;
                            },
                            callbacks: {
                                label: function(context) {
                                    const modelName = chartLabelsForTooltip[context.dataIndex];
                                    let labelStr = modelName || '';
                                    if (labelStr) {
                                        labelStr += ': ';
                                    }
                                    if (context.parsed.x !== null && context.parsed.y !== null) {
                                        labelStr += `(LMArena ELO: ${context.parsed.x}, ManaBench: ${context.parsed.y}%)`;
                                    }
                                    return labelStr;
                                }
                            }
                        }
                    },
                    scales: {
                        x: {
                            type: 'linear',
                            position: 'bottom',
                            title: {
                                display: true,
                                text: 'LMArena ELO (May 2025)'
                            }
                        },
                        y: {
                            beginAtZero: true,
                            title: {
                                display: true,
                                text: 'ManaBench Accuracy (%)'
                            }
                        }
                    }
                }
            });
        });
    </script>

*   **Challenge for Frontier Models:** With the exception of o3, most of the models are far from matching human performance, let alone exceeding it. There is a 3 point gap between the top model (o3 (high) at 65%) and the human baseline, and every model except for o3 remains under 60%. This shows that the benchmark remains unsaturated and presents a significant challenge for frontier models.

*   **Test of Generalization vs. Benchmark Overfitting:** The complexity of MTG deck construction, the private nature of the benchmark questions, and the fact that MTG strategy is unlikely to be a direct optimization target for most LLM labs, collectively make ManaBench a strong test of generalized reasoning. Performance on this benchmark may reveal whether models are truly capable of applying reasoning to novel, complex systems, or if their high scores on common academic benchmarks (like MMLU or MATH) are partly due to overfitting or memorization of those specific test distributions. For example, a model series like Llama 4, which demonstrated strong performance on many standard benchmarks, gave a much weaker showing here, highlighting the value of diverse, specialized evaluations like ManaBench in assessing true generalization. This aligns with the experiences of many users who reported that Llama 4 struggled with real tasks and underperformed expectations.

*   **Cost-Effectiveness and Efficiency:** With 200 questions, the benchmark is relatively concise compared to some larger evaluation suites. This allows for more rapid and cost-effective evaluation cycles, making it feasible to test a wider array of models or fine-tuned variants without incurring prohibitive API costs or excessive computation time, while still providing strong differentiating signals as seen in the results.

## Benchmark Integrity

To maintain the integrity and long-term utility of ManaBench as an evaluation tool, the specific benchmark questions and code are **not being publicly released at this time.** This measure is taken to prevent the benchmark from being inadvertently included in the training data of future LLMs, which would compromise its validity as an unseen test set. If you are a researcher and would like private access, please reach out.

## Conclusion

ManaBench offers a novel approach to evaluating the sophisticated reasoning capabilities of Large Language Models by leveraging the strategic depth of Magic: The Gathering. The benchmark's core "deck completion task" â€“ choosing the optimal 60th card for a 59-card deck - demands an understanding of strategic coherence, system-wide optimization, and complex interactions.

The initial results demonstrate ManaBench's potential as a strong differentiator of LLM reasoning abilities, with even frontier models finding the task challenging. If you would me to add other models to the leaderboard or just think it's a cool project, consider checking out my other related work or following me on [Twitter](https://x.com/JakeABoggs)