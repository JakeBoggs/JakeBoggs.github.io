<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Reverse Engineering GPT-4o | Jake Boggs</title>
<meta name=keywords content><meta name=description content="How does GPT-4o work? Based purely on speculation, I have no inside information. All numbers were pulled out of my ass. This is just my highly-detailed, educated guess on how the model works, along with a Pytorch implementation. If that sounds interesting, then keep reading.
Tokenization The cornerstone of GPT-4o&rsquo;s capabilities lies in its unified representation of diverse input types. Here&rsquo;s how each modality is tokenized:
Text Tokenization If you&rsquo;re familiar with LLMs, you can skip this section."><meta name=author content="Jake Boggs"><link rel=canonical href=https://boggs.tech/posts/gpt-4o-model-architecture/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://boggs.tech/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://boggs.tech/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://boggs.tech/favicon-32x32.png><link rel=apple-touch-icon href=https://boggs.tech/apple-touch-icon.png><link rel=mask-icon href=https://boggs.tech/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://boggs.tech/posts/gpt-4o-model-architecture/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Reverse Engineering GPT-4o"><meta property="og:description" content="How does GPT-4o work? Based purely on speculation, I have no inside information. All numbers were pulled out of my ass. This is just my highly-detailed, educated guess on how the model works, along with a Pytorch implementation. If that sounds interesting, then keep reading.
Tokenization The cornerstone of GPT-4o&rsquo;s capabilities lies in its unified representation of diverse input types. Here&rsquo;s how each modality is tokenized:
Text Tokenization If you&rsquo;re familiar with LLMs, you can skip this section."><meta property="og:type" content="article"><meta property="og:url" content="https://boggs.tech/posts/gpt-4o-model-architecture/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-09T00:00:00+00:00"><meta property="article:modified_time" content="2024-08-09T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Reverse Engineering GPT-4o"><meta name=twitter:description content="How does GPT-4o work? Based purely on speculation, I have no inside information. All numbers were pulled out of my ass. This is just my highly-detailed, educated guess on how the model works, along with a Pytorch implementation. If that sounds interesting, then keep reading.
Tokenization The cornerstone of GPT-4o&rsquo;s capabilities lies in its unified representation of diverse input types. Here&rsquo;s how each modality is tokenized:
Text Tokenization If you&rsquo;re familiar with LLMs, you can skip this section."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://boggs.tech/posts/"},{"@type":"ListItem","position":2,"name":"Reverse Engineering GPT-4o","item":"https://boggs.tech/posts/gpt-4o-model-architecture/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reverse Engineering GPT-4o","name":"Reverse Engineering GPT-4o","description":"How does GPT-4o work? Based purely on speculation, I have no inside information. All numbers were pulled out of my ass. This is just my highly-detailed, educated guess on how the model works, along with a Pytorch implementation. If that sounds interesting, then keep reading.\nTokenization The cornerstone of GPT-4o\u0026rsquo;s capabilities lies in its unified representation of diverse input types. Here\u0026rsquo;s how each modality is tokenized:\nText Tokenization If you\u0026rsquo;re familiar with LLMs, you can skip this section.","keywords":[],"articleBody":"How does GPT-4o work? Based purely on speculation, I have no inside information. All numbers were pulled out of my ass. This is just my highly-detailed, educated guess on how the model works, along with a Pytorch implementation. If that sounds interesting, then keep reading.\nTokenization The cornerstone of GPT-4o’s capabilities lies in its unified representation of diverse input types. Here’s how each modality is tokenized:\nText Tokenization If you’re familiar with LLMs, you can skip this section. GPT-4o employs SentencePiece with BPE (Byte Pair Encoding) for text tokenization. This combination provides a robust, language-independent subword tokenization method that’s particularly effective for handling multiple languages and out-of-vocabulary words. Let’s delve deeper into how this works:\nTokenization Using SentencePiece Suppose we want to train a SentencePiece on the following corpus:\nlower lower lower Step 1: Initial Vocabulary SentencePiece starts with an initial vocabulary consisting of all individual characters from the input text, including a special symbol to denote whitespace. For example:\nInitial vocabulary: {'l', 'o', 'w', 'e', 'r', '▁'} Step 2: Iterative Pair Merging (Byte Pair Encoding) The model then identifies the most frequent contiguous pairs of characters in the text and merges them into subword units. This process is repeated iteratively until the desired vocabulary size is reached. Let’s go through the process in more detail:\nFirst Iteration:\nInput text: lower lower lower The most frequent pair of characters: l and o Merge result: lo New vocabulary: {'lo', 'w', 'e', 'r', '▁'} Updated text: lo w e r lo w e r lo w e r Second Iteration:\nInput text: lo w e r lo w e r lo w e r The most frequent pair of characters: lo and w Merge result: low New vocabulary: {'low', 'e', 'r', '▁'} Updated text: low e r low e r low e r The process continues iteratively until the predefined vocabulary size is reached. The final vocabulary might include common subwords or entire words, depending on their frequency in the training data.\nAudio Tokenization Audio tokenization in GPT-4o is particularly interesting. This process is inspired by recent advancements in neural audio compression, particularly the EnCodec model.\nInput Format: Raw waveform audio at 24 kHz sampling rate. Preprocessing: Resampling to 24 kHz if necessary. Normalization to the range [-1, 1]. Splitting into 250 ms frames (6000 samples per frame). Encoder Architecture The audio encoder in GPT-4o consists of:\nAn initial 1D convolution layer with 32 channels and a kernel size of 7. Four convolutional blocks, each comprising: A residual unit with two convolutions (kernel size 3) and a skip-connection A down-sampling layer (strided convolution) with strides (2, 4, 5, 8) The number of channels doubles after each down-sampling operation A sequence modeling component (two-layer LSTM) for capturing temporal dependencies. A final convolutional layer with a kernel size of 7 to produce the latent representation. The encoder uses ELU (Exponential Linear Unit) as the activation function and employs Weight Normalization for streamable processing.\nQuantization The encoder output is quantized using Residual Vector Quantization (RVQ), a powerful technique that allows for efficient compression of high-dimensional vectors. In the context of GPT-4o’s audio processing:\nEach input frame vector is quantized using a first codebook of size 65,536. The residual (difference between the input and the quantized output) is then quantized using the next codebook. This process is repeated for a predefined number of codebooks, in this case, 4. Token Structure The resulting audio tokens in GPT-4o have a unique structure:\nDimension: 4 x 65536 (4 codebooks x codebook size) These are flattened into a single vector before entering the embedding layer. The codebook size and number were selected because they are approximately the same size as the text and image vocabulary when flattened and provide enough quality for good speech audio.\nImage Tokenization The image tokenization process in GPT-4o is a crucial component that bridges the gap between raw images and the discrete tokens that the transformer model can process. This process is similar to the approach described in the Chameleon paper but includes some modifications and improvements.\nTokenizer Architecture: The image tokenizer is based on a Vector-Quantized Variational Autoencoder (VQ-VAE) architecture. This tokenizer is trained to encode a 768 × 768 image into 16,384 discrete tokens, each selected from a codebook of size 65,536. This is a significant increase from the 8,192 codebook size used in Chameleon, allowing for more detailed image representations and is particurlarly important for OCR, which Chameleon struggles with.\nEncoding Process:\nThe input image is passed through an encoder network, which transforms it into a 3D tensor of latent vectors. Each spatial location in this latent space is then quantized to its nearest neighbor in the learned codebook. The indices of these nearest neighbors become the discrete tokens representing the image. Vector Quantization: The core of the tokenization process is the vector quantization step. Here’s how it works:\nFor each latent vector z_e(x) produced by the encoder: Find the closest embedding e_i in the codebook. The index i becomes the token for this spatial location. Integration with Transformer: These image tokens are then embedded into the same dimensional space as the text and scene tokens. This allows the transformer to process all modalities in a unified manner.\nDecoding: During generation, the transformer predicts image tokens, which are then passed through the VQ-VAE’s decoder to reconstruct the final image.\nSource Flag A binary flag is added to the end of tokens, indicating whether the token was produced by the model or received as input. This is implemented as an additional 0 or 1 at the end of the one-hot encoding for tokens. This crucial feature enables the model to distinguish between its outputs and real-time inputs during processing.\nPositional Encodings This is the real secret sauce of GPT-4o. It has the ability to handle all of these modalities in real time. This is particularly challenging due to the varying rates at which different types of input are received:\nAudio frames: 4 per second Images: Dependent on frame rate Text: Varies based on user input speed To address this, GPT-4o employs a dual encoding system similar to σ-GPTs:\nModality-specific position encoding: Represents the position within a specific modality Time-based encoding: A function of absolute time These encodings are concatenated to the end of the embedding rather than added, preserving their individual information and preventing them from interfering with one another. This goes against typical transformer architecture design, but is vital to allow the temporal and modality-specific positon embeddings to operately separately. During training and inference, the attention mask considers both the position within a modality and the temporal position, ensuring proper alignment of multimodal inputs.\nThe time-based embeddings are at the same frequency as the audio, meaning 4 steps per second. This temporal resolution also applies to the image input stream, limiting the model to a maximum of 4 frames per second for video input. However, you can somewhat work around this by giving multiple frames the same time encoding and varying the modality-specific encoding. In practice, you probably wouldn’t want to do this, as you’d start to be limited by compute and would gain minimal information from the extra frames.\nModel Inputs The model has five input streams, which are all concatenated together with attention masks to enforce temporal boundaries:\nText input stream Audio input stream Image input stream from the real-time video feed Text/image output stream Audio output stream Note that these are not hard boundaries, and tokens from different modalities can be mixed into the same sequence. For example:\n“Describe what is happening in the following image: [image]” “Transcribe the following audio: [audio]” These streams are more akin to logical separators for the real-time inputs. The last two streams are for the model’s outputs, one for text/image and one for audio. This is where the source flag from earlier comes into play, allowing the model to distinguish between its own outputs and the inputs. It needs this because it cannot do autoregressive prediction for real-time inputs, as there will be additional tokens being added to the sequence while it is generating an output.\nModel Outputs GPT-4o features a dual-headed output system:\nText and image output head:\nPredicts text and image tokens using standard one-hot encoding Includes a “No Output” token Audio output head:\nMultiple sub-heads for each codebook in the audio tokenization Also includes a “No Output” token The “No Output” tokens are not passed back into the input and allow the model to:\nChoose which modality to reply with Stay silent without consuming extra tokens Respond with both text, image, and audio simultaneously This design gives GPT-4o the flexibility to generate appropriate responses across modalities.\nModel Size 230 billion parameters. I made this number up, but here’s a plausible justification:\nLlama 3.1 70B costs around $0.5 per million input tokens. GPT-4o-2024-08-06 costs $2.50 per million input tokens. Assume ClosedAI has pricing power due to the model’s intelligence and brand, so they can charge around 50% more. Calculation: 70 * 2.5 / 0.5 / 1.5 ≈ 230\nThis size is also approximately equivalent to one GPT-4 expert, which was rumored to be 8x222B. This could explain why the cost is so much lower compared to GPT-4.\nThis is all just speculation, and I have no affiliation with ClosedAI.\nTraining The training process for GPT-4o is complex and multi-staged, incorporating several advanced techniques to achieve its impressive multimodal and real-time capabilities.\nPretraining During this stage, the temporal embeddings are set to the same value for all text and image samples and are equal to the position embedding for the audio samples. The source flag is also set to 0 for all tokens. Note that the audio and image tokenizers must be trained separately before beginning.\nInterleaved Text and Image Pretraining: Following an approach similar to the Chameleon model, GPT-4o is first pretrained on a large dataset of interleaved text and image data scraped from the web. This allows the model to develop a unified representation across these modalities.\nAudio Tokenizer Training: After the initial pretraining, the audio embeddings are trained while the main model weights are frozen. The model performs autoregressive prediction on audio tokens, learning to understand and generate audio content. Data for this is sourced by scraping online podcasts and applying voice activity detection to filter the audio.\nMultimodal Unfrozen Training: Once the audio tokenizer is sufficiently trained, all weights are unfrozen, and the model trains on all modalities simultaneously, further integrating its understanding across text, image, and audio. Note that audio samples still haven’t been combined with the text and images at this stage, this will happen later. The output head that isn’t used for a sample is trained to predict the “No Output” token.\nFine-tuning and Optimization Instruction Fine-tuning: The model undergoes instruction fine-tuning to improve its ability to follow user instructions and perform specific tasks. The source flag is now set to 1 for tokens produced by the model.\nDirect Preference Optimization (DPO): Utilizing data from user sessions on the ChatGPT website, GPT-4o employs Direct Preference Optimization to align its outputs with user preferences. This method allows for efficient optimization without the need for complex reinforcement learning algorithms, which are difficult to train and prone to over-fitting. The DPO approach used in GPT-4o enables the model to learn from human preferences by directly optimizing a policy to satisfy these preferences, using a simple binary cross-entropy objective.\nChain-of-Thought Training: GPT-4o employs a technique similar to the stepwise internalization method described in recent research. This process helps the model internalize intermediate reasoning steps:\nThe model is initially trained to generate explicit chain-of-thought reasoning steps. Gradually, these intermediate steps are removed during training, forcing the model to internalize the reasoning process. The training progresses through multiple stages, each removing more of the explicit reasoning steps. By the final stage, the model can perform implicit chain-of-thought reasoning, producing high-quality outputs without generating explicit intermediate steps. This technique allows GPT-4o to reason effectively while maintaining the speed advantages of models that don’t use explicit chain-of-thought.\nReal-time Multimodal Integration Until now, all of the training has following the standard autoregressive setup. The final stage of training splits the input into the five channels focuses on integrating the various modalities in real-time:\nSynthetic Multimodal Data Generation: The base model generates scripts that simulate real-time interactions, including dialogue between users and the assistant, along with actions like “Begin transcribing what I’m saying” or “Describe what is happening in these images”. This leverages the model’s text and image generation capabilities to create scripts similar to what you might see in a TV show, along with supplemental images. Portions of the scripts are then converted to audio using an existing text-to-speech model.\nReal-time Processing Simulation: The synthetic data is processed to simulate real-time inputs, with text, audio, and image inputs interleaved to mimic real-world scenarios. A system prompt is added with audio tokens from the target speaker to select a voice for the model.\nFinal Integrated Training: GPT-4o undergoes a final round of training on this synthetic real-time multimodal data, enhancing its ability to seamlessly integrate and respond to text, image, and audio inputs as they arrive, mimicking real-time interaction scenarios.\nPytorch Implementation Here’s a PyTorch implementation, written by Claude 3.5 Sonnet:\nimport torch import torch.nn as nn import torch.nn.functional as F from tokenizers import SentencePieceBPETokenizer class TransformerBlock(nn.Module): \"\"\" A single transformer block, consisting of multi-head attention and a feed-forward network. \"\"\" def __init__(self, d_model, nhead): super().__init__() self.attention = nn.MultiheadAttention(d_model, nhead) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.feed_forward = nn.Sequential( nn.Linear(d_model, 4 * d_model), nn.ReLU(), nn.Linear(4 * d_model, d_model) ) def forward(self, x, attention_mask): # Multi-head attention attended, _ = self.attention(x, x, x, attn_mask=attention_mask) # Add \u0026 Norm x = self.norm1(x + attended) # Feed-forward network fed_forward = self.feed_forward(x) # Add \u0026 Norm return self.norm2(x + fed_forward) class VQVAEEncoder(nn.Module): \"\"\" Vector Quantized Variational Autoencoder (VQ-VAE) Encoder for image tokenization. \"\"\" def __init__(self, in_channels, hidden_dim, num_embeddings=65536, embedding_dim=32): super().__init__() # Encoder network to transform 768x768 image into 128x128 latent space self.encoder = nn.Sequential( nn.Conv2d(in_channels, hidden_dim, kernel_size=4, stride=2, padding=1), nn.ReLU(), nn.Conv2d(hidden_dim, hidden_dim, kernel_size=4, stride=2, padding=1), nn.ReLU(), nn.Conv2d(hidden_dim, hidden_dim, kernel_size=4, stride=2, padding=1), nn.ReLU(), nn.Conv2d(hidden_dim, embedding_dim, kernel_size=3, stride=1, padding=1) ) # Codebook for vector quantization self.codebook = nn.Embedding(num_embeddings, embedding_dim) def forward(self, x): # Encode the input image z = self.encoder(x) z = z.permute(0, 2, 3, 1).contiguous() z_flattened = z.view(-1, z.shape[-1]) # Compute distances to codebook vectors d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\ torch.sum(self.codebook.weight ** 2, dim=1) - \\ 2 * torch.matmul(z_flattened, self.codebook.weight.t()) # Find nearest codebook vectors min_encoding_indices = torch.argmin(d, dim=1) z_q = self.codebook(min_encoding_indices).view(z.shape) return min_encoding_indices, z_q class VQVAEDecoder(nn.Module): \"\"\" VQ-VAE Decoder for image reconstruction from tokens. \"\"\" def __init__(self, out_channels, hidden_dim, embedding_dim): super().__init__() self.decoder = nn.Sequential( nn.ConvTranspose2d(embedding_dim, hidden_dim, 3, 1, 1), nn.ReLU(), nn.ConvTranspose2d(hidden_dim, hidden_dim, 4, 2, 1), nn.ReLU(), nn.ConvTranspose2d(hidden_dim, hidden_dim, 4, 2, 1), nn.ReLU(), nn.ConvTranspose2d(hidden_dim, out_channels, 4, 2, 1) ) def forward(self, z_q): return self.decoder(z_q.permute(0, 3, 1, 2)) class EncoderBlock(nn.Module): \"\"\" Encoder block for the audio tokenizer, featuring residual connections and downsampling. \"\"\" def __init__(self, in_channels, out_channels, stride): super().__init__() self.residual = nn.Sequential( nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1), nn.ELU(), nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1) ) self.downsample = nn.Conv1d(in_channels, out_channels, kernel_size=stride*2, stride=stride, padding=stride//2) self.elu = nn.ELU() self.norm = nn.utils.weight_norm(nn.Conv1d(out_channels, out_channels, kernel_size=1)) def forward(self, x): residual = self.residual(x) shortcut = self.downsample(x) return self.elu(self.norm(residual + shortcut)) class AudioTokenizer(nn.Module): \"\"\" Audio tokenizer using a series of convolutional layers, LSTM, and Residual Vector Quantization (RVQ). \"\"\" def __init__(self, input_dim, hidden_dim, num_codebooks, codebook_size): super().__init__() self.num_codebooks = num_codebooks self.codebook_size = codebook_size # Initial convolution self.initial_conv = nn.Conv1d(input_dim, hidden_dim, kernel_size=7, padding=3) # Encoder blocks with different strides self.encoder_blocks = nn.ModuleList([ EncoderBlock(hidden_dim * 2**min(i, 3), hidden_dim * 2**min(i+1, 3), stride) for i, stride in enumerate([2, 4, 5, 8]) ]) # LSTM for sequence modeling self.lstm = nn.LSTM(hidden_dim * 8, hidden_dim * 8, num_layers=2, batch_first=True) # Final convolution self.final_conv = nn.Conv1d(hidden_dim * 8, num_codebooks * codebook_size, kernel_size=7, padding=3) # RVQ codebooks self.codebooks = nn.ModuleList([nn.Embedding(codebook_size, hidden_dim * 8) for _ in range(num_codebooks)]) def forward(self, x): # Initial convolution and activation x = F.elu(self.initial_conv(x)) # Pass through encoder blocks for block in self.encoder_blocks: x = block(x) # LSTM processing x = x.transpose(1, 2) x, _ = self.lstm(x) x = x.transpose(1, 2) # Final convolution and reshaping x = self.final_conv(x) x = x.transpose(1, 2).contiguous() x = x.view(-1, self.num_codebooks, self.codebook_size) # Residual Vector Quantization indices = [] quantized = [] residual = x for i in range(self.num_codebooks): # Find nearest codebook vector idx = torch.argmin(torch.sum((residual.unsqueeze(2) - self.codebooks[i].weight.unsqueeze(0).unsqueeze(0)) ** 2, dim=-1), dim=-1) quant = self.codebooks[i](idx) indices.append(idx) quantized.append(quant) residual = residual - quant indices = torch.stack(indices, dim=-1) quantized = torch.stack(quantized, dim=-1).sum(dim=-1) return indices, quantized class GPT4o(nn.Module): class GPT4o(nn.Module): def __init__(self, vocab_size, d_model, nhead, num_layers, num_codebooks, audio_vocab_size, image_vocab_size): super().__init__() self.d_model = d_model self.vocab_size = vocab_size self.audio_vocab_size = audio_vocab_size self.image_vocab_size = image_vocab_size self.num_codebooks = num_codebooks # Text tokenizer self.text_tokenizer = SentencePieceBPETokenizer() # Note: In practice, you would train this tokenizer on your corpus separately # and save/load it, rather than training it here. # self.text_tokenizer.train(files=[\"path/to/your/corpus.txt\"], vocab_size=vocab_size) # Embeddings for different modalities self.text_embedding = nn.Embedding(vocab_size, self.d_model) self.audio_embedding = nn.Embedding(audio_vocab_size * num_codebooks, self.d_model) self.image_embedding = nn.Embedding(image_vocab_size, self.d_model) # Audio tokenizer self.audio_tokenizer = AudioTokenizer(1, 32, num_codebooks, audio_vocab_size) # Image encoder (VQ-VAE) self.image_encoder = VQVAEEncoder(3, 256, num_embeddings=image_vocab_size, embedding_dim=32) self.image_decoder = VQVAEDecoder(3, 256, 32) # Positional and temporal encodings self.pos_encoding = nn.Embedding(5000, self.d_model) # Max 5000 positions self.time_encoding = nn.Embedding(1000, self.d_model) # Max 1000 time steps (250 seconds at 4 steps/sec) # Main transformer blocks self.blocks = nn.ModuleList([TransformerBlock(self.d_model, nhead) for _ in range(num_layers)]) # Output heads for text/image and audio self.text_image_output = nn.Linear(self.d_model, vocab_size + image_vocab_size + 1) # +1 for \"No Output\" token self.audio_output = nn.ModuleList([ nn.Linear(self.d_model, audio_vocab_size + 1) for _ in range(num_codebooks) ]) def tokenize_text(self, text): # Tokenize the input text encoded = self.text_tokenizer.encode(text) return torch.tensor(encoded.ids) def forward(self, text_input, audio_input, image_input, positions, times, flags, attention_mask): # Ensure all inputs are on the same device device = self.text_embedding.weight.device # Tokenize and embed text inputs text_tokens = [self.tokenize_text(text).to(device) for text in text_input] text_tokens = nn.utils.rnn.pad_sequence(text_tokens, batch_first=True) text_emb = self.text_embedding(text_tokens) # Tokenize and embed audio inputs audio_input = audio_input.to(device) audio_tokens, audio_quantized = self.audio_tokenizer(audio_input) audio_emb = self.audio_embedding(audio_tokens.view(-1, self.num_codebooks * self.audio_vocab_size)) # Tokenize and embed image inputs image_embs = [] for img in image_input: img = img.to(device) image_tokens, _ = self.image_encoder(img) image_emb = self.image_embedding(image_tokens.view(-1, 16384)) # 16384 tokens for 128x128 latent space image_embs.append(image_emb) image_emb = torch.cat(image_embs, dim=1) # Ensure all embeddings have the same sequence length max_len = max(text_emb.shape[1], audio_emb.shape[1], image_emb.shape[1]) text_emb = F.pad(text_emb, (0, 0, 0, max_len - text_emb.shape[1])) audio_emb = F.pad(audio_emb, (0, 0, 0, max_len - audio_emb.shape[1])) image_emb = F.pad(image_emb, (0, 0, 0, max_len - image_emb.shape[1])) # Combine embeddings from all modalities combined_emb = torch.cat([text_emb, audio_emb, image_emb], dim=1) # Add positional and temporal encodings pos_emb = self.pos_encoding(positions) time_emb = self.time_encoding(times) combined_emb = torch.cat([combined_emb, pos_emb, time_emb], dim=-1) # Add input/output flag to each token embedding flags = flags.unsqueeze(-1).float() # Shape: [batch_size, seq_len, 1] combined_emb = torch.cat([combined_emb, flags], dim=-1) # Pass through transformer blocks for block in self.blocks: combined_emb = block(combined_emb, attention_mask) # Generate outputs for text/image and audio text_image_logits = self.text_image_output(combined_emb) audio_logits = [head(combined_emb) for head in self.audio_output] return text_image_logits, audio_logits def generate_image(self, image_tokens): # Convert image tokens back to embeddings z_q = self.image_embedding(image_tokens).view(-1, 128, 128, 32) # 32 is the embedding_dim from VQVAEEncoder # Decode the image return self.image_decoder(z_q) def decode_text(self, text_tokens): # Convert text tokens back to string return self.text_tokenizer.decode(text_tokens.tolist()) def decode_audio(self, audio_tokens): # This is a placeholder. In practice, you'd need to implement # a method to convert audio tokens back to waveform. return audio_tokens # Example usage vocab_size = 200019 d_model = 4096 nhead = 32 num_layers = 64 num_codebooks = 4 audio_vocab_size = 65536 image_vocab_size = 65536 # Initialize the GPT-4o model model = GPT4o(vocab_size, d_model, nhead, num_layers, num_codebooks, audio_vocab_size, image_vocab_size) # Create dummy inputs for demonstration batch_size = 4 seq_len = 100 audio_frame_length = 6000 # 250 ms at 24 kHz num_audio_frames = 4 # 4 frames per second num_images = 1 # Assuming one image per sequence, adjust as needed # Text input (list of strings) text_input = [\"This is a sample text input\" for _ in range(batch_size)] # Audio input (tensor) audio_input = torch.randn(batch_size, 1, audio_frame_length * num_audio_frames) # 1 second of audio # Image input (tensor) image_input = torch.randn(batch_size, num_images, 3, 768, 768) # Batch of RGB images # Positions, times, and flags positions = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1) times = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1) flags = torch.randint(0, 2, (batch_size, seq_len)) # 0 for input, 1 for output # Attention mask attention_mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).expand(batch_size, -1, -1) # Forward pass through the model text_image_logits, audio_logits = model(text_input, audio_input, image_input, positions, times, flags, attention_mask) # Print output shapes print(\"Text and Image logits shape:\", text_image_logits.shape) print(\"Audio logits shape:\", [logits.shape for logits in audio_logits]) # Example of using generate_image image_tokens = torch.randint(0, image_vocab_size, (batch_size, 16384)) generated_image = model.generate_image(image_tokens) print(\"Generated image shape:\", generated_image.shape) # Example of decoding text text_tokens = torch.argmax(text_image_logits[:, :, :vocab_size], dim=-1) decoded_text = model.decode_text(text_tokens[0]) # Decode the first sequence in the batch print(\"Decoded text:\", decoded_text) # Example of decoding audio audio_tokens = [torch.argmax(logits, dim=-1) for logits in audio_logits] decoded_audio = model.decode_audio(audio_tokens) print(\"Decoded audio shape:\", [tokens.shape for tokens in decoded_audio]) If you’ve made it this far and somehow have access to the ungodly number of GPUs required to train this monster, hit me up and let’s give ClosedAI a run for their money.\n","wordCount":"3522","inLanguage":"en","datePublished":"2024-08-09T00:00:00Z","dateModified":"2024-08-09T00:00:00Z","author":{"@type":"Person","name":"Jake Boggs"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://boggs.tech/posts/gpt-4o-model-architecture/"},"publisher":{"@type":"Organization","name":"Jake Boggs","logo":{"@type":"ImageObject","url":"https://boggs.tech/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://boggs.tech/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://boggs.tech/startups title=Startups><span>Startups</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Reverse Engineering GPT-4o</h1><div class=post-meta><span title='2024-08-09 00:00:00 +0000 UTC'>August 9, 2024</span>&nbsp;·&nbsp;17 min&nbsp;·&nbsp;3522 words&nbsp;·&nbsp;Jake Boggs</div></header><div class=post-content><h2 id=how-does-gpt-4o-work>How does GPT-4o work?<a hidden class=anchor aria-hidden=true href=#how-does-gpt-4o-work>#</a></h2><p>Based purely on speculation, I have no inside information. All numbers were pulled out of my ass. This is just my highly-detailed, educated guess on how the model works, along with a Pytorch implementation. If that sounds interesting, then keep reading.</p><h2 id=tokenization>Tokenization<a hidden class=anchor aria-hidden=true href=#tokenization>#</a></h2><p>The cornerstone of GPT-4o&rsquo;s capabilities lies in its unified representation of diverse input types. Here&rsquo;s how each modality is tokenized:</p><h3 id=text-tokenization>Text Tokenization<a hidden class=anchor aria-hidden=true href=#text-tokenization>#</a></h3><p>If you&rsquo;re familiar with LLMs, you can skip this section. GPT-4o employs SentencePiece with BPE (Byte Pair Encoding) for text tokenization. This combination provides a robust, language-independent subword tokenization method that&rsquo;s particularly effective for handling multiple languages and out-of-vocabulary words. Let&rsquo;s delve deeper into how this works:</p><h4 id=tokenization-using-sentencepiece>Tokenization Using SentencePiece<a hidden class=anchor aria-hidden=true href=#tokenization-using-sentencepiece>#</a></h4><p>Suppose we want to train a SentencePiece on the following corpus:</p><pre tabindex=0><code>lower lower lower
</code></pre><h4 id=step-1-initial-vocabulary>Step 1: Initial Vocabulary<a hidden class=anchor aria-hidden=true href=#step-1-initial-vocabulary>#</a></h4><p>SentencePiece starts with an initial vocabulary consisting of all individual characters from the input text, including a special symbol to denote whitespace. For example:</p><pre tabindex=0><code>Initial vocabulary: {&#39;l&#39;, &#39;o&#39;, &#39;w&#39;, &#39;e&#39;, &#39;r&#39;, &#39;▁&#39;}
</code></pre><h4 id=step-2-iterative-pair-merging-byte-pair-encoding>Step 2: Iterative Pair Merging (Byte Pair Encoding)<a hidden class=anchor aria-hidden=true href=#step-2-iterative-pair-merging-byte-pair-encoding>#</a></h4><p>The model then identifies the most frequent contiguous pairs of characters in the text and merges them into subword units. This process is repeated iteratively until the desired vocabulary size is reached. Let&rsquo;s go through the process in more detail:</p><ol><li><p><strong>First Iteration:</strong></p><ul><li>Input text: <code>lower lower lower</code></li><li>The most frequent pair of characters: <code>l</code> and <code>o</code></li><li>Merge result: <code>lo</code></li><li>New vocabulary: <code>{'lo', 'w', 'e', 'r', '▁'}</code></li><li>Updated text: <code>lo w e r lo w e r lo w e r</code></li></ul></li><li><p><strong>Second Iteration:</strong></p><ul><li>Input text: <code>lo w e r lo w e r lo w e r</code></li><li>The most frequent pair of characters: <code>lo</code> and <code>w</code></li><li>Merge result: <code>low</code></li><li>New vocabulary: <code>{'low', 'e', 'r', '▁'}</code></li><li>Updated text: <code>low e r low e r low e r</code></li></ul></li></ol><p>The process continues iteratively until the predefined vocabulary size is reached. The final vocabulary might include common subwords or entire words, depending on their frequency in the training data.</p><h3 id=audio-tokenization>Audio Tokenization<a hidden class=anchor aria-hidden=true href=#audio-tokenization>#</a></h3><p>Audio tokenization in GPT-4o is particularly interesting. This process is inspired by recent advancements in neural audio compression, particularly the <a href=https://arxiv.org/abs/2210.13438>EnCodec</a> model.</p><ul><li><strong>Input Format</strong>: Raw waveform audio at 24 kHz sampling rate.</li><li><strong>Preprocessing</strong>:<ol><li>Resampling to 24 kHz if necessary.</li><li>Normalization to the range [-1, 1].</li><li>Splitting into 250 ms frames (6000 samples per frame).</li></ol></li></ul><h4 id=encoder-architecture>Encoder Architecture<a hidden class=anchor aria-hidden=true href=#encoder-architecture>#</a></h4><p>The audio encoder in GPT-4o consists of:</p><ol><li>An initial 1D convolution layer with 32 channels and a kernel size of 7.</li><li>Four convolutional blocks, each comprising:<ul><li>A residual unit with two convolutions (kernel size 3) and a skip-connection</li><li>A down-sampling layer (strided convolution) with strides (2, 4, 5, 8)</li><li>The number of channels doubles after each down-sampling operation</li></ul></li><li>A sequence modeling component (two-layer LSTM) for capturing temporal dependencies.</li><li>A final convolutional layer with a kernel size of 7 to produce the latent representation.</li></ol><p>The encoder uses ELU (Exponential Linear Unit) as the activation function and employs Weight Normalization for streamable processing.</p><h4 id=quantization>Quantization<a hidden class=anchor aria-hidden=true href=#quantization>#</a></h4><p>The encoder output is quantized using <a href=https://www.assemblyai.com/blog/what-is-residual-vector-quantization/>Residual Vector Quantization</a> (RVQ), a powerful technique that allows for efficient compression of high-dimensional vectors. In the context of GPT-4o&rsquo;s audio processing:</p><ol><li>Each input frame vector is quantized using a first codebook of size 65,536.</li><li>The residual (difference between the input and the quantized output) is then quantized using the next codebook.</li><li>This process is repeated for a predefined number of codebooks, in this case, 4.</li></ol><h4 id=token-structure>Token Structure<a hidden class=anchor aria-hidden=true href=#token-structure>#</a></h4><p>The resulting audio tokens in GPT-4o have a unique structure:</p><ul><li>Dimension: 4 x 65536 (4 codebooks x codebook size)</li><li>These are flattened into a single vector before entering the embedding layer.</li></ul><p>The codebook size and number were selected because they are approximately the same size as the text and image vocabulary when flattened and provide enough quality for good speech audio.</p><h3 id=image-tokenization>Image Tokenization<a hidden class=anchor aria-hidden=true href=#image-tokenization>#</a></h3><p>The image tokenization process in GPT-4o is a crucial component that bridges the gap between raw images and the discrete tokens that the transformer model can process. This process is similar to the approach described in the <a href=https://arxiv.org/abs/2405.09818>Chameleon paper</a> but includes some modifications and improvements.</p><ol><li><p><strong>Tokenizer Architecture</strong>: The image tokenizer is based on a <a href=https://arxiv.org/abs/2203.13131>Vector-Quantized Variational Autoencoder</a> (VQ-VAE) architecture. This tokenizer is trained to encode a 768 × 768 image into 16,384 discrete tokens, each selected from a codebook of size 65,536. This is a significant increase from the 8,192 codebook size used in Chameleon, allowing for more detailed image representations and is particurlarly important for OCR, which Chameleon struggles with.</p></li><li><p><strong>Encoding Process</strong>:</p><ol><li>The input image is passed through an encoder network, which transforms it into a 3D tensor of latent vectors.</li><li>Each spatial location in this latent space is then quantized to its nearest neighbor in the learned codebook.</li><li>The indices of these nearest neighbors become the discrete tokens representing the image.</li></ol></li><li><p><strong>Vector Quantization</strong>: The core of the tokenization process is the vector quantization step. Here&rsquo;s how it works:</p><ul><li>For each latent vector <code>z_e(x)</code> produced by the encoder:<ol><li>Find the closest embedding <code>e_i</code> in the codebook.</li><li>The index <code>i</code> becomes the token for this spatial location.</li></ol></li></ul></li><li><p><strong>Integration with Transformer</strong>: These image tokens are then embedded into the same dimensional space as the text and scene tokens. This allows the transformer to process all modalities in a unified manner.</p></li><li><p><strong>Decoding</strong>: During generation, the transformer predicts image tokens, which are then passed through the VQ-VAE&rsquo;s decoder to reconstruct the final image.</p></li></ol><h3 id=source-flag>Source Flag<a hidden class=anchor aria-hidden=true href=#source-flag>#</a></h3><p>A binary flag is added to the end of tokens, indicating whether the token was produced by the model or received as input. This is implemented as an additional 0 or 1 at the end of the one-hot encoding for tokens. This crucial feature enables the model to distinguish between its outputs and real-time inputs during processing.</p><h2 id=positional-encodings>Positional Encodings<a hidden class=anchor aria-hidden=true href=#positional-encodings>#</a></h2><p>This is the real secret sauce of GPT-4o. It has the ability to handle all of these modalities in real time. This is particularly challenging due to the varying rates at which different types of input are received:</p><ul><li>Audio frames: 4 per second</li><li>Images: Dependent on frame rate</li><li>Text: Varies based on user input speed</li></ul><p>To address this, GPT-4o employs a dual encoding system similar to <a href=https://arxiv.org/abs/2404.09562>σ-GPTs</a>:</p><ol><li>Modality-specific position encoding: Represents the position within a specific modality</li><li>Time-based encoding: A function of absolute time</li></ol><p>These encodings are concatenated to the end of the embedding rather than added, preserving their individual information and preventing them from interfering with one another. This goes against typical transformer architecture design, but is vital to allow the temporal and modality-specific positon embeddings to operately separately. During training and inference, the attention mask considers both the position within a modality and the temporal position, ensuring proper alignment of multimodal inputs.</p><p>The time-based embeddings are at the same frequency as the audio, meaning 4 steps per second. This temporal resolution also applies to the image input stream, limiting the model to a maximum of 4 frames per second for video input. However, you can somewhat work around this by giving multiple frames the same time encoding and varying the modality-specific encoding. In practice, you probably wouldn&rsquo;t want to do this, as you&rsquo;d start to be limited by compute and would gain minimal information from the extra frames.</p><h2 id=model-inputs>Model Inputs<a hidden class=anchor aria-hidden=true href=#model-inputs>#</a></h2><p>The model has five input streams, which are all concatenated together with attention masks to enforce temporal boundaries:</p><ol><li>Text input stream</li><li>Audio input stream</li><li>Image input stream from the real-time video feed</li><li>Text/image output stream</li><li>Audio output stream</li></ol><p>Note that these are not hard boundaries, and tokens from different modalities can be mixed into the same sequence. For example:</p><ul><li>&ldquo;Describe what is happening in the following image: [image]&rdquo;</li><li>&ldquo;Transcribe the following audio: [audio]&rdquo;</li></ul><p>These streams are more akin to logical separators for the real-time inputs. The last two streams are for the model&rsquo;s outputs, one for text/image and one for audio. This is where the source flag from earlier comes into play, allowing the model to distinguish between its own outputs and the inputs. It needs this because it cannot do autoregressive prediction for real-time inputs, as there will be additional tokens being added to the sequence while it is generating an output.</p><h2 id=model-outputs>Model Outputs<a hidden class=anchor aria-hidden=true href=#model-outputs>#</a></h2><p>GPT-4o features a dual-headed output system:</p><ol><li><p>Text and image output head:</p><ul><li>Predicts text and image tokens using standard one-hot encoding</li><li>Includes a &ldquo;No Output&rdquo; token</li></ul></li><li><p>Audio output head:</p><ul><li>Multiple sub-heads for each codebook in the audio tokenization</li><li>Also includes a &ldquo;No Output&rdquo; token</li></ul></li></ol><p>The &ldquo;No Output&rdquo; tokens are not passed back into the input and allow the model to:</p><ul><li>Choose which modality to reply with</li><li>Stay silent without consuming extra tokens</li><li>Respond with both text, image, and audio simultaneously</li></ul><p>This design gives GPT-4o the flexibility to generate appropriate responses across modalities.</p><h2 id=model-size>Model Size<a hidden class=anchor aria-hidden=true href=#model-size>#</a></h2><p>230 billion parameters. I made this number up, but here&rsquo;s a plausible justification:</p><ul><li>Llama 3.1 70B costs around $0.5 per million input tokens.</li><li>GPT-4o-2024-08-06 costs $2.50 per million input tokens.</li><li>Assume ClosedAI has pricing power due to the model&rsquo;s intelligence and brand, so they can charge around 50% more.</li></ul><p>Calculation: 70 * 2.5 / 0.5 / 1.5 ≈ 230</p><p>This size is also approximately equivalent to one GPT-4 expert, which was rumored to be 8x222B. This could explain why the cost is so much lower compared to GPT-4.</p><p>This is all just speculation, and I have no affiliation with ClosedAI.</p><h2 id=training>Training<a hidden class=anchor aria-hidden=true href=#training>#</a></h2><p>The training process for GPT-4o is complex and multi-staged, incorporating several advanced techniques to achieve its impressive multimodal and real-time capabilities.</p><h3 id=pretraining>Pretraining<a hidden class=anchor aria-hidden=true href=#pretraining>#</a></h3><p>During this stage, the temporal embeddings are set to the same value for all text and image samples and are equal to the position embedding for the audio samples. The source flag is also set to 0 for all tokens. Note that the audio and image tokenizers must be trained separately before beginning.</p><ol><li><p><strong>Interleaved Text and Image Pretraining</strong>:
Following an approach similar to the Chameleon model, GPT-4o is first pretrained on a large dataset of interleaved text and image data scraped from the web. This allows the model to develop a unified representation across these modalities.</p></li><li><p><strong>Audio Tokenizer Training</strong>:
After the initial pretraining, the audio embeddings are trained while the main model weights are frozen. The model performs autoregressive prediction on audio tokens, learning to understand and generate audio content. Data for this is sourced by scraping online podcasts and applying voice activity detection to filter the audio.</p></li><li><p><strong>Multimodal Unfrozen Training</strong>:
Once the audio tokenizer is sufficiently trained, all weights are unfrozen, and the model trains on all modalities simultaneously, further integrating its understanding across text, image, and audio. Note that audio samples still haven&rsquo;t been combined with the text and images at this stage, this will happen later. The output head that isn&rsquo;t used for a sample is trained to predict the &ldquo;No Output&rdquo; token.</p></li></ol><h3 id=fine-tuning-and-optimization>Fine-tuning and Optimization<a hidden class=anchor aria-hidden=true href=#fine-tuning-and-optimization>#</a></h3><ol><li><p><strong>Instruction Fine-tuning</strong>:
The model undergoes instruction fine-tuning to improve its ability to follow user instructions and perform specific tasks. The source flag is now set to 1 for tokens produced by the model.</p></li><li><p><strong>Direct Preference Optimization (DPO)</strong>:
Utilizing data from user sessions on the ChatGPT website, GPT-4o employs <a href=https://arxiv.org/abs/2305.18290>Direct Preference Optimization</a> to align its outputs with user preferences. This method allows for efficient optimization without the need for complex reinforcement learning algorithms, which are difficult to train and prone to over-fitting. The DPO approach used in GPT-4o enables the model to learn from human preferences by directly optimizing a policy to satisfy these preferences, using a simple binary cross-entropy objective.</p></li><li><p><strong>Chain-of-Thought Training</strong>:
GPT-4o employs a technique similar to the <a href=https://arxiv.org/abs/2405.14838>stepwise internalization method</a> described in recent research. This process helps the model internalize intermediate reasoning steps:</p><ul><li>The model is initially trained to generate explicit chain-of-thought reasoning steps.</li><li>Gradually, these intermediate steps are removed during training, forcing the model to internalize the reasoning process.</li><li>The training progresses through multiple stages, each removing more of the explicit reasoning steps.</li><li>By the final stage, the model can perform implicit chain-of-thought reasoning, producing high-quality outputs without generating explicit intermediate steps.</li></ul><p>This technique allows GPT-4o to reason effectively while maintaining the speed advantages of models that don&rsquo;t use explicit chain-of-thought.</p></li></ol><h3 id=real-time-multimodal-integration>Real-time Multimodal Integration<a hidden class=anchor aria-hidden=true href=#real-time-multimodal-integration>#</a></h3><p>Until now, all of the training has following the standard autoregressive setup. The final stage of training splits the input into the five channels focuses on integrating the various modalities in real-time:</p><ol><li><p><strong>Synthetic Multimodal Data Generation</strong>:
The base model generates scripts that simulate real-time interactions, including dialogue between users and the assistant, along with actions like &ldquo;Begin transcribing what I&rsquo;m saying&rdquo; or &ldquo;Describe what is happening in these images&rdquo;. This leverages the model&rsquo;s text and image generation capabilities to create scripts similar to what you might see in a TV show, along with supplemental images. Portions of the scripts are then converted to audio using an existing text-to-speech model.</p></li><li><p><strong>Real-time Processing Simulation</strong>:
The synthetic data is processed to simulate real-time inputs, with text, audio, and image inputs interleaved to mimic real-world scenarios. A system prompt is added with audio tokens from the target speaker to select a voice for the model.</p></li><li><p><strong>Final Integrated Training</strong>:
GPT-4o undergoes a final round of training on this synthetic real-time multimodal data, enhancing its ability to seamlessly integrate and respond to text, image, and audio inputs as they arrive, mimicking real-time interaction scenarios.</p></li></ol><h2 id=pytorch-implementation>Pytorch Implementation<a hidden class=anchor aria-hidden=true href=#pytorch-implementation>#</a></h2><p>Here&rsquo;s a PyTorch implementation, written by Claude 3.5 Sonnet:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tokenizers <span style=color:#f92672>import</span> SentencePieceBPETokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TransformerBlock</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    A single transformer block, consisting of multi-head attention and a feed-forward network.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, d_model, nhead):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>attention <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>MultiheadAttention(d_model, nhead)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>norm1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(d_model)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>norm2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(d_model)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>feed_forward <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(d_model, <span style=color:#ae81ff>4</span> <span style=color:#f92672>*</span> d_model),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>4</span> <span style=color:#f92672>*</span> d_model, d_model)
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x, attention_mask):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Multi-head attention</span>
</span></span><span style=display:flex><span>        attended, _ <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>attention(x, x, x, attn_mask<span style=color:#f92672>=</span>attention_mask)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Add &amp; Norm</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>norm1(x <span style=color:#f92672>+</span> attended)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Feed-forward network</span>
</span></span><span style=display:flex><span>        fed_forward <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>feed_forward(x)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Add &amp; Norm</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>norm2(x <span style=color:#f92672>+</span> fed_forward)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>VQVAEEncoder</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Vector Quantized Variational Autoencoder (VQ-VAE) Encoder for image tokenization.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, in_channels, hidden_dim, num_embeddings<span style=color:#f92672>=</span><span style=color:#ae81ff>65536</span>, embedding_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        <span style=color:#75715e># Encoder network to transform 768x768 image into 128x128 latent space</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>encoder <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv2d(in_channels, hidden_dim, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv2d(hidden_dim, hidden_dim, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv2d(hidden_dim, hidden_dim, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv2d(hidden_dim, embedding_dim, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#75715e># Codebook for vector quantization</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>codebook <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(num_embeddings, embedding_dim)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Encode the input image</span>
</span></span><span style=display:flex><span>        z <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>encoder(x)
</span></span><span style=display:flex><span>        z <span style=color:#f92672>=</span> z<span style=color:#f92672>.</span>permute(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>contiguous()
</span></span><span style=display:flex><span>        z_flattened <span style=color:#f92672>=</span> z<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, z<span style=color:#f92672>.</span>shape[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Compute distances to codebook vectors</span>
</span></span><span style=display:flex><span>        d <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sum(z_flattened <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, keepdim<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>) <span style=color:#f92672>+</span> \
</span></span><span style=display:flex><span>            torch<span style=color:#f92672>.</span>sum(self<span style=color:#f92672>.</span>codebook<span style=color:#f92672>.</span>weight <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>) <span style=color:#f92672>-</span> \
</span></span><span style=display:flex><span>            <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>matmul(z_flattened, self<span style=color:#f92672>.</span>codebook<span style=color:#f92672>.</span>weight<span style=color:#f92672>.</span>t())
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Find nearest codebook vectors</span>
</span></span><span style=display:flex><span>        min_encoding_indices <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>argmin(d, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        z_q <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>codebook(min_encoding_indices)<span style=color:#f92672>.</span>view(z<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> min_encoding_indices, z_q
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>VQVAEDecoder</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    VQ-VAE Decoder for image reconstruction from tokens.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, out_channels, hidden_dim, embedding_dim):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>decoder <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ConvTranspose2d(embedding_dim, hidden_dim, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ConvTranspose2d(hidden_dim, hidden_dim, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ConvTranspose2d(hidden_dim, hidden_dim, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ConvTranspose2d(hidden_dim, out_channels, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, z_q):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>decoder(z_q<span style=color:#f92672>.</span>permute(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>EncoderBlock</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Encoder block for the audio tokenizer, featuring residual connections and downsampling.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, in_channels, out_channels, stride):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>residual <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv1d(in_channels, out_channels, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ELU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv1d(out_channels, out_channels, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>downsample <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv1d(in_channels, out_channels, kernel_size<span style=color:#f92672>=</span>stride<span style=color:#f92672>*</span><span style=color:#ae81ff>2</span>, stride<span style=color:#f92672>=</span>stride, padding<span style=color:#f92672>=</span>stride<span style=color:#f92672>//</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>elu <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ELU()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>norm <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>utils<span style=color:#f92672>.</span>weight_norm(nn<span style=color:#f92672>.</span>Conv1d(out_channels, out_channels, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        residual <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>residual(x)
</span></span><span style=display:flex><span>        shortcut <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>downsample(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>elu(self<span style=color:#f92672>.</span>norm(residual <span style=color:#f92672>+</span> shortcut))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>AudioTokenizer</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Audio tokenizer using a series of convolutional layers, LSTM, and Residual Vector Quantization (RVQ).
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, input_dim, hidden_dim, num_codebooks, codebook_size):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>num_codebooks <span style=color:#f92672>=</span> num_codebooks
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>codebook_size <span style=color:#f92672>=</span> codebook_size
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Initial convolution</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>initial_conv <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv1d(input_dim, hidden_dim, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>7</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Encoder blocks with different strides</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>encoder_blocks <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ModuleList([
</span></span><span style=display:flex><span>            EncoderBlock(hidden_dim <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span><span style=color:#f92672>**</span>min(i, <span style=color:#ae81ff>3</span>), hidden_dim <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span><span style=color:#f92672>**</span>min(i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>), stride)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> i, stride <span style=color:#f92672>in</span> enumerate([<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>8</span>])
</span></span><span style=display:flex><span>        ])
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># LSTM for sequence modeling</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lstm <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LSTM(hidden_dim <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span>, hidden_dim <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span>, num_layers<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, batch_first<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Final convolution</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>final_conv <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv1d(hidden_dim <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span>, num_codebooks <span style=color:#f92672>*</span> codebook_size, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>7</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># RVQ codebooks</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>codebooks <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ModuleList([nn<span style=color:#f92672>.</span>Embedding(codebook_size, hidden_dim <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span>) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(num_codebooks)])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Initial convolution and activation</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>elu(self<span style=color:#f92672>.</span>initial_conv(x))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Pass through encoder blocks</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> block <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>encoder_blocks:
</span></span><span style=display:flex><span>            x <span style=color:#f92672>=</span> block(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># LSTM processing</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        x, _ <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>lstm(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Final convolution and reshaping</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>final_conv(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>contiguous()
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, self<span style=color:#f92672>.</span>num_codebooks, self<span style=color:#f92672>.</span>codebook_size)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Residual Vector Quantization</span>
</span></span><span style=display:flex><span>        indices <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        quantized <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        residual <span style=color:#f92672>=</span> x
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(self<span style=color:#f92672>.</span>num_codebooks):
</span></span><span style=display:flex><span>            <span style=color:#75715e># Find nearest codebook vector</span>
</span></span><span style=display:flex><span>            idx <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>argmin(torch<span style=color:#f92672>.</span>sum((residual<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>2</span>) <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>codebooks[i]<span style=color:#f92672>.</span>weight<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>), dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>            quant <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>codebooks[i](idx)
</span></span><span style=display:flex><span>            indices<span style=color:#f92672>.</span>append(idx)
</span></span><span style=display:flex><span>            quantized<span style=color:#f92672>.</span>append(quant)
</span></span><span style=display:flex><span>            residual <span style=color:#f92672>=</span> residual <span style=color:#f92672>-</span> quant
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        indices <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>stack(indices, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        quantized <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>stack(quantized, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>sum(dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> indices, quantized
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>GPT4o</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>class</span> <span style=color:#a6e22e>GPT4o</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, vocab_size, d_model, nhead, num_layers, num_codebooks, audio_vocab_size, image_vocab_size):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>d_model <span style=color:#f92672>=</span> d_model
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>vocab_size <span style=color:#f92672>=</span> vocab_size
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>audio_vocab_size <span style=color:#f92672>=</span> audio_vocab_size
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>image_vocab_size <span style=color:#f92672>=</span> image_vocab_size
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>num_codebooks <span style=color:#f92672>=</span> num_codebooks
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Text tokenizer</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>text_tokenizer <span style=color:#f92672>=</span> SentencePieceBPETokenizer()
</span></span><span style=display:flex><span>        <span style=color:#75715e># Note: In practice, you would train this tokenizer on your corpus separately</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># and save/load it, rather than training it here.</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># self.text_tokenizer.train(files=[&#34;path/to/your/corpus.txt&#34;], vocab_size=vocab_size)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Embeddings for different modalities</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>text_embedding <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(vocab_size, self<span style=color:#f92672>.</span>d_model)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>audio_embedding <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(audio_vocab_size <span style=color:#f92672>*</span> num_codebooks, self<span style=color:#f92672>.</span>d_model)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>image_embedding <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(image_vocab_size, self<span style=color:#f92672>.</span>d_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Audio tokenizer</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>audio_tokenizer <span style=color:#f92672>=</span> AudioTokenizer(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>32</span>, num_codebooks, audio_vocab_size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Image encoder (VQ-VAE)</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>image_encoder <span style=color:#f92672>=</span> VQVAEEncoder(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>256</span>, num_embeddings<span style=color:#f92672>=</span>image_vocab_size, embedding_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>image_decoder <span style=color:#f92672>=</span> VQVAEDecoder(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>32</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Positional and temporal encodings</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>pos_encoding <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(<span style=color:#ae81ff>5000</span>, self<span style=color:#f92672>.</span>d_model)  <span style=color:#75715e># Max 5000 positions</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>time_encoding <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(<span style=color:#ae81ff>1000</span>, self<span style=color:#f92672>.</span>d_model)  <span style=color:#75715e># Max 1000 time steps (250 seconds at 4 steps/sec)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Main transformer blocks</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>blocks <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ModuleList([TransformerBlock(self<span style=color:#f92672>.</span>d_model, nhead) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(num_layers)])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Output heads for text/image and audio</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>text_image_output <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>d_model, vocab_size <span style=color:#f92672>+</span> image_vocab_size <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)  <span style=color:#75715e># +1 for &#34;No Output&#34; token</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>audio_output <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ModuleList([
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>d_model, audio_vocab_size <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(num_codebooks)
</span></span><span style=display:flex><span>        ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>tokenize_text</span>(self, text):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Tokenize the input text</span>
</span></span><span style=display:flex><span>        encoded <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>text_tokenizer<span style=color:#f92672>.</span>encode(text)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>tensor(encoded<span style=color:#f92672>.</span>ids)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, text_input, audio_input, image_input, positions, times, flags, attention_mask):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Ensure all inputs are on the same device</span>
</span></span><span style=display:flex><span>        device <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>text_embedding<span style=color:#f92672>.</span>weight<span style=color:#f92672>.</span>device
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Tokenize and embed text inputs</span>
</span></span><span style=display:flex><span>        text_tokens <span style=color:#f92672>=</span> [self<span style=color:#f92672>.</span>tokenize_text(text)<span style=color:#f92672>.</span>to(device) <span style=color:#66d9ef>for</span> text <span style=color:#f92672>in</span> text_input]
</span></span><span style=display:flex><span>        text_tokens <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>utils<span style=color:#f92672>.</span>rnn<span style=color:#f92672>.</span>pad_sequence(text_tokens, batch_first<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        text_emb <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>text_embedding(text_tokens)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Tokenize and embed audio inputs</span>
</span></span><span style=display:flex><span>        audio_input <span style=color:#f92672>=</span> audio_input<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>        audio_tokens, audio_quantized <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>audio_tokenizer(audio_input)
</span></span><span style=display:flex><span>        audio_emb <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>audio_embedding(audio_tokens<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, self<span style=color:#f92672>.</span>num_codebooks <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>audio_vocab_size))
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Tokenize and embed image inputs</span>
</span></span><span style=display:flex><span>        image_embs <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> img <span style=color:#f92672>in</span> image_input:
</span></span><span style=display:flex><span>            img <span style=color:#f92672>=</span> img<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>            image_tokens, _ <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>image_encoder(img)
</span></span><span style=display:flex><span>            image_emb <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>image_embedding(image_tokens<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>16384</span>))  <span style=color:#75715e># 16384 tokens for 128x128 latent space</span>
</span></span><span style=display:flex><span>            image_embs<span style=color:#f92672>.</span>append(image_emb)
</span></span><span style=display:flex><span>        image_emb <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat(image_embs, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Ensure all embeddings have the same sequence length</span>
</span></span><span style=display:flex><span>        max_len <span style=color:#f92672>=</span> max(text_emb<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>], audio_emb<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>], image_emb<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>        text_emb <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>pad(text_emb, (<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, max_len <span style=color:#f92672>-</span> text_emb<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]))
</span></span><span style=display:flex><span>        audio_emb <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>pad(audio_emb, (<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, max_len <span style=color:#f92672>-</span> audio_emb<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]))
</span></span><span style=display:flex><span>        image_emb <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>pad(image_emb, (<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, max_len <span style=color:#f92672>-</span> image_emb<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Combine embeddings from all modalities</span>
</span></span><span style=display:flex><span>        combined_emb <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat([text_emb, audio_emb, image_emb], dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Add positional and temporal encodings</span>
</span></span><span style=display:flex><span>        pos_emb <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>pos_encoding(positions)
</span></span><span style=display:flex><span>        time_emb <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>time_encoding(times)
</span></span><span style=display:flex><span>        combined_emb <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat([combined_emb, pos_emb, time_emb], dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Add input/output flag to each token embedding</span>
</span></span><span style=display:flex><span>        flags <span style=color:#f92672>=</span> flags<span style=color:#f92672>.</span>unsqueeze(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>float()  <span style=color:#75715e># Shape: [batch_size, seq_len, 1]</span>
</span></span><span style=display:flex><span>        combined_emb <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat([combined_emb, flags], dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Pass through transformer blocks</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> block <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>blocks:
</span></span><span style=display:flex><span>            combined_emb <span style=color:#f92672>=</span> block(combined_emb, attention_mask)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Generate outputs for text/image and audio</span>
</span></span><span style=display:flex><span>        text_image_logits <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>text_image_output(combined_emb)
</span></span><span style=display:flex><span>        audio_logits <span style=color:#f92672>=</span> [head(combined_emb) <span style=color:#66d9ef>for</span> head <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>audio_output]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> text_image_logits, audio_logits
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_image</span>(self, image_tokens):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Convert image tokens back to embeddings</span>
</span></span><span style=display:flex><span>        z_q <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>image_embedding(image_tokens)<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>32</span>)  <span style=color:#75715e># 32 is the embedding_dim from VQVAEEncoder</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Decode the image</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>image_decoder(z_q)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>decode_text</span>(self, text_tokens):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Convert text tokens back to string</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>text_tokenizer<span style=color:#f92672>.</span>decode(text_tokens<span style=color:#f92672>.</span>tolist())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>decode_audio</span>(self, audio_tokens):
</span></span><span style=display:flex><span>        <span style=color:#75715e># This is a placeholder. In practice, you&#39;d need to implement</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># a method to convert audio tokens back to waveform.</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> audio_tokens
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Example usage</span>
</span></span><span style=display:flex><span>vocab_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>200019</span>
</span></span><span style=display:flex><span>d_model <span style=color:#f92672>=</span> <span style=color:#ae81ff>4096</span>
</span></span><span style=display:flex><span>nhead <span style=color:#f92672>=</span> <span style=color:#ae81ff>32</span>
</span></span><span style=display:flex><span>num_layers <span style=color:#f92672>=</span> <span style=color:#ae81ff>64</span>
</span></span><span style=display:flex><span>num_codebooks <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>audio_vocab_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>65536</span>
</span></span><span style=display:flex><span>image_vocab_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>65536</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Initialize the GPT-4o model</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> GPT4o(vocab_size, d_model, nhead, num_layers, num_codebooks, audio_vocab_size, image_vocab_size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create dummy inputs for demonstration</span>
</span></span><span style=display:flex><span>batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>seq_len <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>
</span></span><span style=display:flex><span>audio_frame_length <span style=color:#f92672>=</span> <span style=color:#ae81ff>6000</span>  <span style=color:#75715e># 250 ms at 24 kHz</span>
</span></span><span style=display:flex><span>num_audio_frames <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>  <span style=color:#75715e># 4 frames per second</span>
</span></span><span style=display:flex><span>num_images <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>  <span style=color:#75715e># Assuming one image per sequence, adjust as needed</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Text input (list of strings)</span>
</span></span><span style=display:flex><span>text_input <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;This is a sample text input&#34;</span> <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(batch_size)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Audio input (tensor)</span>
</span></span><span style=display:flex><span>audio_input <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(batch_size, <span style=color:#ae81ff>1</span>, audio_frame_length <span style=color:#f92672>*</span> num_audio_frames)  <span style=color:#75715e># 1 second of audio</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Image input (tensor)</span>
</span></span><span style=display:flex><span>image_input <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(batch_size, num_images, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>768</span>, <span style=color:#ae81ff>768</span>)  <span style=color:#75715e># Batch of RGB images</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Positions, times, and flags</span>
</span></span><span style=display:flex><span>positions <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>arange(seq_len)<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>expand(batch_size, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>times <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>arange(seq_len)<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>expand(batch_size, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>flags <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>, (batch_size, seq_len))  <span style=color:#75715e># 0 for input, 1 for output</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Attention mask</span>
</span></span><span style=display:flex><span>attention_mask <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tril(torch<span style=color:#f92672>.</span>ones(seq_len, seq_len))<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>expand(batch_size, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Forward pass through the model</span>
</span></span><span style=display:flex><span>text_image_logits, audio_logits <span style=color:#f92672>=</span> model(text_input, audio_input, image_input, positions, times, flags, attention_mask)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Print output shapes</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Text and Image logits shape:&#34;</span>, text_image_logits<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Audio logits shape:&#34;</span>, [logits<span style=color:#f92672>.</span>shape <span style=color:#66d9ef>for</span> logits <span style=color:#f92672>in</span> audio_logits])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Example of using generate_image</span>
</span></span><span style=display:flex><span>image_tokens <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, image_vocab_size, (batch_size, <span style=color:#ae81ff>16384</span>))
</span></span><span style=display:flex><span>generated_image <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>generate_image(image_tokens)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Generated image shape:&#34;</span>, generated_image<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Example of decoding text</span>
</span></span><span style=display:flex><span>text_tokens <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>argmax(text_image_logits[:, :, :vocab_size], dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>decoded_text <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>decode_text(text_tokens[<span style=color:#ae81ff>0</span>])  <span style=color:#75715e># Decode the first sequence in the batch</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Decoded text:&#34;</span>, decoded_text)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Example of decoding audio</span>
</span></span><span style=display:flex><span>audio_tokens <span style=color:#f92672>=</span> [torch<span style=color:#f92672>.</span>argmax(logits, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>) <span style=color:#66d9ef>for</span> logits <span style=color:#f92672>in</span> audio_logits]
</span></span><span style=display:flex><span>decoded_audio <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>decode_audio(audio_tokens)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Decoded audio shape:&#34;</span>, [tokens<span style=color:#f92672>.</span>shape <span style=color:#66d9ef>for</span> tokens <span style=color:#f92672>in</span> decoded_audio])
</span></span></code></pre></div><p>If you&rsquo;ve made it this far and somehow have access to the ungodly number of GPUs required to train this monster, hit me up and let&rsquo;s give ClosedAI a run for their money.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://boggs.tech/>Jake Boggs</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>