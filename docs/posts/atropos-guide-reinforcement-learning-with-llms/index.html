<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=author content="Jake Boggs"><title>Scalable Reinforcement Learning with LLMs - Atropos Guide | Jake Boggs</title><link rel=stylesheet href=/css/main.min.css><link rel=icon href=/favicon.png><link rel=canonical href=https://boggs.tech/posts/atropos-guide-reinforcement-learning-with-llms/></head><body><header><div class=container><a href=https://boggs.tech/ class=site-title>Jake Boggs</a><nav><a href=/about>About me</a><a href=/startups>Startups</a></nav></div></header><main><div class=container><article><h1>Scalable Reinforcement Learning with LLMs - Atropos Guide</h1><div class=article-meta><span>May 16, 2025</span>
<span>9 min read</span></div><div class=article-content><p>This weekend, I will be in attendance at the <a href=https://cerebralvalley.ai/e/nous-research-rl-environments-hackathon-9be3062a>Nous Research â€“ RL Environments Hackathon</a>, so to prepare I&rsquo;ve been playing around with Atropos, their new RL framework that we will be using for the event. After failing to find any guides online, I decided to write my own.</p><blockquote><p><strong>Update:</strong> I got 2nd place with VR-CLImax, my implementation of Verified Rewards via Completion Likelihood Improvement, an RL environment for teaching LLMs how to make jokes! You can find the code merged into the <a href=https://github.com/NousResearch/atropos/tree/main/environments/community/punchline_vrcli>Atropos repository</a>.</p></blockquote><h2 id=what-is-atropos>What is Atropos?</h2><p>Atropos is a library from <a href=https://nousresearch.com/>Nous Research</a> for performing reinforcement learning with LLMs. It provides a framework for managing environments and collecting rollouts. The training process is broken into four main components (environments, inference, trainer, orchestration), each running separately, enabling them to be distributed across multiple machines.</p><p>An example of this might look like training a coding agent, where the training script (handling loss calculations, backpropagation, and weight updates) happens on a powerful GPU cluster, while multiple code execution environments run on smaller CPU nodes. The orchestration server manages the communication between them, collecting rollouts (which could be a code update and its execution results in this example) from the environments and batching them to send to the trainer.</p><p>The recommended configuration provided in the repository uses vLLM running inside the trainer process. The environments can then query this vLLM instance when generating rollouts. These rollouts occur asynchronously, with the results passed to the orchestration server after a rollout is complete. Periodically (e.g. every few training steps), the vLLM server is restarted so that it uses the latest set of model weights from the trainer.</p><p><img src=/images/atropos.png alt="Atropos Diagram"></p><h2 id=why-do-we-need-this>Why do we need this?</h2><p>There are already plenty of reinforcement learning libraries such as <a href=https://huggingface.co/docs/trl/en/index>TRL</a>, but the structure of Atropos makes it particularly useful for large training setups.</p><ul><li><strong>Scalability and Efficiency:</strong> By separating the environments, inference, and training, each component can be scaled independently. For example, you can run numerous environments in parallel on cost-effective CPU instances and maximize the utilization of your inference hardware. This distributed approach and significantly speeds up the data collection and training cycle.</li><li><strong>Flexibility:</strong> Atropos allows for heterogeneous hardware setups. Environments can run on different operating systems or hardware configurations without impacting the trainer.</li><li><strong>Real-World Use Cases:</strong><ul><li><strong>Coding Agents:</strong> As mentioned earlier, training an AI to write or debug code can involve numerous sandboxed execution environments. Atropos can manage these environments, collect the outcomes of code execution (success, failure, errors), and feed this data back to the trainer.</li><li><strong>Game AI:</strong> Developing AI for complex games can require simulating many game instances simultaneously. Each game instance acts as an environment, and Atropos can orchestrate the collection of gameplay data (actions, states, rewards).</li><li><strong>Robotics:</strong> Training robots often involves physical or simulated environments. Atropos can help manage these diverse environments, allowing for parallel data collection from multiple robots or simulations.</li></ul></li></ul><p>To drive continued performance gains, future models are likely to spend a larger portion of their compute budget on RL instead of pretraining. This will necessitate scalable frameworks.</p><h2 id=environment-setup>Environment Setup</h2><p>Create a python environment and install dependencies:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python -m venv atropos
</span></span><span style=display:flex><span>source atropos/bin/activate
</span></span><span style=display:flex><span>pip install <span style=color:#e6db74>&#34;vllm&gt;=0.8.5&#34;</span> torch transformers datasets wandb tenacity atroposlib pydantic
</span></span></code></pre></div><h2 id=code-structure>Code Structure</h2><p>We&rsquo;ll go over the GSM8K example from the <a href=https://github.com/NousResearch/atropos>Atropos repository</a>. At the time of writing, there are a couple of small bugs that you will need to fix if you plan to run them yourself.</p><ol><li><p>In <code>trainer.py</code>, the wandb variables need to be empty strings and not <code>None</code> if you are not using wandb, otherwise the script will error:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>wandb_project: Optional[str] <span style=color:#f92672>=</span> Field(<span style=color:#e6db74>&#34;&#34;</span>, description<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Wandb project name&#34;</span>)
</span></span><span style=display:flex><span>wandb_group: Optional[str] <span style=color:#f92672>=</span> Field(<span style=color:#e6db74>&#34;&#34;</span>, description<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Wandb group name&#34;</span>)
</span></span></code></pre></div></li><li><p>Again in <code>trainer.py</code>, you need to set <code>model.config.use_cache = False</code> to support the gradient accumulation.</p></li></ol><p>With that out of the way, here are the important files and functions:</p><h3 id=gsm8k_environmentpy><a href=https://github.com/NousResearch/atropos/blob/main/environments/gsm8k_server.py><code>gsm8k_environment.py</code></a></h3><p>This script defines the <code>GSM8kEnv</code> class, which is responsible for interacting with the GSM8k dataset, generating prompts, collecting model completions, and scoring them.</p><p><strong>Key Components:</strong></p><ol><li><p><strong><code>GSM8kEnv(BaseEnv)</code></strong>:</p><ul><li>Inherits from <code>BaseEnv</code> in the <code>atroposlib</code>.</li><li>Manages the GSM8k environment, including data loading, interaction with the LLM server for completions, and scoring.</li><li>Handles wandb logging for metrics like percent correct.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>GSM8kEnv</span>(BaseEnv):
</span></span><span style=display:flex><span>    name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;gsm8k&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(
</span></span><span style=display:flex><span>        self,
</span></span><span style=display:flex><span>        config: BaseEnvConfig,
</span></span><span style=display:flex><span>        server_configs: List[APIServerConfig],
</span></span><span style=display:flex><span>        slurm<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        testing<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>    ):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>(config, server_configs, slurm, testing)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>percent_correct_buffer <span style=color:#f92672>=</span> list()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>eval_metrics <span style=color:#f92672>=</span> list()
</span></span><span style=display:flex><span>        <span style=color:#75715e># Add tracking for wandb visualizations</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>rollouts_for_wandb <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>completion_lengths <span style=color:#f92672>=</span> []
</span></span></code></pre></div></li><li><p><strong><code>config_init()</code></strong>:</p><ul><li>A class method to define default configurations for the environment (<code>BaseEnvConfig</code>) and the API server(s) (<code>APIServerConfig</code>) it interacts with. This includes tokenizer name, batch sizes, wandb settings, and model details for the inference server.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@classmethod</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>config_init</span>(cls) <span style=color:#f92672>-&gt;</span> Tuple[BaseEnvConfig, List[APIServerConfig]]:
</span></span><span style=display:flex><span>    env_config <span style=color:#f92672>=</span> BaseEnvConfig(
</span></span><span style=display:flex><span>        tokenizer_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;NousResearch/DeepHermes-3-Llama-3-3B-Preview&#34;</span>,
</span></span><span style=display:flex><span>        group_size<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>,
</span></span><span style=display:flex><span>        use_wandb<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        rollout_server_url<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;http://localhost:8000&#34;</span>,
</span></span><span style=display:flex><span>        total_steps<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>,
</span></span><span style=display:flex><span>        batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>12</span>,
</span></span><span style=display:flex><span>        steps_per_eval<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>,
</span></span><span style=display:flex><span>        max_token_length<span style=color:#f92672>=</span><span style=color:#ae81ff>2048</span>,
</span></span><span style=display:flex><span>        wandb_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;gsm8k&#34;</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    server_configs <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        APIServerConfig(
</span></span><span style=display:flex><span>            model_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;NousResearch/DeepHermes-3-Llama-3-3B-Preview&#34;</span>,
</span></span><span style=display:flex><span>            base_url<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;http://localhost:9001/v1&#34;</span>, <span style=color:#75715e># Points to the vLLM server started by trainer.py</span>
</span></span><span style=display:flex><span>            api_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;x&#34;</span>, <span style=color:#75715e># Placeholder, as vLLM by default doesn&#39;t require an API key</span>
</span></span><span style=display:flex><span>            num_requests_for_eval<span style=color:#f92672>=</span><span style=color:#ae81ff>256</span>,
</span></span><span style=display:flex><span>        ),
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> env_config, server_configs
</span></span></code></pre></div></li><li><p><strong><code>setup()</code></strong>:</p><ul><li>Loads and preprocesses the GSM8k dataset (train and test splits).</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>setup</span>(self):
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>train <span style=color:#f92672>=</span> load_dataset(<span style=color:#e6db74>&#34;gsm8k&#34;</span>, <span style=color:#e6db74>&#34;main&#34;</span>, split<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;train&#34;</span>)<span style=color:#f92672>.</span>shuffle(seed<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>    test_data <span style=color:#f92672>=</span> load_dataset(<span style=color:#e6db74>&#34;gsm8k&#34;</span>, <span style=color:#e6db74>&#34;main&#34;</span>, split<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;test&#34;</span>)<span style=color:#f92672>.</span>shuffle(seed<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>test <span style=color:#f92672>=</span> list()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> test_data:
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>test<span style=color:#f92672>.</span>append(
</span></span><span style=display:flex><span>            {
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;question&#34;</span>: item[<span style=color:#e6db74>&#34;question&#34;</span>],
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;gold_answer&#34;</span>: item[<span style=color:#e6db74>&#34;answer&#34;</span>]
</span></span><span style=display:flex><span>                <span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;#&#34;</span>)[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>                <span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>                <span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#34;,&#34;</span>, <span style=color:#e6db74>&#34;&#34;</span>),
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>iter <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span></code></pre></div></li><li><p><strong><code>collect_trajectories(item: GSM8kRow)</code></strong>:</p><ul><li>Takes a data item (question and answer).</li><li>Formats the prompt.</li><li>Sends requests to vLLM to get <code>n</code> completions (rollouts) for the question.</li><li>Prepares the data for scoring.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>collect_trajectories</span>(
</span></span><span style=display:flex><span>    self, item: GSM8kRow
</span></span><span style=display:flex><span>) <span style=color:#f92672>-&gt;</span> Tuple[ScoredDataGroup, list[Item]]:
</span></span><span style=display:flex><span>    user_message <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: item[<span style=color:#e6db74>&#34;question&#34;</span>]}
</span></span><span style=display:flex><span>    gold_answer <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\b</span><span style=color:#e6db74>oxed{&#34;</span> <span style=color:#f92672>+</span> item[<span style=color:#e6db74>&#34;answer&#34;</span>]<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;#&#34;</span>)[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>strip()<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#34;,&#34;</span>, <span style=color:#e6db74>&#34;&#34;</span>) <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;}&#34;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    chat_completions <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>server<span style=color:#f92672>.</span>chat_completion(
</span></span><span style=display:flex><span>        messages<span style=color:#f92672>=</span>[{<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: system_prompt}, user_message],
</span></span><span style=display:flex><span>        n<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>config<span style=color:#f92672>.</span>group_size, <span style=color:#75715e># Number of completions to generate</span>
</span></span><span style=display:flex><span>        max_tokens<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>config<span style=color:#f92672>.</span>max_token_length,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#75715e># ... prepares data for scoring ...</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> to_postprocess, to_backlog
</span></span></code></pre></div></li><li><p><strong><code>score(rollout_group_data)</code></strong>:</p><ul><li>Takes a group of rollouts.</li><li>Parses the generated answers and the gold answer using <code>latex2sympy2_extended</code> and <code>math_verify</code>.</li><li>Assigns a reward (1.0 for correct, -1.0 for incorrect).</li><li>Tokenizes the messages for the trainer.</li><li>Applies a length penalty if all answers in a group are correct, to encourage conciseness.</li><li>Returns <code>None</code> if all scores are identical (e.g., all correct or all incorrect) to avoid sending uninformative data to the trainer, or if the gold solution is unparseable.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>score</span>(
</span></span><span style=display:flex><span>    self, rollout_group_data
</span></span><span style=display:flex><span>) <span style=color:#f92672>-&gt;</span> Union[Optional[ScoredDataGroup], List[Optional[ScoredDataGroup]]]:
</span></span><span style=display:flex><span>    <span style=color:#75715e># ... parsing and verification logic ...</span>
</span></span><span style=display:flex><span>    reward <span style=color:#f92672>=</span> verify(answer_parsed, gold_parsed)
</span></span><span style=display:flex><span>    <span style=color:#75715e># ... tokenization ...</span>
</span></span><span style=display:flex><span>    scores[<span style=color:#e6db74>&#34;scores&#34;</span>]<span style=color:#f92672>.</span>append(<span style=color:#ae81ff>1.0</span> <span style=color:#66d9ef>if</span> reward <span style=color:#66d9ef>else</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1.0</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># ... length penalty logic ...</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> all([scores[<span style=color:#e6db74>&#34;scores&#34;</span>][<span style=color:#ae81ff>0</span>] <span style=color:#f92672>==</span> score <span style=color:#66d9ef>for</span> score <span style=color:#f92672>in</span> scores[<span style=color:#e6db74>&#34;scores&#34;</span>]]):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>None</span> <span style=color:#75715e># If all the same, we return None</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> scores
</span></span></code></pre></div></li><li><p><strong><code>get_next_item()</code></strong>:</p><ul><li>Provides the next training item from the dataset.</li></ul></li></ol><h3 id=trainerpy><a href=https://github.com/NousResearch/atropos/blob/main/example_trainer/grpo.py><code>trainer.py</code></a></h3><p>This script is responsible for the actual model training process. It initializes the model and tokenizer, sets up the optimizer, fetches data batches from the orchestration server (which gets them from <code>gsm8k_environment.py</code>), performs the training steps, and manages the vLLM inference server.</p><p><strong>Key Components:</strong></p><ol><li><p><strong><code>TrainingConfig(BaseModel)</code></strong>:</p><ul><li>A Pydantic model defining all necessary configurations for training, such as model name, learning rate, batch size, sequence length, device, save paths, and vLLM specific settings.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TrainingConfig</span>(BaseModel):
</span></span><span style=display:flex><span>    model_name: str <span style=color:#f92672>=</span> Field(<span style=color:#f92672>...</span>, description<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Name of the base model to train&#34;</span>)
</span></span><span style=display:flex><span>    lr: float <span style=color:#f92672>=</span> Field(<span style=color:#ae81ff>1e-5</span>, description<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Learning rate for the optimizer&#34;</span>)
</span></span><span style=display:flex><span>    training_steps: int <span style=color:#f92672>=</span> Field(<span style=color:#ae81ff>10</span>, description<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Number of training steps&#34;</span>)
</span></span><span style=display:flex><span>    batch_size: int <span style=color:#f92672>=</span> Field(<span style=color:#ae81ff>2</span>, description<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Batch size for training&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># ... other fields ...</span>
</span></span><span style=display:flex><span>    vllm_port: int <span style=color:#f92672>=</span> Field(<span style=color:#ae81ff>9001</span>, description<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Port for the vLLM server&#34;</span>)
</span></span><span style=display:flex><span>    use_wandb: bool <span style=color:#f92672>=</span> Field(<span style=color:#66d9ef>False</span>, description<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Whether to use Weights &amp; Biases for logging&#34;</span>)
</span></span></code></pre></div></li><li><p><strong><code>register_trainer(config: TrainingConfig)</code></strong>:</p><ul><li>Sends a POST request to the orchestration server (<code>http://localhost:8000/register</code>) to register itself, providing its configuration details. This allows the orchestration server to know about the trainer and its requirements.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@retry</span>(stop<span style=color:#f92672>=</span>stop_after_attempt(<span style=color:#ae81ff>3</span>), wait<span style=color:#f92672>=</span>wait_exponential(multiplier<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, min<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, max<span style=color:#f92672>=</span><span style=color:#ae81ff>15</span>))
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>register_trainer</span>(config: TrainingConfig):
</span></span><span style=display:flex><span>    requests<span style=color:#f92672>.</span>post(
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;http://localhost:8000/register&#34;</span>,
</span></span><span style=display:flex><span>        json<span style=color:#f92672>=</span>{
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;wandb_group&#34;</span>: config<span style=color:#f92672>.</span>wandb_group,
</span></span><span style=display:flex><span>            <span style=color:#75715e># ... other registration details ...</span>
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;num_steps&#34;</span>: config<span style=color:#f92672>.</span>training_steps,
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        timeout<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>,
</span></span><span style=display:flex><span>    )
</span></span></code></pre></div></li><li><p><strong><code>get_data(batch_size: int, seq_len: int)</code></strong>:</p><ul><li>Continuously polls the orchestration server (<code>http://localhost:8000/batch</code>) for new batches of data.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_data</span>(
</span></span><span style=display:flex><span>    batch_size: int, seq_len: int
</span></span><span style=display:flex><span>) <span style=color:#f92672>-&gt;</span> List[Tuple[List[torch<span style=color:#f92672>.</span>Tensor], List[torch<span style=color:#f92672>.</span>Tensor], List[torch<span style=color:#f92672>.</span>Tensor]]]:
</span></span><span style=display:flex><span>    batches <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> get_batch() <span style=color:#75715e># Fetches from http://localhost:8000/batch</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> data[<span style=color:#e6db74>&#34;batch&#34;</span>] <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            batches<span style=color:#f92672>.</span>append(pad_data_to_good_offset(data, batch_size))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> len(batches) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> batches
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            time<span style=color:#f92672>.</span>sleep(<span style=color:#ae81ff>1</span>)
</span></span></code></pre></div></li><li><p><strong><code>train(config: TrainingConfig)</code></strong>:</p><ul><li><strong>Initialization</strong>:<ul><li>Sets up Weights & Biases (wandb) if configured.</li><li>Loads the tokenizer and model from Hugging Face (<code>AutoTokenizer</code>, <code>AutoModelForCausalLM</code>).</li><li>Sets up the AdamW optimizer.</li><li>Registers the trainer with the orchestration server.</li></ul></li><li><strong>vLLM Management</strong>:<ul><li>Launches an initial vLLM server instance as a subprocess using the base model.</li><li>The <code>vllm_process</code> global variable tracks this subprocess.</li></ul></li><li><strong>Training Loop</strong>:<ul><li>Iterates for <code>config.training_steps</code>.</li><li>Fetches data using <code>get_data()</code>.</li><li>For each batch:<ul><li>Performs a forward pass through the model.</li><li>Calculates the GRPO loss. The loss encourages actions with positive advantages and discourages those with negative advantages.</li><li>Performs backpropagation and optimizer step.</li><li>Logs metrics (loss, learning rate, gradient norm, log probabilities) to the console and wandb.</li></ul></li><li><strong>vLLM Restart and Checkpointing</strong>:<ul><li>Periodically (defined by <code>config.vllm_restart_interval</code>) or on the last step:<ul><li>Saves a model checkpoint (weights and tokenizer).</li><li>Terminates the current vLLM process.</li><li>Launches a new vLLM process using the <em>newly saved checkpoint</em>, allowing the environment to use the updated model for subsequent rollouts.</li></ul></li></ul></li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train</span>(config: TrainingConfig):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>global</span> vllm_process
</span></span><span style=display:flex><span>    <span style=color:#75715e># ... Wandb Setup, Model &amp; Optimizer Init ...</span>
</span></span><span style=display:flex><span>    register_trainer(config)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Init vLLM with base model</span>
</span></span><span style=display:flex><span>    vllm_command <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;python&#34;</span>, <span style=color:#e6db74>&#34;-m&#34;</span>, <span style=color:#e6db74>&#34;vllm.entrypoints.openai.api_server&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;--model&#34;</span>, config<span style=color:#f92672>.</span>model_name,
</span></span><span style=display:flex><span>        <span style=color:#75715e># ... other vLLM args ...</span>
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    vllm_process <span style=color:#f92672>=</span> subprocess<span style=color:#f92672>.</span>Popen(vllm_command)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> step <span style=color:#f92672>in</span> range(config<span style=color:#f92672>.</span>training_steps):
</span></span><span style=display:flex><span>        <span style=color:#75715e># ... fetch data ...</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># ... training step, loss calculation, optimizer.step() ...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (step <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>%</span> config<span style=color:#f92672>.</span>vllm_restart_interval <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>or</span> step <span style=color:#f92672>==</span> config<span style=color:#f92672>.</span>training_steps <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>:
</span></span><span style=display:flex><span>            checkpoint_path <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(config<span style=color:#f92672>.</span>save_path, <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;step_</span><span style=color:#e6db74>{</span>step<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>            model<span style=color:#f92672>.</span>save_pretrained(checkpoint_path)
</span></span><span style=display:flex><span>            tokenizer<span style=color:#f92672>.</span>save_pretrained(checkpoint_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Terminate existing vLLM</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> vllm_process:
</span></span><span style=display:flex><span>                vllm_process<span style=color:#f92672>.</span>terminate()
</span></span><span style=display:flex><span>                vllm_process<span style=color:#f92672>.</span>wait()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Launch new vLLM with updated model</span>
</span></span><span style=display:flex><span>            updated_vllm_command <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;python&#34;</span>, <span style=color:#e6db74>&#34;-m&#34;</span>, <span style=color:#e6db74>&#34;vllm.entrypoints.openai.api_server&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;--model&#34;</span>, checkpoint_path, <span style=color:#75715e># Use the new checkpoint</span>
</span></span><span style=display:flex><span>                <span style=color:#75715e># ... other vLLM args ...</span>
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>            vllm_process <span style=color:#f92672>=</span> subprocess<span style=color:#f92672>.</span>Popen(updated_vllm_command)
</span></span><span style=display:flex><span>    <span style=color:#75715e># ... final save and cleanup ...</span>
</span></span></code></pre></div></li></ol><h2 id=training-process>Training Process</h2><p>Once you have installed the dependencies, download the <a href=https://github.com/NousResearch/atropos/blob/main/environments/gsm8k_server.py>environment</a> and <a href=https://github.com/NousResearch/atropos/blob/main/example_trainer/grpo.py>training</a> scripts (you might need to rename them), then complete the steps below:</p><ol><li><p><strong>Start Orchestration Server</strong>:
Open a terminal, activate your environment, and run:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir empty
</span></span><span style=display:flex><span>cd empty
</span></span><span style=display:flex><span>run-api
</span></span></code></pre></div><p>This server listens on <code>http://localhost:8000</code> and coordinates between the environment and trainer. We&rsquo;re doing this inside an empty folder because Atropos listens for files changes for some reason and will restart the server every time a checkpoint is saved, breaking the training process.</p></li><li><p><strong>Start Environment</strong>:
Open another terminal, activate the environment, and run:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python gsm8k_environment.py serve --slurm false
</span></span></code></pre></div><p>The environment will attempt to connect to the orchestration server. Initially, it might wait if the trainer hasn&rsquo;t registered yet.</p></li><li><p><strong>Start Trainer</strong>:
Open a third terminal, activate the environment, and run:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python trainer.py
</span></span></code></pre></div><ul><li>The <code>trainer.py</code> script will:<ul><li>Initialize a model (e.g., &ldquo;Qwen/Qwen2.5-1.5B-Instruct&rdquo;).</li><li>Start a vLLM server instance using this base model on a specified port (default 9001).</li><li>Register itself with the orchestration server.</li></ul></li><li>Once the trainer registers, the environment(s) will start generating rollouts using the vLLM server managed by the trainer.</li><li>The environment sends scored rollouts to the orchestration server.</li><li>The trainer fetches these rollouts from the orchestration server, performs training steps, and updates its model weights.</li><li>Periodically, the trainer saves a checkpoint and restarts the vLLM server with the <em>updated</em> model weights, allowing the environment to benefit from the training progress.</li></ul></li></ol><p>This setup allows for a decoupled system where data generation (environment) and model training (trainer) can happen independently and potentially on different hardware, coordinated by the orchestration server.</p><p>The project is actively being developed, so I would not be surprised if this guide quickly becomes outdated. Hopefully it&rsquo;s at least useful for someone else at the hackathon. See you all there!</p><p>Twitter: <a href=https://x.com/JakeABoggs>@JakeABoggs</a></p></div></article></div></main><footer><div class=container><p>&copy; 2026 Jake Boggs</p></div></footer></body></html>