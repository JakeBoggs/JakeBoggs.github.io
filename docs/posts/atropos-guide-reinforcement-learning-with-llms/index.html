<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Scalable Reinforcement Learning with LLMs - Atropos Guide | Jake Boggs</title>
<meta name=keywords content><meta name=description content="This weekend, I will be in attendance at the Nous Research – RL Environments Hackathon, so to prepare I&rsquo;ve been playing around with Atropos, their new RL framework that we will be using for the event. After failing to find any guides online, I decided to write my own.
What is Atropos? Atropos is a library from Nous Research for performing reinforcement learning with LLMs. It provides a framework for managing environments and collecting rollouts."><meta name=author content="Jake Boggs"><link rel=canonical href=https://boggs.tech/posts/atropos-guide-reinforcement-learning-with-llms/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://boggs.tech/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://boggs.tech/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://boggs.tech/favicon-32x32.png><link rel=apple-touch-icon href=https://boggs.tech/apple-touch-icon.png><link rel=mask-icon href=https://boggs.tech/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://boggs.tech/posts/atropos-guide-reinforcement-learning-with-llms/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Scalable Reinforcement Learning with LLMs - Atropos Guide"><meta property="og:description" content="This weekend, I will be in attendance at the Nous Research – RL Environments Hackathon, so to prepare I&rsquo;ve been playing around with Atropos, their new RL framework that we will be using for the event. After failing to find any guides online, I decided to write my own.
What is Atropos? Atropos is a library from Nous Research for performing reinforcement learning with LLMs. It provides a framework for managing environments and collecting rollouts."><meta property="og:type" content="article"><meta property="og:url" content="https://boggs.tech/posts/atropos-guide-reinforcement-learning-with-llms/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-16T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-16T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Scalable Reinforcement Learning with LLMs - Atropos Guide"><meta name=twitter:description content="This weekend, I will be in attendance at the Nous Research – RL Environments Hackathon, so to prepare I&rsquo;ve been playing around with Atropos, their new RL framework that we will be using for the event. After failing to find any guides online, I decided to write my own.
What is Atropos? Atropos is a library from Nous Research for performing reinforcement learning with LLMs. It provides a framework for managing environments and collecting rollouts."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://boggs.tech/posts/"},{"@type":"ListItem","position":2,"name":"Scalable Reinforcement Learning with LLMs - Atropos Guide","item":"https://boggs.tech/posts/atropos-guide-reinforcement-learning-with-llms/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Scalable Reinforcement Learning with LLMs - Atropos Guide","name":"Scalable Reinforcement Learning with LLMs - Atropos Guide","description":"This weekend, I will be in attendance at the Nous Research – RL Environments Hackathon, so to prepare I\u0026rsquo;ve been playing around with Atropos, their new RL framework that we will be using for the event. After failing to find any guides online, I decided to write my own.\nWhat is Atropos? Atropos is a library from Nous Research for performing reinforcement learning with LLMs. It provides a framework for managing environments and collecting rollouts.","keywords":[],"articleBody":"This weekend, I will be in attendance at the Nous Research – RL Environments Hackathon, so to prepare I’ve been playing around with Atropos, their new RL framework that we will be using for the event. After failing to find any guides online, I decided to write my own.\nWhat is Atropos? Atropos is a library from Nous Research for performing reinforcement learning with LLMs. It provides a framework for managing environments and collecting rollouts. The training process is broken into four main components (environments, inference, trainer, orchestration), each running separately, enabling them to be distributed across multiple machines.\nAn example of this might look like training a coding agent, where the training script (handling backpropagation, loss calculations, and weight updates) happens on a powerful GPU cluster, while multiple code execution environments run on smaller CPU nodes. The orchestration server manages the communication between them, collecting rollouts (which could be a code update and its execution results in this example) from the environments and batching them to send to the trainer.\nThe recommended configuration provided in the repository uses vLLM running inside the trainer process. The environments can then query this vLLM instance when generating rollouts. These rollouts occur asynchronously, with the results passed to the orchestration server after a rollout is complete. Periodically (e.g. every few training steps), the vLLM server is restarted so that it uses the latest set of model weights from the trainer.\nWhy do we need this? There are already plenty of reinforcement learning libraries such as TRL, but the structure of Atropos makes it particularly useful for large training setups.\nScalability and Efficiency: By separating the environments, inference, and training, each component can be scaled independently. For example, you can run numerous environments in parallel on cost-effective CPU instances and maximize the utilization of your inference hardware. This distributed approach and significantly speeds up the data collection and training cycle. Flexibility: Atropos allows for heterogeneous hardware setups. Environments can run on different operating systems or hardware configurations without impacting the trainer. Real-World Use Cases: Coding Agents: As mentioned earlier, training an AI to write or debug code can involve numerous sandboxed execution environments. Atropos can manage these environments, collect the outcomes of code execution (success, failure, errors), and feed this data back to the trainer. Game AI: Developing AI for complex games can require simulating many game instances simultaneously. Each game instance acts as an environment, and Atropos can orchestrate the collection of gameplay data (actions, states, rewards). Robotics: Training robots often involves physical or simulated environments. Atropos can help manage these diverse environments, allowing for parallel data collection from multiple robots or simulations. To drive continued performance gains, future models are likely to spend a larger portion of their compute budget on RL instead of pretraining. This will necessitate scalable frameworks.\nEnvironment Setup Create a python environment and install dependencies:\npython -m venv atropos source atropos/bin/activate pip install \"vllm\u003e=0.8.5\" torch transformers datasets wandb tenacity atroposlib pydantic Code Structure We’ll go over the GSM8K example from the Atropos repository. At the time of writing, there are a couple of small bugs that you will need to fix if you plan to run them yourself.\nIn trainer.py, the wandb variables need to be empty strings and not None if you are not using wandb, otherwise the script will error:\nwandb_project: Optional[str] = Field(\"\", description=\"Wandb project name\") wandb_group: Optional[str] = Field(\"\", description=\"Wandb group name\") Again in trainer.py, you need to set model.config.use_cache = False to support the gradient accumulation.\nWith that out of the way, here are the important files and functions:\ngsm8k_environment.py This script defines the GSM8kEnv class, which is responsible for interacting with the GSM8k dataset, generating prompts, collecting model completions, and scoring them.\nKey Components:\nGSM8kEnv(BaseEnv):\nInherits from BaseEnv in the atroposlib. Manages the GSM8k environment, including data loading, interaction with the LLM server for completions, and scoring. Handles wandb logging for metrics like percent correct. class GSM8kEnv(BaseEnv): name = \"gsm8k\" def __init__( self, config: BaseEnvConfig, server_configs: List[APIServerConfig], slurm=True, testing=False, ): super().__init__(config, server_configs, slurm, testing) self.percent_correct_buffer = list() self.eval_metrics = list() # Add tracking for wandb visualizations self.rollouts_for_wandb = [] self.completion_lengths = [] config_init():\nA class method to define default configurations for the environment (BaseEnvConfig) and the API server(s) (APIServerConfig) it interacts with. This includes tokenizer name, batch sizes, wandb settings, and model details for the inference server. @classmethod def config_init(cls) -\u003e Tuple[BaseEnvConfig, List[APIServerConfig]]: env_config = BaseEnvConfig( tokenizer_name=\"NousResearch/DeepHermes-3-Llama-3-3B-Preview\", group_size=8, use_wandb=True, rollout_server_url=\"http://localhost:8000\", total_steps=1000, batch_size=12, steps_per_eval=100, max_token_length=2048, wandb_name=\"gsm8k\", ) server_configs = [ APIServerConfig( model_name=\"NousResearch/DeepHermes-3-Llama-3-3B-Preview\", base_url=\"http://localhost:9001/v1\", # Points to the vLLM server started by trainer.py api_key=\"x\", # Placeholder, as vLLM by default doesn't require an API key num_requests_for_eval=256, ), ] return env_config, server_configs setup():\nLoads and preprocesses the GSM8k dataset (train and test splits). async def setup(self): self.train = load_dataset(\"gsm8k\", \"main\", split=\"train\").shuffle(seed=42) test_data = load_dataset(\"gsm8k\", \"main\", split=\"test\").shuffle(seed=42) self.test = list() for item in test_data: self.test.append( { \"question\": item[\"question\"], \"gold_answer\": item[\"answer\"] .split(\"#\")[-1] .strip() .replace(\",\", \"\"), } ) self.iter = 0 collect_trajectories(item: GSM8kRow):\nTakes a data item (question and answer). Formats the prompt. Sends requests to vLLM to get n completions (rollouts) for the question. Prepares the data for scoring. async def collect_trajectories( self, item: GSM8kRow ) -\u003e Tuple[ScoredDataGroup, list[Item]]: user_message = {\"role\": \"user\", \"content\": item[\"question\"]} gold_answer = ( \"\\boxed{\" + item[\"answer\"].split(\"#\")[-1].strip().replace(\",\", \"\") + \"}\" ) chat_completions = await self.server.chat_completion( messages=[{\"role\": \"system\", \"content\": system_prompt}, user_message], n=self.config.group_size, # Number of completions to generate max_tokens=self.config.max_token_length, ) # ... prepares data for scoring ... return to_postprocess, to_backlog score(rollout_group_data):\nTakes a group of rollouts. Parses the generated answers and the gold answer using latex2sympy2_extended and math_verify. Assigns a reward (1.0 for correct, -1.0 for incorrect). Tokenizes the messages for the trainer. Applies a length penalty if all answers in a group are correct, to encourage conciseness. Returns None if all scores are identical (e.g., all correct or all incorrect) to avoid sending uninformative data to the trainer, or if the gold solution is unparseable. async def score( self, rollout_group_data ) -\u003e Union[Optional[ScoredDataGroup], List[Optional[ScoredDataGroup]]]: # ... parsing and verification logic ... reward = verify(answer_parsed, gold_parsed) # ... tokenization ... scores[\"scores\"].append(1.0 if reward else -1.0) # ... length penalty logic ... if all([scores[\"scores\"][0] == score for score in scores[\"scores\"]]): return None # If all the same, we return None return scores get_next_item():\nProvides the next training item from the dataset. trainer.py This script is responsible for the actual model training process. It initializes the model and tokenizer, sets up the optimizer, fetches data batches from the orchestration server (which gets them from gsm8k_environment.py), performs the training steps, and manages the vLLM inference server.\nKey Components:\nTrainingConfig(BaseModel):\nA Pydantic model defining all necessary configurations for training, such as model name, learning rate, batch size, sequence length, device, save paths, and vLLM specific settings. class TrainingConfig(BaseModel): model_name: str = Field(..., description=\"Name of the base model to train\") lr: float = Field(1e-5, description=\"Learning rate for the optimizer\") training_steps: int = Field(10, description=\"Number of training steps\") batch_size: int = Field(2, description=\"Batch size for training\") # ... other fields ... vllm_port: int = Field(9001, description=\"Port for the vLLM server\") use_wandb: bool = Field(False, description=\"Whether to use Weights \u0026 Biases for logging\") register_trainer(config: TrainingConfig):\nSends a POST request to the orchestration server (http://localhost:8000/register) to register itself, providing its configuration details. This allows the orchestration server to know about the trainer and its requirements. @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=15)) def register_trainer(config: TrainingConfig): requests.post( \"http://localhost:8000/register\", json={ \"wandb_group\": config.wandb_group, # ... other registration details ... \"num_steps\": config.training_steps, }, timeout=10, ) get_data(batch_size: int, seq_len: int):\nContinuously polls the orchestration server (http://localhost:8000/batch) for new batches of data. def get_data( batch_size: int, seq_len: int ) -\u003e List[Tuple[List[torch.Tensor], List[torch.Tensor], List[torch.Tensor]]]: batches = [] while True: data = get_batch() # Fetches from http://localhost:8000/batch if data[\"batch\"] is not None: batches.append(pad_data_to_good_offset(data, batch_size)) elif len(batches) \u003e 0: return batches else: time.sleep(1) train(config: TrainingConfig):\nInitialization: Sets up Weights \u0026 Biases (wandb) if configured. Loads the tokenizer and model from Hugging Face (AutoTokenizer, AutoModelForCausalLM). Sets up the AdamW optimizer. Registers the trainer with the orchestration server. vLLM Management: Launches an initial vLLM server instance as a subprocess using the base model. The vllm_process global variable tracks this subprocess. Training Loop: Iterates for config.training_steps. Fetches data using get_data(). For each batch: Performs a forward pass through the model. Calculates the GRPO loss. The loss encourages actions with positive advantages and discourages those with negative advantages. Performs backpropagation and optimizer step. Logs metrics (loss, learning rate, gradient norm, log probabilities) to the console and wandb. vLLM Restart and Checkpointing: Periodically (defined by config.vllm_restart_interval) or on the last step: Saves a model checkpoint (weights and tokenizer). Terminates the current vLLM process. Launches a new vLLM process using the newly saved checkpoint, allowing the environment to use the updated model for subsequent rollouts. def train(config: TrainingConfig): global vllm_process # ... Wandb Setup, Model \u0026 Optimizer Init ... register_trainer(config) # Init vLLM with base model vllm_command = [ \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\", \"--model\", config.model_name, # ... other vLLM args ... ] vllm_process = subprocess.Popen(vllm_command) for step in range(config.training_steps): # ... fetch data ... # ... training step, loss calculation, optimizer.step() ... if (step + 1) % config.vllm_restart_interval == 0 or step == config.training_steps - 1: checkpoint_path = os.path.join(config.save_path, f\"step_{step+1}\") model.save_pretrained(checkpoint_path) tokenizer.save_pretrained(checkpoint_path) # Terminate existing vLLM if vllm_process: vllm_process.terminate() vllm_process.wait() # Launch new vLLM with updated model updated_vllm_command = [ \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\", \"--model\", checkpoint_path, # Use the new checkpoint # ... other vLLM args ... ] vllm_process = subprocess.Popen(updated_vllm_command) # ... final save and cleanup ... Training Process Once you have installed the dependencies, download the environment and training scripts (you might need to rename them), then complete the steps below:\nStart Orchestration Server: Open a terminal, activate your environment, and run:\nmkdir empty cd empty run-api This server listens on http://localhost:8000 and coordinates between the environment and trainer. We’re doing this inside an empty folder because Atropos listens for files changes for some reason and will restart the server every time a checkpoint is saved, breaking the training process.\nStart Environment: Open another terminal, activate the environment, and run:\npython gsm8k_environment.py serve --slurm false The environment will attempt to connect to the orchestration server. Initially, it might wait if the trainer hasn’t registered yet.\nStart Trainer: Open a third terminal, activate the environment, and run:\npython trainer.py The trainer.py script will: Initialize a model (e.g., “Qwen/Qwen2.5-1.5B-Instruct”). Start a vLLM server instance using this base model on a specified port (default 9001). Register itself with the orchestration server. Once the trainer registers, the environment(s) will start generating rollouts using the vLLM server managed by the trainer. The environment sends scored rollouts to the orchestration server. The trainer fetches these rollouts from the orchestration server, performs training steps, and updates its model weights. Periodically, the trainer saves a checkpoint and restarts the vLLM server with the updated model weights, allowing the environment to benefit from the training progress. This setup allows for a decoupled system where data generation (environment) and model training (trainer) can happen independently and potentially on different hardware, coordinated by the orchestration server.\nThe project is actively being developed, so I would not be surprised if this guide quickly becomes outdated. Hopefully it’s at least useful for someone else at the hackathon. See you all there!\nTwitter: @JakeABoggs\n","wordCount":"1860","inLanguage":"en","datePublished":"2025-05-16T00:00:00Z","dateModified":"2025-05-16T00:00:00Z","author":{"@type":"Person","name":"Jake Boggs"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://boggs.tech/posts/atropos-guide-reinforcement-learning-with-llms/"},"publisher":{"@type":"Organization","name":"Jake Boggs","logo":{"@type":"ImageObject","url":"https://boggs.tech/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://boggs.tech/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://boggs.tech/startups title=Startups><span>Startups</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Scalable Reinforcement Learning with LLMs - Atropos Guide</h1><div class=post-meta><span title='2025-05-16 00:00:00 +0000 UTC'>May 16, 2025</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;1860 words&nbsp;·&nbsp;Jake Boggs</div></header><div class=post-content><p>This weekend, I will be in attendance at the <a href=https://cerebralvalley.ai/e/nous-research-rl-environments-hackathon-9be3062a>Nous Research – RL Environments Hackathon</a>, so to prepare I&rsquo;ve been playing around with Atropos, their new RL framework that we will be using for the event. After failing to find any guides online, I decided to write my own.</p><h2 id=what-is-atropos>What is Atropos?<a hidden class=anchor aria-hidden=true href=#what-is-atropos>#</a></h2><p>Atropos is a library from <a href=https://nousresearch.com/>Nous Research</a> for performing reinforcement learning with LLMs. It provides a framework for managing environments and collecting rollouts. The training process is broken into four main components (environments, inference, trainer, orchestration), each running separately, enabling them to be distributed across multiple machines.</p><p>An example of this might look like training a coding agent, where the training script (handling backpropagation, loss calculations, and weight updates) happens on a powerful GPU cluster, while multiple code execution environments run on smaller CPU nodes. The orchestration server manages the communication between them, collecting rollouts (which could be a code update and its execution results in this example) from the environments and batching them to send to the trainer.</p><p>The recommended configuration provided in the repository uses vLLM running inside the trainer process. The environments can then query this vLLM instance when generating rollouts. These rollouts occur asynchronously, with the results passed to the orchestration server after a rollout is complete. Periodically (e.g. every few training steps), the vLLM server is restarted so that it uses the latest set of model weights from the trainer.</p><p><img loading=lazy src=/images/atropos.png alt="Atropos Diagram"></p><h2 id=why-do-we-need-this>Why do we need this?<a hidden class=anchor aria-hidden=true href=#why-do-we-need-this>#</a></h2><p>There are already plenty of reinforcement learning libraries such as <a href=https://huggingface.co/docs/trl/en/index>TRL</a>, but the structure of Atropos makes it particularly useful for large training setups.</p><ul><li><strong>Scalability and Efficiency:</strong> By separating the environments, inference, and training, each component can be scaled independently. For example, you can run numerous environments in parallel on cost-effective CPU instances and maximize the utilization of your inference hardware. This distributed approach and significantly speeds up the data collection and training cycle.</li><li><strong>Flexibility:</strong> Atropos allows for heterogeneous hardware setups. Environments can run on different operating systems or hardware configurations without impacting the trainer.</li><li><strong>Real-World Use Cases:</strong><ul><li><strong>Coding Agents:</strong> As mentioned earlier, training an AI to write or debug code can involve numerous sandboxed execution environments. Atropos can manage these environments, collect the outcomes of code execution (success, failure, errors), and feed this data back to the trainer.</li><li><strong>Game AI:</strong> Developing AI for complex games can require simulating many game instances simultaneously. Each game instance acts as an environment, and Atropos can orchestrate the collection of gameplay data (actions, states, rewards).</li><li><strong>Robotics:</strong> Training robots often involves physical or simulated environments. Atropos can help manage these diverse environments, allowing for parallel data collection from multiple robots or simulations.</li></ul></li></ul><p>To drive continued performance gains, future models are likely to spend a larger portion of their compute budget on RL instead of pretraining. This will necessitate scalable frameworks.</p><h2 id=environment-setup>Environment Setup<a hidden class=anchor aria-hidden=true href=#environment-setup>#</a></h2><p>Create a python environment and install dependencies:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python -m venv atropos
</span></span><span style=display:flex><span>source atropos/bin/activate
</span></span><span style=display:flex><span>pip install <span style=color:#e6db74>&#34;vllm&gt;=0.8.5&#34;</span> torch transformers datasets wandb tenacity atroposlib pydantic
</span></span></code></pre></div><h2 id=code-structure>Code Structure<a hidden class=anchor aria-hidden=true href=#code-structure>#</a></h2><p>We&rsquo;ll go over the GSM8K example from the <a href=https://github.com/NousResearch/atropos>Atropos repository</a>. At the time of writing, there are a couple of small bugs that you will need to fix if you plan to run them yourself.</p><ol><li><p>In <code>trainer.py</code>, the wandb variables need to be empty strings and not <code>None</code> if you are not using wandb, otherwise the script will error:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>wandb_project: Optional[str] <span style=color:#f92672>=</span> Field(<span style=color:#e6db74>&#34;&#34;</span>, description<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Wandb project name&#34;</span>)
</span></span><span style=display:flex><span>wandb_group: Optional[str] <span style=color:#f92672>=</span> Field(<span style=color:#e6db74>&#34;&#34;</span>, description<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Wandb group name&#34;</span>)
</span></span></code></pre></div></li><li><p>Again in <code>trainer.py</code>, you need to set <code>model.config.use_cache = False</code> to support the gradient accumulation.</p></li></ol><p>With that out of the way, here are the important files and functions:</p><h3 id=gsm8k_environmentpyhttpsgithubcomnousresearchatroposblobmainenvironmentsgsm8k_serverpy><a href=https://github.com/NousResearch/atropos/blob/main/environments/gsm8k_server.py><code>gsm8k_environment.py</code></a><a hidden class=anchor aria-hidden=true href=#gsm8k_environmentpyhttpsgithubcomnousresearchatroposblobmainenvironmentsgsm8k_serverpy>#</a></h3><p>This script defines the <code>GSM8kEnv</code> class, which is responsible for interacting with the GSM8k dataset, generating prompts, collecting model completions, and scoring them.</p><p><strong>Key Components:</strong></p><ol><li><p><strong><code>GSM8kEnv(BaseEnv)</code></strong>:</p><ul><li>Inherits from <code>BaseEnv</code> in the <code>atroposlib</code>.</li><li>Manages the GSM8k environment, including data loading, interaction with the LLM server for completions, and scoring.</li><li>Handles wandb logging for metrics like percent correct.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>GSM8kEnv</span>(BaseEnv):
</span></span><span style=display:flex><span>    name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;gsm8k&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(
</span></span><span style=display:flex><span>        self,
</span></span><span style=display:flex><span>        config: BaseEnvConfig,
</span></span><span style=display:flex><span>        server_configs: List[APIServerConfig],
</span></span><span style=display:flex><span>        slurm<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        testing<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>    ):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__(config, server_configs, slurm, testing)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>percent_correct_buffer <span style=color:#f92672>=</span> list()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>eval_metrics <span style=color:#f92672>=</span> list()
</span></span><span style=display:flex><span>        <span style=color:#75715e># Add tracking for wandb visualizations</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>rollouts_for_wandb <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>completion_lengths <span style=color:#f92672>=</span> []
</span></span></code></pre></div></li><li><p><strong><code>config_init()</code></strong>:</p><ul><li>A class method to define default configurations for the environment (<code>BaseEnvConfig</code>) and the API server(s) (<code>APIServerConfig</code>) it interacts with. This includes tokenizer name, batch sizes, wandb settings, and model details for the inference server.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@classmethod</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>config_init</span>(cls) <span style=color:#f92672>-&gt;</span> Tuple[BaseEnvConfig, List[APIServerConfig]]:
</span></span><span style=display:flex><span>    env_config <span style=color:#f92672>=</span> BaseEnvConfig(
</span></span><span style=display:flex><span>        tokenizer_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;NousResearch/DeepHermes-3-Llama-3-3B-Preview&#34;</span>,
</span></span><span style=display:flex><span>        group_size<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>,
</span></span><span style=display:flex><span>        use_wandb<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        rollout_server_url<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;http://localhost:8000&#34;</span>,
</span></span><span style=display:flex><span>        total_steps<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>,
</span></span><span style=display:flex><span>        batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>12</span>,
</span></span><span style=display:flex><span>        steps_per_eval<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>,
</span></span><span style=display:flex><span>        max_token_length<span style=color:#f92672>=</span><span style=color:#ae81ff>2048</span>,
</span></span><span style=display:flex><span>        wandb_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;gsm8k&#34;</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    server_configs <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        APIServerConfig(
</span></span><span style=display:flex><span>            model_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;NousResearch/DeepHermes-3-Llama-3-3B-Preview&#34;</span>,
</span></span><span style=display:flex><span>            base_url<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;http://localhost:9001/v1&#34;</span>, <span style=color:#75715e># Points to the vLLM server started by trainer.py</span>
</span></span><span style=display:flex><span>            api_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;x&#34;</span>, <span style=color:#75715e># Placeholder, as vLLM by default doesn&#39;t require an API key</span>
</span></span><span style=display:flex><span>            num_requests_for_eval<span style=color:#f92672>=</span><span style=color:#ae81ff>256</span>,
</span></span><span style=display:flex><span>        ),
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> env_config, server_configs
</span></span></code></pre></div></li><li><p><strong><code>setup()</code></strong>:</p><ul><li>Loads and preprocesses the GSM8k dataset (train and test splits).</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>setup</span>(self):
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>train <span style=color:#f92672>=</span> load_dataset(<span style=color:#e6db74>&#34;gsm8k&#34;</span>, <span style=color:#e6db74>&#34;main&#34;</span>, split<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;train&#34;</span>)<span style=color:#f92672>.</span>shuffle(seed<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>    test_data <span style=color:#f92672>=</span> load_dataset(<span style=color:#e6db74>&#34;gsm8k&#34;</span>, <span style=color:#e6db74>&#34;main&#34;</span>, split<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;test&#34;</span>)<span style=color:#f92672>.</span>shuffle(seed<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>test <span style=color:#f92672>=</span> list()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> test_data:
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>test<span style=color:#f92672>.</span>append(
</span></span><span style=display:flex><span>            {
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;question&#34;</span>: item[<span style=color:#e6db74>&#34;question&#34;</span>],
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;gold_answer&#34;</span>: item[<span style=color:#e6db74>&#34;answer&#34;</span>]
</span></span><span style=display:flex><span>                <span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;#&#34;</span>)[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>                <span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>                <span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#34;,&#34;</span>, <span style=color:#e6db74>&#34;&#34;</span>),
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>iter <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span></code></pre></div></li><li><p><strong><code>collect_trajectories(item: GSM8kRow)</code></strong>:</p><ul><li>Takes a data item (question and answer).</li><li>Formats the prompt.</li><li>Sends requests to vLLM to get <code>n</code> completions (rollouts) for the question.</li><li>Prepares the data for scoring.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>collect_trajectories</span>(
</span></span><span style=display:flex><span>    self, item: GSM8kRow
</span></span><span style=display:flex><span>) <span style=color:#f92672>-&gt;</span> Tuple[ScoredDataGroup, list[Item]]:
</span></span><span style=display:flex><span>    user_message <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: item[<span style=color:#e6db74>&#34;question&#34;</span>]}
</span></span><span style=display:flex><span>    gold_answer <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\b</span><span style=color:#e6db74>oxed{&#34;</span> <span style=color:#f92672>+</span> item[<span style=color:#e6db74>&#34;answer&#34;</span>]<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;#&#34;</span>)[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>strip()<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#34;,&#34;</span>, <span style=color:#e6db74>&#34;&#34;</span>) <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;}&#34;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    chat_completions <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>server<span style=color:#f92672>.</span>chat_completion(
</span></span><span style=display:flex><span>        messages<span style=color:#f92672>=</span>[{<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: system_prompt}, user_message],
</span></span><span style=display:flex><span>        n<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>config<span style=color:#f92672>.</span>group_size, <span style=color:#75715e># Number of completions to generate</span>
</span></span><span style=display:flex><span>        max_tokens<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>config<span style=color:#f92672>.</span>max_token_length,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#75715e># ... prepares data for scoring ...</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> to_postprocess, to_backlog
</span></span></code></pre></div></li><li><p><strong><code>score(rollout_group_data)</code></strong>:</p><ul><li>Takes a group of rollouts.</li><li>Parses the generated answers and the gold answer using <code>latex2sympy2_extended</code> and <code>math_verify</code>.</li><li>Assigns a reward (1.0 for correct, -1.0 for incorrect).</li><li>Tokenizes the messages for the trainer.</li><li>Applies a length penalty if all answers in a group are correct, to encourage conciseness.</li><li>Returns <code>None</code> if all scores are identical (e.g., all correct or all incorrect) to avoid sending uninformative data to the trainer, or if the gold solution is unparseable.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>score</span>(
</span></span><span style=display:flex><span>    self, rollout_group_data
</span></span><span style=display:flex><span>) <span style=color:#f92672>-&gt;</span> Union[Optional[ScoredDataGroup], List[Optional[ScoredDataGroup]]]:
</span></span><span style=display:flex><span>    <span style=color:#75715e># ... parsing and verification logic ...</span>
</span></span><span style=display:flex><span>    reward <span style=color:#f92672>=</span> verify(answer_parsed, gold_parsed)
</span></span><span style=display:flex><span>    <span style=color:#75715e># ... tokenization ...</span>
</span></span><span style=display:flex><span>    scores[<span style=color:#e6db74>&#34;scores&#34;</span>]<span style=color:#f92672>.</span>append(<span style=color:#ae81ff>1.0</span> <span style=color:#66d9ef>if</span> reward <span style=color:#66d9ef>else</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1.0</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># ... length penalty logic ...</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> all([scores[<span style=color:#e6db74>&#34;scores&#34;</span>][<span style=color:#ae81ff>0</span>] <span style=color:#f92672>==</span> score <span style=color:#66d9ef>for</span> score <span style=color:#f92672>in</span> scores[<span style=color:#e6db74>&#34;scores&#34;</span>]]):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>None</span> <span style=color:#75715e># If all the same, we return None</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> scores
</span></span></code></pre></div></li><li><p><strong><code>get_next_item()</code></strong>:</p><ul><li>Provides the next training item from the dataset.</li></ul></li></ol><h3 id=trainerpyhttpsgithubcomnousresearchatroposblobmainexample_trainergrpopy><a href=https://github.com/NousResearch/atropos/blob/main/example_trainer/grpo.py><code>trainer.py</code></a><a hidden class=anchor aria-hidden=true href=#trainerpyhttpsgithubcomnousresearchatroposblobmainexample_trainergrpopy>#</a></h3><p>This script is responsible for the actual model training process. It initializes the model and tokenizer, sets up the optimizer, fetches data batches from the orchestration server (which gets them from <code>gsm8k_environment.py</code>), performs the training steps, and manages the vLLM inference server.</p><p><strong>Key Components:</strong></p><ol><li><p><strong><code>TrainingConfig(BaseModel)</code></strong>:</p><ul><li>A Pydantic model defining all necessary configurations for training, such as model name, learning rate, batch size, sequence length, device, save paths, and vLLM specific settings.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TrainingConfig</span>(BaseModel):
</span></span><span style=display:flex><span>    model_name: str <span style=color:#f92672>=</span> Field(<span style=color:#f92672>...</span>, description<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Name of the base model to train&#34;</span>)
</span></span><span style=display:flex><span>    lr: float <span style=color:#f92672>=</span> Field(<span style=color:#ae81ff>1e-5</span>, description<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Learning rate for the optimizer&#34;</span>)
</span></span><span style=display:flex><span>    training_steps: int <span style=color:#f92672>=</span> Field(<span style=color:#ae81ff>10</span>, description<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Number of training steps&#34;</span>)
</span></span><span style=display:flex><span>    batch_size: int <span style=color:#f92672>=</span> Field(<span style=color:#ae81ff>2</span>, description<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Batch size for training&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># ... other fields ...</span>
</span></span><span style=display:flex><span>    vllm_port: int <span style=color:#f92672>=</span> Field(<span style=color:#ae81ff>9001</span>, description<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Port for the vLLM server&#34;</span>)
</span></span><span style=display:flex><span>    use_wandb: bool <span style=color:#f92672>=</span> Field(<span style=color:#66d9ef>False</span>, description<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Whether to use Weights &amp; Biases for logging&#34;</span>)
</span></span></code></pre></div></li><li><p><strong><code>register_trainer(config: TrainingConfig)</code></strong>:</p><ul><li>Sends a POST request to the orchestration server (<code>http://localhost:8000/register</code>) to register itself, providing its configuration details. This allows the orchestration server to know about the trainer and its requirements.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@retry</span>(stop<span style=color:#f92672>=</span>stop_after_attempt(<span style=color:#ae81ff>3</span>), wait<span style=color:#f92672>=</span>wait_exponential(multiplier<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, min<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, max<span style=color:#f92672>=</span><span style=color:#ae81ff>15</span>))
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>register_trainer</span>(config: TrainingConfig):
</span></span><span style=display:flex><span>    requests<span style=color:#f92672>.</span>post(
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;http://localhost:8000/register&#34;</span>,
</span></span><span style=display:flex><span>        json<span style=color:#f92672>=</span>{
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;wandb_group&#34;</span>: config<span style=color:#f92672>.</span>wandb_group,
</span></span><span style=display:flex><span>            <span style=color:#75715e># ... other registration details ...</span>
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;num_steps&#34;</span>: config<span style=color:#f92672>.</span>training_steps,
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        timeout<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>,
</span></span><span style=display:flex><span>    )
</span></span></code></pre></div></li><li><p><strong><code>get_data(batch_size: int, seq_len: int)</code></strong>:</p><ul><li>Continuously polls the orchestration server (<code>http://localhost:8000/batch</code>) for new batches of data.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_data</span>(
</span></span><span style=display:flex><span>    batch_size: int, seq_len: int
</span></span><span style=display:flex><span>) <span style=color:#f92672>-&gt;</span> List[Tuple[List[torch<span style=color:#f92672>.</span>Tensor], List[torch<span style=color:#f92672>.</span>Tensor], List[torch<span style=color:#f92672>.</span>Tensor]]]:
</span></span><span style=display:flex><span>    batches <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> get_batch() <span style=color:#75715e># Fetches from http://localhost:8000/batch</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> data[<span style=color:#e6db74>&#34;batch&#34;</span>] <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            batches<span style=color:#f92672>.</span>append(pad_data_to_good_offset(data, batch_size))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> len(batches) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> batches
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            time<span style=color:#f92672>.</span>sleep(<span style=color:#ae81ff>1</span>)
</span></span></code></pre></div></li><li><p><strong><code>train(config: TrainingConfig)</code></strong>:</p><ul><li><strong>Initialization</strong>:<ul><li>Sets up Weights & Biases (wandb) if configured.</li><li>Loads the tokenizer and model from Hugging Face (<code>AutoTokenizer</code>, <code>AutoModelForCausalLM</code>).</li><li>Sets up the AdamW optimizer.</li><li>Registers the trainer with the orchestration server.</li></ul></li><li><strong>vLLM Management</strong>:<ul><li>Launches an initial vLLM server instance as a subprocess using the base model.</li><li>The <code>vllm_process</code> global variable tracks this subprocess.</li></ul></li><li><strong>Training Loop</strong>:<ul><li>Iterates for <code>config.training_steps</code>.</li><li>Fetches data using <code>get_data()</code>.</li><li>For each batch:<ul><li>Performs a forward pass through the model.</li><li>Calculates the GRPO loss. The loss encourages actions with positive advantages and discourages those with negative advantages.</li><li>Performs backpropagation and optimizer step.</li><li>Logs metrics (loss, learning rate, gradient norm, log probabilities) to the console and wandb.</li></ul></li><li><strong>vLLM Restart and Checkpointing</strong>:<ul><li>Periodically (defined by <code>config.vllm_restart_interval</code>) or on the last step:<ul><li>Saves a model checkpoint (weights and tokenizer).</li><li>Terminates the current vLLM process.</li><li>Launches a new vLLM process using the <em>newly saved checkpoint</em>, allowing the environment to use the updated model for subsequent rollouts.</li></ul></li></ul></li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train</span>(config: TrainingConfig):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>global</span> vllm_process
</span></span><span style=display:flex><span>    <span style=color:#75715e># ... Wandb Setup, Model &amp; Optimizer Init ...</span>
</span></span><span style=display:flex><span>    register_trainer(config)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Init vLLM with base model</span>
</span></span><span style=display:flex><span>    vllm_command <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;python&#34;</span>, <span style=color:#e6db74>&#34;-m&#34;</span>, <span style=color:#e6db74>&#34;vllm.entrypoints.openai.api_server&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;--model&#34;</span>, config<span style=color:#f92672>.</span>model_name,
</span></span><span style=display:flex><span>        <span style=color:#75715e># ... other vLLM args ...</span>
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    vllm_process <span style=color:#f92672>=</span> subprocess<span style=color:#f92672>.</span>Popen(vllm_command)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> step <span style=color:#f92672>in</span> range(config<span style=color:#f92672>.</span>training_steps):
</span></span><span style=display:flex><span>        <span style=color:#75715e># ... fetch data ...</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># ... training step, loss calculation, optimizer.step() ...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (step <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>%</span> config<span style=color:#f92672>.</span>vllm_restart_interval <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>or</span> step <span style=color:#f92672>==</span> config<span style=color:#f92672>.</span>training_steps <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>:
</span></span><span style=display:flex><span>            checkpoint_path <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(config<span style=color:#f92672>.</span>save_path, <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;step_</span><span style=color:#e6db74>{</span>step<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>            model<span style=color:#f92672>.</span>save_pretrained(checkpoint_path)
</span></span><span style=display:flex><span>            tokenizer<span style=color:#f92672>.</span>save_pretrained(checkpoint_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Terminate existing vLLM</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> vllm_process:
</span></span><span style=display:flex><span>                vllm_process<span style=color:#f92672>.</span>terminate()
</span></span><span style=display:flex><span>                vllm_process<span style=color:#f92672>.</span>wait()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Launch new vLLM with updated model</span>
</span></span><span style=display:flex><span>            updated_vllm_command <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;python&#34;</span>, <span style=color:#e6db74>&#34;-m&#34;</span>, <span style=color:#e6db74>&#34;vllm.entrypoints.openai.api_server&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;--model&#34;</span>, checkpoint_path, <span style=color:#75715e># Use the new checkpoint</span>
</span></span><span style=display:flex><span>                <span style=color:#75715e># ... other vLLM args ...</span>
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>            vllm_process <span style=color:#f92672>=</span> subprocess<span style=color:#f92672>.</span>Popen(updated_vllm_command)
</span></span><span style=display:flex><span>    <span style=color:#75715e># ... final save and cleanup ...</span>
</span></span></code></pre></div></li></ol><h2 id=training-process>Training Process<a hidden class=anchor aria-hidden=true href=#training-process>#</a></h2><p>Once you have installed the dependencies, download the <a href=https://github.com/NousResearch/atropos/blob/main/environments/gsm8k_server.py>environment</a> and <a href=https://github.com/NousResearch/atropos/blob/main/example_trainer/grpo.py>training</a> scripts (you might need to rename them), then complete the steps below:</p><ol><li><p><strong>Start Orchestration Server</strong>:
Open a terminal, activate your environment, and run:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir empty
</span></span><span style=display:flex><span>cd empty
</span></span><span style=display:flex><span>run-api
</span></span></code></pre></div><p>This server listens on <code>http://localhost:8000</code> and coordinates between the environment and trainer. We&rsquo;re doing this inside an empty folder because Atropos listens for files changes for some reason and will restart the server every time a checkpoint is saved, breaking the training process.</p></li><li><p><strong>Start Environment</strong>:
Open another terminal, activate the environment, and run:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python gsm8k_environment.py serve --slurm false
</span></span></code></pre></div><p>The environment will attempt to connect to the orchestration server. Initially, it might wait if the trainer hasn&rsquo;t registered yet.</p></li><li><p><strong>Start Trainer</strong>:
Open a third terminal, activate the environment, and run:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python trainer.py
</span></span></code></pre></div><ul><li>The <code>trainer.py</code> script will:<ul><li>Initialize a model (e.g., &ldquo;Qwen/Qwen2.5-1.5B-Instruct&rdquo;).</li><li>Start a vLLM server instance using this base model on a specified port (default 9001).</li><li>Register itself with the orchestration server.</li></ul></li><li>Once the trainer registers, the environment(s) will start generating rollouts using the vLLM server managed by the trainer.</li><li>The environment sends scored rollouts to the orchestration server.</li><li>The trainer fetches these rollouts from the orchestration server, performs training steps, and updates its model weights.</li><li>Periodically, the trainer saves a checkpoint and restarts the vLLM server with the <em>updated</em> model weights, allowing the environment to benefit from the training progress.</li></ul></li></ol><p>This setup allows for a decoupled system where data generation (environment) and model training (trainer) can happen independently and potentially on different hardware, coordinated by the orchestration server.</p><p>The project is actively being developed, so I would not be surprised if this guide quickly becomes outdated. Hopefully it&rsquo;s at least useful for someone else at the hackathon. See you all there!</p><p>Twitter: <a href=https://x.com/JakeABoggs>@JakeABoggs</a></p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://boggs.tech/>Jake Boggs</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>