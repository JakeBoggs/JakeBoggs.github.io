<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Jake Boggs</title>
    <link>https://boggs.tech/posts/</link>
    <description>Recent content in Posts on Jake Boggs</description>
    <generator>Hugo -- 0.126.1</generator>
    <language>en</language>
    <lastBuildDate>Fri, 24 May 2024 23:39:24 -0400</lastBuildDate>
    <atom:link href="https://boggs.tech/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Large Language Models for Magic: the Gathering</title>
      <link>https://boggs.tech/posts/large-language-models-for-magic-the-gathering/</link>
      <pubDate>Fri, 24 May 2024 23:39:24 -0400</pubDate>
      <guid>https://boggs.tech/posts/large-language-models-for-magic-the-gathering/</guid>
      <description>Magic: The Gathering (MTG) has always fascinated me with its complexity and strategic depth, thanks to its extensive rulebook and vast array of unique cards. Despite the game&amp;rsquo;s popularity, AI systems specifically designed for MTG have been few and far between, often falling short due to their inability to accurately interpret the intricate rules and interactions between cards. This blog post chronicles my recent endeavor to bridge this gap with large language models (LLMs) by creating a specialized dataset and evaluation metric to improve AI performance in MTG-related tasks.</description>
    </item>
  </channel>
</rss>
