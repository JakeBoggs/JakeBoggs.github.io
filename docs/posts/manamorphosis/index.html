<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Introducing Manamorphosis: A Diffusion Model for MTG Deck Generation | Jake Boggs</title>
<meta name=keywords content><meta name=description content="This post details Manamorphosis, a first-of-its-kind diffusion model developed to complete Magic: The Gathering decklists. It takes a set of known cards and fills in the rest to form a 60-card main deck. Subsequently, using the completed main deck as context, it can complete a 15-card sideboard. The core generative mechanism is based on Denoising Diffusion Probabilistic Models (DDPMs), the same family of models powering many image generation systems like Stable Diffusion and Midjourney, but adapted here for the unique domain of card sets."><meta name=author content="Jake Boggs"><link rel=canonical href=https://boggs.tech/posts/manamorphosis/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://boggs.tech/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://boggs.tech/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://boggs.tech/favicon-32x32.png><link rel=apple-touch-icon href=https://boggs.tech/apple-touch-icon.png><link rel=mask-icon href=https://boggs.tech/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://boggs.tech/posts/manamorphosis/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Introducing Manamorphosis: A Diffusion Model for MTG Deck Generation"><meta property="og:description" content="This post details Manamorphosis, a first-of-its-kind diffusion model developed to complete Magic: The Gathering decklists. It takes a set of known cards and fills in the rest to form a 60-card main deck. Subsequently, using the completed main deck as context, it can complete a 15-card sideboard. The core generative mechanism is based on Denoising Diffusion Probabilistic Models (DDPMs), the same family of models powering many image generation systems like Stable Diffusion and Midjourney, but adapted here for the unique domain of card sets."><meta property="og:type" content="article"><meta property="og:url" content="https://boggs.tech/posts/manamorphosis/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-05T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-05T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Introducing Manamorphosis: A Diffusion Model for MTG Deck Generation"><meta name=twitter:description content="This post details Manamorphosis, a first-of-its-kind diffusion model developed to complete Magic: The Gathering decklists. It takes a set of known cards and fills in the rest to form a 60-card main deck. Subsequently, using the completed main deck as context, it can complete a 15-card sideboard. The core generative mechanism is based on Denoising Diffusion Probabilistic Models (DDPMs), the same family of models powering many image generation systems like Stable Diffusion and Midjourney, but adapted here for the unique domain of card sets."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://boggs.tech/posts/"},{"@type":"ListItem","position":2,"name":"Introducing Manamorphosis: A Diffusion Model for MTG Deck Generation","item":"https://boggs.tech/posts/manamorphosis/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Introducing Manamorphosis: A Diffusion Model for MTG Deck Generation","name":"Introducing Manamorphosis: A Diffusion Model for MTG Deck Generation","description":"This post details Manamorphosis, a first-of-its-kind diffusion model developed to complete Magic: The Gathering decklists. It takes a set of known cards and fills in the rest to form a 60-card main deck. Subsequently, using the completed main deck as context, it can complete a 15-card sideboard. The core generative mechanism is based on Denoising Diffusion Probabilistic Models (DDPMs), the same family of models powering many image generation systems like Stable Diffusion and Midjourney, but adapted here for the unique domain of card sets.","keywords":[],"articleBody":"This post details Manamorphosis, a first-of-its-kind diffusion model developed to complete Magic: The Gathering decklists. It takes a set of known cards and fills in the rest to form a 60-card main deck. Subsequently, using the completed main deck as context, it can complete a 15-card sideboard. The core generative mechanism is based on Denoising Diffusion Probabilistic Models (DDPMs), the same family of models powering many image generation systems like Stable Diffusion and Midjourney, but adapted here for the unique domain of card sets. Applying AI models to MTG has long been a pet project of mine and I’m exciting to share this model, as I believe it is the state-of-the-art (and only) AI model dedicated to understanding deck construction.\nDemo Video Your browser does not support the video tag.\rCard Representation: Doc2Vec Embeddings Card identity is represented by 128-dimensional vectors (EMB_DIM=128). These embeddings are generated by training a Doc2Vec model (train_embedding_model.py) on preprocessed text data for each card obtained from MTGJSON’s AtomicCards.json. This captures semantic relationships based on card text. Much like modern RAG engines use embeddings to understand the meaning behind search queries and documents, these Doc2Vec embeddings allow the system to grasp the functional similarities between cards based on their textual descriptions (cost, type, rules).\nRationale for Using Embeddings Instead of training the model to directly predict discrete cards from a vocabulary of ~28,000+, Manamorphosis uses pre-trained Doc2Vec embeddings as an intermediate representation. This approach offers several advantages:\nHandling Data Sparsity: The training dataset (~47,000 decks) contains only a fraction (~5,000) of all legal MTG cards. A model predicting cards directly would struggle to learn meaningful representations or generation logic for the vast majority of cards rarely or never seen during training. The embedding model, trained on all card text, provides a representation for every card. Generalization to New/Unseen Cards: Because the embedding is derived from card text, the system can generate an embedding for any card, including newly released ones, without retraining the embedding model (though retraining the diffusion model might improve performance with new metagames). The diffusion model learns to operate on the semantic meaning captured in the 128-dimensional embedding space, rather than being limited to the fixed vocabulary seen during its own training. This contrasts with models that predict discrete tokens, which typically require a fixed, predefined vocabulary. Capturing Semantic Relationships: Doc2Vec learns vectors where cards with similar functions, costs, types, or textual patterns (e.g., different variations of counterspells, cheap red burn spells, evasive creatures) are closer together in the embedding space. This allows the diffusion model to learn higher-level concepts (“needs more removal,” “add card draw”) rather than just memorizing specific card co-occurrences, leading to potentially more robust and contextually relevant deck completions. This focus on semantic similarity is analogous to how embedding-based search engines return results that are conceptually related, not just keyword matches. Dimensionality Reduction \u0026 Decoupling: Working with dense 128-dimensional vectors is more computationally manageable for the transformer architecture than using extremely high-dimensional one-hot vectors (one per card). It also decouples the task of understanding card text semantics (Doc2Vec) from the task of generative deck construction (Diffusion Model). Card Text Preprocessing (train_embedding_model.py:get_text) The get_text function preprocesses card data into a consistent string format suitable for Doc2Vec training. Specific choices include:\nMana Cost: Replaced curly braces {} with pipe symbols | (|W|, |U|, etc.) and ensured spacing around symbols ({W}{U} -\u003e |W| |U|). This treats each mana symbol as a distinct token and distinguishes them from mana symbols in the card text. Power/Toughness: Represented as $Power$ #Toughness# (e.g., $2$ #2#). This creates unique tokens for P/T values. Card Type: The supertype (e.g., “Creature”, “Instant”) is split into individual tokens surrounded by pipes (|Creature|, |Instant|). Subtypes (e.g., “Goblin”, “Wizard”) are kept as single words. Rules Text: Card name references replaced with @. This prevents the model from overfitting to specific card names and focuses on the actions/effects. Common self-references (“this creature”, “this enchantment”, etc.) also replaced with @. Line breaks, semicolons replaced with spaces. Colons have spaces added (: -\u003e :). Reminder text (within parentheses) is removed using regex (re.sub(reminder_remover, '', ...)). Special characters like \u0026, −, —, ', ,, ., ', \" are handled (replaced or removed). Text is converted to lowercase. Stop Words: Common English stop words (like “the”, “a”, “is”) are removed using nltk.corpus.stopwords to reduce noise and focus on meaningful terms. This preprocessing aims to convert structured card information and natural language text into a sequence of meaningful tokens that the Doc2Vec model can learn relationships from.\n# From train_embedding_model.py (Illustrative snippet) def get_text(card): text = '' if 'manaCost' in card: text += card['manaCost'].replace('}{', '} {').replace('{', '|').replace('}', '|') + ' ' if 'power' in card: text += '$' + card['power'] + '$ #' + card['toughness'] + '# ' text += ' '.join(['|' + word + '|' for word in card['type'].split(' — ')[0].split()]) + ' ' if '—' in card['type']: text += card['type'].split(' — ')[1] + ' ' if 'text' in card: # ... (Handling basic lands) ... processed_text = card['text'].replace('\u0026', 'and').replace(card['name'], '@').replace(card['name'].split(',')[0], '@') # simplified processed_text = processed_text.replace('this creature', '@').replace('this enchantment', '@').replace('this artifact', '@').replace('this land', '@') processed_text = processed_text.replace('\\\\n', ' ').replace(';', ' ').replace(':', ' :').replace('|', '•') text += processed_text text = re.sub(reminder_remover, '', text.lower()... ) # Lowercasing, punctuation, etc. words = [word for word in text.split(' ') if word != ''] filtered_words = [word for word in words if word not in stop_words] return ' '.join(filtered_words) # Doc2Vec Training (Conceptual) model = Doc2Vec(vector_size=128, dm=0, dbow_words=0, min_count=2, epochs=200, workers=cores, seed=42) model.build_vocab(corpus) # corpus yields TaggedDocument(processed_text.split(), [card_id]) model.train(corpus, ...) A simple linear classifier (train_embedding_classifier.py) is trained separately to map these 128-dim embeddings back to unique card indices. This classifier is essential during the reverse diffusion process to identify the most likely card corresponding to a denoised embedding vector. This is more efficient than performing cosine similarity search for each generated embedding during inference.\n# From train_embedding_classifier.py class CardClassifier(nn.Module): def __init__(self, embedding_dim, num_classes): super(CardClassifier, self).__init__() self.network = nn.Linear(embedding_dim, num_classes) def forward(self, x): return self.network(x) Denoising Diffusion Probabilistic Model (DDPM) Framework The core generation mechanism is a DDPM, mirroring the approach used in image generation.\nForward Process (Noise Addition): Starting with the true deck embeddings x0, Gaussian noise is progressively added over T timesteps (here, T=1000). This is analogous to how image diffusion models start with a clear image and gradually add noise until only static remains. The noise level at each step is determined by a predefined variance schedule, specifically a cosine schedule (cosine_beta_schedule).\n# diffusion_model.py def cosine_beta_schedule(T, s=0.008): steps = torch.linspace(0, T, T + 1, dtype=torch.float64) alpha_bar = torch.cos(((steps / T) + s) / (1 + s) * torch.pi * 0.5) ** 2 alpha_bar = alpha_bar / alpha_bar[0] betas = 1 - (alpha_bar[1:] / alpha_bar[:-1]) return torch.clip(betas, 0, 0.999).float() # Within DiffusionTrainer class: beta = cosine_beta_schedule(T).to(device) # T = TIMESTEPS (e.g., 1000) alpha = 1.0 - beta alpha_bar = torch.cumprod(alpha, dim=0) # Precompute terms used in diffusion and sampling: self.register(\"sqrt_alpha_bar\", torch.sqrt(alpha_bar)) self.register(\"sqrt_one_minus_alpha_bar\", torch.sqrt(1.0 - alpha_bar)) self.register(\"beta\", beta) self.register(\"alpha\", alpha) # ... (other registered buffers) The state x_t at timestep t can be sampled directly using the cumulative product alpha_bar: x_t = sqrt(alpha_bar_t) * x0 + sqrt(1 - alpha_bar_t) * noise\n# diffusion_model.py: DiffusionTrainer.q_sample # Note: The actual implementation modifies x_t for known cards # based on the mask *after* the initial noise addition, # returning a mix of original x0 and noised x_t. def q_sample(self, x0, t, mask): mask_expanded = mask.expand_as(x0) sqrt_ab = self._extract(self.sqrt_alpha_bar, t, x0.shape, x0.device) sqrt_1mab = self._extract(self.sqrt_one_minus_alpha_bar, t, x0.shape, x0.device) noise = torch.randn_like(x0) # Calculate the fully noised version first x_t_noised = sqrt_ab * x0 + sqrt_1mab * noise # Return original embeddings for known positions, noised for unknown x_t_masked = mask_expanded * x0 + (1 - mask_expanded) * x_t_noised # Returns the masked noisy sample and the *original* noise (for loss) return x_t_masked, noise Reverse Process (Denoising): The model learns to predict the noise epsilon added at timestep t. Starting from pure noise x_T, the model iteratively refines the embeddings by predicting the noise epsilon_pred = model(x_t, x0, sb_x_t, t, mask, sb_mask) and estimating x_{t-1} until x0, the original noise-free main deck, is reached. During inference, the actual denoising step involves sampling x_{t-1} by subtracting the predicted noise epsilon_pred from x_t, then adding a smaller amount of noise for the next timestep.\nModel Architecture (diffusion_model.py:DiffusionModel) The model uses a transformer-based architecture, the same core building block behind Large Language Models like GPT and BERT, but adapted for set-based data rather than sequences. It lacks positional embeddings, treating decks as unordered sets, which differs from typical NLP or vision transformer usage where sequence order is crucial. It has distinct paths for main deck and sideboard processing. The internal model dimension is model_dim=384, and the embedding dimension is EMB_DIM=128.\nTime Embeddings: Timestep t is encoded using standard sinusoidal embeddings, processed by separate MLPs for main deck and sideboard paths.\n# diffusion_model.py def sinusoidal_embedding(t: torch.Tensor, dim: int = EMB_DIM): half = dim // 2 freqs = torch.exp(-math.log(10000) * torch.arange(half, device=t.device) / (half - 1)) args = t[:, None] * freqs[None] emb = torch.cat((args.sin(), args.cos()), dim=-1) if dim % 2: emb = nn.functional.pad(emb, (0, 1)) return emb # Within DiffusionModel.__init__ ff_dim = cfg[\"dim_feedforward\"] # 3072 self.main_time_mlp = nn.Sequential( nn.Linear(EMB_DIM, ff_dim), nn.SiLU(), nn.Linear(ff_dim, EMB_DIM), ) self.sb_time_mlp = nn.Sequential( # For Sideboard Decoder path nn.Linear(EMB_DIM, ff_dim), nn.SiLU(), nn.Linear(ff_dim, EMB_DIM), ) Mask Embeddings: Binary masks (1.0 for known, 0.0 for unknown) are processed by separate MLPs.\n# Within DiffusionModel.__init__ self.main_mask_mlp = nn.Sequential( nn.Linear(1, EMB_DIM), nn.SiLU(), nn.Linear(EMB_DIM, EMB_DIM), ) self.sb_mask_mlp = nn.Sequential( # For Sideboard Decoder path nn.Linear(1, ff_dim), nn.SiLU(), nn.Linear(ff_dim, EMB_DIM), # Note: intermediate dim is ff_dim here ) Input Processing: Input embeddings x_t (main) or sb_x_t (sideboard) are combined via addition with their respective time and mask embeddings, then projected to model_dim.\n# Within DiffusionModel.forward sin_emb = sinusoidal_embedding(t, EMB_DIM) # Shape: [Batch, EMB_DIM] # Main Deck Input main_t_emb_flat = self.main_time_mlp(sin_emb) # Shape: [Batch, EMB_DIM] main_t_emb = main_t_emb_flat[:, None, :].expand(-1, DECK_SIZE, -1) # Shape: [Batch, DECK_SIZE, EMB_DIM] main_mask_emb = self.main_mask_mlp(mask) # mask shape: [Batch, DECK_SIZE, 1] -\u003e Output: [Batch, DECK_SIZE, EMB_DIM] h_main = x_t + main_t_emb + main_mask_emb # x_t shape: [Batch, DECK_SIZE, EMB_DIM] h_main_proj = self.main_input_proj(h_main) # Linear(EMB_DIM, model_dim) -\u003e Shape: [Batch, DECK_SIZE, model_dim] # Sideboard Input sb_decoder_t_emb_flat = self.sb_time_mlp(sin_emb) # Shape: [Batch, EMB_DIM] sb_decoder_t_emb = sb_decoder_t_emb_flat[:, None, :].expand(-1, SIDEBOARD_SIZE, -1) # Shape: [Batch, SIDEBOARD_SIZE, EMB_DIM] sb_decoder_mask_emb = self.sb_mask_mlp(sb_mask) # sb_mask shape: [Batch, SIDEBOARD_SIZE, 1] -\u003e Output: [Batch, SIDEBOARD_SIZE, EMB_DIM] h_sb = sb_x_t + sb_decoder_t_emb + sb_decoder_mask_emb # sb_x_t shape: [Batch, SIDEBOARD_SIZE, EMB_DIM] h_sb_proj = self.sb_input_proj(h_sb) # Linear(EMB_DIM, model_dim) -\u003e Shape: [Batch, SIDEBOARD_SIZE, model_dim] Main Deck Path (Encoder): Processes h_main_proj through a standard nn.TransformerEncoder (layers=8, nhead=8). The output is projected back to EMB_DIM to predict main deck noise (main_noise_pred).\n# Within DiffusionModel.__init__ main_encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=nhead, ...) self.main_transformer_encoder = nn.TransformerEncoder(main_encoder_layer, num_layers=num_layers) self.main_output_proj = nn.Linear(model_dim, EMB_DIM) # Within DiffusionModel.forward main_encoded = self.main_transformer_encoder(h_main_proj) main_noise_pred = self.main_output_proj(main_encoded) Sideboard Context Path (Encoder): Processes the original main deck embeddings x0 (noise-free) through a separate, shallow nn.TransformerEncoder (num_layers=1) to create context (sb_context_encoded). This context is used by the sideboard decoder.\n# Within DiffusionModel.__init__ self.sb_context_input_proj = nn.Linear(EMB_DIM, model_dim) sb_context_encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=nhead, ...) self.sideboard_context_encoder = nn.TransformerEncoder(sb_context_encoder_layer, num_layers=1) # Within DiffusionModel.forward h_sb_context = x0 # Original main deck embeddings h_sb_context_proj = self.sb_context_input_proj(h_sb_context) sb_context_encoded = self.sideboard_context_encoder(h_sb_context_proj) Sideboard Path (Decoder): Processes projected sideboard embeddings h_sb_proj using a nn.TransformerDecoder (num_layers=1), conditioned on sb_context_encoded via cross-attention (memory). The decoder output is passed through another nn.TransformerEncoder (sb_layers=8) before the final projection back to EMB_DIM to predict sideboard noise (sb_noise_pred). The use of cross-attention to condition the sideboard generation on the main deck context is conceptually similar to how text-to-image models use cross-attention to condition image generation on a text prompt embedding.\n# Within DiffusionModel.__init__ sb_decoder_layer = nn.TransformerDecoderLayer(d_model=model_dim, nhead=nhead, ...) self.sb_transformer_decoder = nn.TransformerDecoder(sb_decoder_layer, num_layers=1) # Uses the same sb_context_encoder_layer definition for the subsequent encoder self.sb_transformer_output = nn.TransformerEncoder(sb_context_encoder_layer, num_layers=sb_num_layers) self.sb_output_proj = nn.Linear(model_dim, EMB_DIM) # Within DiffusionModel.forward sb_decoded = self.sb_transformer_decoder(tgt=h_sb_proj, memory=sb_context_encoded) sb_decoded = self.sb_transformer_output(sb_decoded) sb_noise_pred = self.sb_output_proj(sb_decoded) Conditioning: Masking Strategy Conditional generation (deck completion) is handled via masking. Known card embeddings are provided by the user (or determined during training). This mechanism is analogous to providing a starting image and a mask for inpainting in image generation models, or providing a text prompt to guide generation.\nTraining: For each training sample (x0_embeddings, x0_indices, etc.), multiple masks (masks_per_deck) are generated dynamically per deck.\nThe number of known main deck cards k_main is sampled from partitioned ranges [1, 59] across the generated masks to ensure diverse k values are seen. Sideboard k_sb is sampled randomly from [1, 14], with a 50% chance of being forced to 0. This is done so that the model performs well at generating sideboards from scratch, which I expect to be a common use case. Masking Logic (diffusion_model.py:DiffusionTrainer._create_mask_row): This function generates a single mask row (shape [deck_size, 1]) for a target k. It identifies unique available card indices in the current deck (current_deck_indices). It samples these unique cards without replacement using weights derived from pre-calculated popularity scores (self.card_popularity), slightly favoring less popular cards (0.5 + score, where score is 1.0 - normalized_count). It iterates through these weighted, shuffled unique cards. For each unique card, with 85% probability, it attempts to mask all available copies (up to k remaining); with 15% probability, it masks a random number of available copies (from 1 up to available, limited by k remaining). This process repeats until k positions are marked as known (1.0 in the mask). We typically want to mask all instances of a card to make the task harder, so that the model has to learn what cards go together and not just to add additional copies.\n# diffusion_model.py: DiffusionTrainer._create_mask_row (Simplified Pseudocode) def _create_mask_row(k_target, deck_size, current_deck_indices, popularity_scores): mask_row = torch.zeros(deck_size, 1) available_indices = torch.ones(deck_size, dtype=torch.bool) masked_count = 0 while masked_count \u003c k_target and available_indices.any(): # 1. Get unique card indices from currently *available* positions unique_cards = torch.unique(current_deck_indices[available_indices]) if not unique_cards: break # 2. Calculate sampling weights (favor less popular) weights = torch.tensor([0.5 + popularity_scores.get(idx.item(), 1.0) for idx in unique_cards]) # 3. Sample unique cards without replacement based on weights perm_indices = torch.multinomial(weights, num_samples=len(unique_cards), replacement=False) shuffled_unique_cards = unique_cards[perm_indices] for card_idx in shuffled_unique_cards: if masked_count \u003e= k_target: break # 4. Find available positions for this specific card_idx potential_pos = (current_deck_indices == card_idx).nonzero(as_tuple=True)[0] available_pos = potential_pos[available_indices[potential_pos]] # Filter by available available_count = len(available_pos) if available_count == 0: continue needed = k_target - masked_count # 5. Decide how many copies to mask (85% all available, 15% random count) if random.random() \u003c 0.85: num_to_mask = min(available_count, needed) else: max_can_mask = min(available_count, needed) if max_can_mask \u003c= 0: continue num_to_mask = random.randint(1, max(1, max_can_mask)) # 6. Select specific positions to mask and update mask_row/available_indices indices_to_mask = available_pos[torch.randperm(available_count)[:num_to_mask]] mask_row[indices_to_mask] = 1.0 available_indices[indices_to_mask] = False masked_count += num_to_mask return mask_row Loss Calculation: The MSE loss is computed only between the predicted noise (main_noise_pred, sb_noise_pred) and the true noise (noise, sb_noise from q_sample) for the unknown (mask value 0.0) card slots. This focuses the model on learning to generate the missing parts.\n# diffusion_model.py: DiffusionTrainer.p_losses main_loss = ((noise - main_noise_pred) * (1 - mask.expand_as(noise))).pow(2).mean() sb_loss = ((sb_noise - sb_noise_pred) * (1 - sb_mask.expand_as(sb_noise))).pow(2).mean() total_loss = main_loss + sb_loss Inference: During the reverse diffusion process (sampling x_{t-1} from x_t), the known card embeddings x0_known (provided by the user) are reapplied at each step to guide the generation towards the desired completion. A common approach (simplified):\nPredict noise: epsilon_pred = model(x_t, x0_context, sb_x_t, t, mask, sb_mask) Calculate the parameters (mean, variance) of the distribution p(x_{t-1} | x_t) using x_t, t, and epsilon_pred according to the diffusion schedule. Sample the potential next state x_{t-1}_sample from this distribution (adding noise if t \u003e 0, otherwise using the mean). Re-apply knowns to the sample: x_{t-1}_conditioned = mask * x0_known + (1 - mask) * x_{t-1}_sample. Use x_{t-1}_conditioned as the input x_t for the next step (t-2). The main deck context sb_context_encoded for the sideboard decoder is generated from the final denoised main deck embeddings (x0_main_final). Training (diffusion_model.py:DiffusionTrainer) The current model was trained using ~47,000 decks scraped from MTGTop8 and is format agnostic, with the training data covering Standard, Modern, Pioneer, Pauper, Legacy, and Vintage. The full model contains ~56 million parameters. Due to its small size, training was feasible on consumer hardware, specifically a single Nvidia 3050 Laptop GPU with 4GB of VRAM, taking roughly 4 days to complete 100 epochs.\nDataset: DeckDataset loads decks and filters for exact 60 main deck / 15 sideboard card counts. It converts card names to the pre-trained Doc2Vec embeddings (card_embeddings.pkl) and retrieves corresponding integer indices using the mapping from the trained classifier (card_classifier.pt). Decks with cards missing from embeddings or the classifier map are skipped. It also calculates card popularity scores based on deck frequency for the masking strategy. Optimizer: AdamW (torch.optim.AdamW) with learning rate (lr=5e-6 default) and weight decay (diff_weight_decay=1e-3 default). Objective: Minimize the combined MSE loss total_loss described above, calculated over masks_per_deck different masks for each deck in the batch. Process: Standard PyTorch training loop: iterates epochs, loads batches via DataLoader, calculates loss using p_losses, performs backpropagation (total_loss.backward()), clips gradients (nn.utils.clip_grad_norm_), updates optimizer (opt.step()). Checkpoints containing model state dict, epoch, and config are saved periodically (torch.save). Inference: Enforcing Deck Rules with Iterative Refinement While the diffusion model learns the underlying patterns of deck construction from the training data, it doesn’t inherently guarantee adherence to strict game rules like the 4-copy limit for non-basic cards or format legality during the raw generation process. To address this, the inference functions employ an iterative refinement strategy after the initial denoising pass:\nInitial Generation: The standard reverse diffusion process is performed once to generate initial embeddings for all unknown card slots, conditioned on any user-provided cards. Classification \u0026 Rule Check: The resulting embeddings (both originally known and newly generated) are converted back to card names using the trained linear classifier (CardClassifier). The system then checks for violations: 4-Copy Limit: It counts occurrences of each non-basic card name. For sideboard generation, this count considers cards in both the main deck and the current sideboard iteration. Format Legality: Using preloaded card data (derived from MTGJSON’s AtomicCards.json), it verifies if each generated card is legal in the specified format (e.g., ‘Modern’, ‘Standard’). Basic lands are exempt from this check. Identify Violations: The system identifies the specific generated card slots that violate either the 4-copy limit or format legality. User-provided cards are never marked for regeneration. Mask Update \u0026 Regeneration: A new mask is created. User-provided cards and valid generated cards from the current iteration are marked as “known”. Slots corresponding to rule violations are marked as “unknown”. Re-run Diffusion: The reverse diffusion sampling process is run again, using the updated mask and the embeddings of the known cards (including the valid generated ones) as fixed context. The model only needs to generate new embeddings for the slots marked as unknown due to rule violations. Repeat: Steps 2-5 are repeated up to a fixed number of maximum refinement iterations (MAX_REFINEMENT_ITERATIONS). This loop continues until no rule violations are found among the generated cards or the iteration limit is reached. This refinement loop significantly improves the likelihood of producing valid and legal deck completions by correcting rule violations after the initial generation, leveraging the classifier and external card data to guide the process without needing to bake these complex constraints directly into the diffusion model’s training objective. The final deck combines the original user input with the cards generated and refined through this process.\nConclusion Manamorphosis applies diffusion models to Magic: The Gathering deck generation and was a fun, but time-consuming project. Developing this system, including embedding tuning, model architecture experiments, and refinement loops, was a substantial effort (two weeks of me procrastinating before finals).\nWhile there’s always more to explore, like testing different embeddings or format specializations, the current model provides a solid foundation for AI-driven deck completion.\nThe complete code is available on GitHub if you’d like to run it yourself or see the nitty-gritty details: Manamorphosis GitHub repository.\nTwitter: @JakeABoggs\n","wordCount":"3311","inLanguage":"en","datePublished":"2025-05-05T00:00:00Z","dateModified":"2025-05-05T00:00:00Z","author":{"@type":"Person","name":"Jake Boggs"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://boggs.tech/posts/manamorphosis/"},"publisher":{"@type":"Organization","name":"Jake Boggs","logo":{"@type":"ImageObject","url":"https://boggs.tech/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://boggs.tech/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://boggs.tech/about title="About me"><span>About me</span></a></li><li><a href=https://boggs.tech/startups title=Startups><span>Startups</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Introducing Manamorphosis: A Diffusion Model for MTG Deck Generation</h1><div class=post-meta><span title='2025-05-05 00:00:00 +0000 UTC'>May 5, 2025</span>&nbsp;·&nbsp;16 min&nbsp;·&nbsp;3311 words&nbsp;·&nbsp;Jake Boggs</div></header><div class=post-content><p>This post details <strong>Manamorphosis</strong>, a first-of-its-kind diffusion model developed to complete Magic: The Gathering decklists. It takes a set of known cards and fills in the rest to form a 60-card main deck. Subsequently, using the completed main deck as context, it can complete a 15-card sideboard. The core generative mechanism is based on Denoising Diffusion Probabilistic Models (DDPMs), the same family of models powering many image generation systems like Stable Diffusion and Midjourney, but adapted here for the unique domain of card sets. Applying AI models to MTG has long been a pet project of mine and I&rsquo;m exciting to share this model, as I believe it is the state-of-the-art (and only) AI model dedicated to understanding deck construction.</p><h2 id=demo-video>Demo Video<a hidden class=anchor aria-hidden=true href=#demo-video>#</a></h2><video width=100% controls>
<source src=/videos/manamorphosis.mp4 type=video/mp4>Your browser does not support the video tag.</video><h2 id=card-representation-doc2vec-embeddings>Card Representation: Doc2Vec Embeddings<a hidden class=anchor aria-hidden=true href=#card-representation-doc2vec-embeddings>#</a></h2><p>Card identity is represented by 128-dimensional vectors (<code>EMB_DIM=128</code>). These embeddings are generated by training a Doc2Vec model (<code>train_embedding_model.py</code>) on preprocessed text data for each card obtained from MTGJSON&rsquo;s <code>AtomicCards.json</code>. This captures semantic relationships based on card text. Much like modern RAG engines use embeddings to understand the meaning behind search queries and documents, these Doc2Vec embeddings allow the system to grasp the functional similarities between cards based on their textual descriptions (cost, type, rules).</p><h3 id=rationale-for-using-embeddings>Rationale for Using Embeddings<a hidden class=anchor aria-hidden=true href=#rationale-for-using-embeddings>#</a></h3><p>Instead of training the model to directly predict discrete cards from a vocabulary of ~28,000+, Manamorphosis uses pre-trained Doc2Vec embeddings as an intermediate representation. This approach offers several advantages:</p><ol><li><strong>Handling Data Sparsity:</strong> The training dataset (~47,000 decks) contains only a fraction (~5,000) of all legal MTG cards. A model predicting cards directly would struggle to learn meaningful representations or generation logic for the vast majority of cards rarely or never seen during training. The embedding model, trained on <em>all</em> card text, provides a representation for every card.</li><li><strong>Generalization to New/Unseen Cards:</strong> Because the embedding is derived from card text, the system can generate an embedding for <em>any</em> card, including newly released ones, without retraining the embedding model (though retraining the diffusion model might improve performance with new metagames). The diffusion model learns to operate on the <em>semantic meaning</em> captured in the 128-dimensional embedding space, rather than being limited to the fixed vocabulary seen during its own training. This contrasts with models that predict discrete tokens, which typically require a fixed, predefined vocabulary.</li><li><strong>Capturing Semantic Relationships:</strong> Doc2Vec learns vectors where cards with similar functions, costs, types, or textual patterns (e.g., different variations of counterspells, cheap red burn spells, evasive creatures) are closer together in the embedding space. This allows the diffusion model to learn higher-level concepts (&ldquo;needs more removal,&rdquo; &ldquo;add card draw&rdquo;) rather than just memorizing specific card co-occurrences, leading to potentially more robust and contextually relevant deck completions. This focus on semantic similarity is analogous to how embedding-based search engines return results that are conceptually related, not just keyword matches.</li><li><strong>Dimensionality Reduction & Decoupling:</strong> Working with dense 128-dimensional vectors is more computationally manageable for the transformer architecture than using extremely high-dimensional one-hot vectors (one per card). It also decouples the task of understanding card text semantics (Doc2Vec) from the task of generative deck construction (Diffusion Model).</li></ol><h3 id=card-text-preprocessing-train_embedding_modelpyget_text>Card Text Preprocessing (<code>train_embedding_model.py:get_text</code>)<a hidden class=anchor aria-hidden=true href=#card-text-preprocessing-train_embedding_modelpyget_text>#</a></h3><p>The <code>get_text</code> function preprocesses card data into a consistent string format suitable for Doc2Vec training. Specific choices include:</p><ul><li><strong>Mana Cost:</strong> Replaced curly braces <code>{}</code> with pipe symbols <code>|</code> (<code>|W|</code>, <code>|U|</code>, etc.) and ensured spacing around symbols (<code>{W}{U}</code> -> <code>|W| |U|</code>). This treats each mana symbol as a distinct token and distinguishes them from mana symbols in the card text.</li><li><strong>Power/Toughness:</strong> Represented as <code>$Power$ #Toughness#</code> (e.g., <code>$2$ #2#</code>). This creates unique tokens for P/T values.</li><li><strong>Card Type:</strong> The supertype (e.g., &ldquo;Creature&rdquo;, &ldquo;Instant&rdquo;) is split into individual tokens surrounded by pipes (<code>|Creature|</code>, <code>|Instant|</code>). Subtypes (e.g., &ldquo;Goblin&rdquo;, &ldquo;Wizard&rdquo;) are kept as single words.</li><li><strong>Rules Text:</strong><ul><li>Card name references replaced with <code>@</code>. This prevents the model from overfitting to specific card names and focuses on the actions/effects.</li><li>Common self-references (&ldquo;this creature&rdquo;, &ldquo;this enchantment&rdquo;, etc.) also replaced with <code>@</code>.</li><li>Line breaks, semicolons replaced with spaces. Colons have spaces added (<code>:</code> -> <code>:</code>).</li><li>Reminder text (within parentheses) is removed using regex (<code>re.sub(reminder_remover, '', ...)</code>).</li><li>Special characters like <code>&</code>, <code>−</code>, <code>—</code>, <code>'</code>, <code>,</code>, <code>.</code>, <code>'</code>, <code>"</code> are handled (replaced or removed).</li><li>Text is converted to lowercase.</li></ul></li><li><strong>Stop Words:</strong> Common English stop words (like &ldquo;the&rdquo;, &ldquo;a&rdquo;, &ldquo;is&rdquo;) are removed using <code>nltk.corpus.stopwords</code> to reduce noise and focus on meaningful terms.</li></ul><p>This preprocessing aims to convert structured card information and natural language text into a sequence of meaningful tokens that the Doc2Vec model can learn relationships from.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># From train_embedding_model.py (Illustrative snippet)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_text</span>(card):
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#39;manaCost&#39;</span> <span style=color:#f92672>in</span> card:
</span></span><span style=display:flex><span>        text <span style=color:#f92672>+=</span> card[<span style=color:#e6db74>&#39;manaCost&#39;</span>]<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39;}{&#39;</span>, <span style=color:#e6db74>&#39;} {&#39;</span>)<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39;{&#39;</span>, <span style=color:#e6db74>&#39;|&#39;</span>)<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39;}&#39;</span>, <span style=color:#e6db74>&#39;|&#39;</span>) <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39; &#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#39;power&#39;</span> <span style=color:#f92672>in</span> card:
</span></span><span style=display:flex><span>        text <span style=color:#f92672>+=</span> <span style=color:#e6db74>&#39;$&#39;</span> <span style=color:#f92672>+</span> card[<span style=color:#e6db74>&#39;power&#39;</span>] <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;$ #&#39;</span> <span style=color:#f92672>+</span> card[<span style=color:#e6db74>&#39;toughness&#39;</span>] <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;# &#39;</span>
</span></span><span style=display:flex><span>    text <span style=color:#f92672>+=</span> <span style=color:#e6db74>&#39; &#39;</span><span style=color:#f92672>.</span>join([<span style=color:#e6db74>&#39;|&#39;</span> <span style=color:#f92672>+</span> word <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;|&#39;</span> <span style=color:#66d9ef>for</span> word <span style=color:#f92672>in</span> card[<span style=color:#e6db74>&#39;type&#39;</span>]<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#39; — &#39;</span>)[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>split()]) <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39; &#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#39;—&#39;</span> <span style=color:#f92672>in</span> card[<span style=color:#e6db74>&#39;type&#39;</span>]:
</span></span><span style=display:flex><span>        text <span style=color:#f92672>+=</span> card[<span style=color:#e6db74>&#39;type&#39;</span>]<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#39; — &#39;</span>)[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39; &#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#39;text&#39;</span> <span style=color:#f92672>in</span> card:
</span></span><span style=display:flex><span>        <span style=color:#75715e># ... (Handling basic lands) ...</span>
</span></span><span style=display:flex><span>        processed_text <span style=color:#f92672>=</span> card[<span style=color:#e6db74>&#39;text&#39;</span>]<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39;&amp;&#39;</span>, <span style=color:#e6db74>&#39;and&#39;</span>)<span style=color:#f92672>.</span>replace(card[<span style=color:#e6db74>&#39;name&#39;</span>], <span style=color:#e6db74>&#39;@&#39;</span>)<span style=color:#f92672>.</span>replace(card[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#39;,&#39;</span>)[<span style=color:#ae81ff>0</span>], <span style=color:#e6db74>&#39;@&#39;</span>) <span style=color:#75715e># simplified</span>
</span></span><span style=display:flex><span>        processed_text <span style=color:#f92672>=</span> processed_text<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39;this creature&#39;</span>, <span style=color:#e6db74>&#39;@&#39;</span>)<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39;this enchantment&#39;</span>, <span style=color:#e6db74>&#39;@&#39;</span>)<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39;this artifact&#39;</span>, <span style=color:#e6db74>&#39;@&#39;</span>)<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39;this land&#39;</span>, <span style=color:#e6db74>&#39;@&#39;</span>)
</span></span><span style=display:flex><span>        processed_text <span style=color:#f92672>=</span> processed_text<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\\</span><span style=color:#e6db74>n&#39;</span>, <span style=color:#e6db74>&#39; &#39;</span>)<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39;;&#39;</span>, <span style=color:#e6db74>&#39; &#39;</span>)<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39;:&#39;</span>, <span style=color:#e6db74>&#39; :&#39;</span>)<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39;|&#39;</span>, <span style=color:#e6db74>&#39;•&#39;</span>)
</span></span><span style=display:flex><span>        text <span style=color:#f92672>+=</span> processed_text
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(reminder_remover, <span style=color:#e6db74>&#39;&#39;</span>, text<span style=color:#f92672>.</span>lower()<span style=color:#f92672>...</span> ) <span style=color:#75715e># Lowercasing, punctuation, etc.</span>
</span></span><span style=display:flex><span>    words <span style=color:#f92672>=</span> [word <span style=color:#66d9ef>for</span> word <span style=color:#f92672>in</span> text<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#39; &#39;</span>) <span style=color:#66d9ef>if</span> word <span style=color:#f92672>!=</span> <span style=color:#e6db74>&#39;&#39;</span>]
</span></span><span style=display:flex><span>    filtered_words <span style=color:#f92672>=</span> [word <span style=color:#66d9ef>for</span> word <span style=color:#f92672>in</span> words <span style=color:#66d9ef>if</span> word <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> stop_words]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#39; &#39;</span><span style=color:#f92672>.</span>join(filtered_words)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Doc2Vec Training (Conceptual)</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> Doc2Vec(vector_size<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>, dm<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, dbow_words<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, min_count<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>200</span>, workers<span style=color:#f92672>=</span>cores, seed<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>build_vocab(corpus) <span style=color:#75715e># corpus yields TaggedDocument(processed_text.split(), [card_id])</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>train(corpus, <span style=color:#f92672>...</span>)
</span></span></code></pre></div><p>A simple linear classifier (<code>train_embedding_classifier.py</code>) is trained separately to map these 128-dim embeddings back to unique card indices. This classifier is essential during the reverse diffusion process to identify the most likely card corresponding to a denoised embedding vector. This is more efficient than performing cosine similarity search for each generated embedding during inference.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># From train_embedding_classifier.py</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>CardClassifier</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, embedding_dim, num_classes):
</span></span><span style=display:flex><span>        super(CardClassifier, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>network <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(embedding_dim, num_classes)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>network(x)
</span></span></code></pre></div><h2 id=denoising-diffusion-probabilistic-model-ddpm-framework>Denoising Diffusion Probabilistic Model (DDPM) Framework<a hidden class=anchor aria-hidden=true href=#denoising-diffusion-probabilistic-model-ddpm-framework>#</a></h2><p>The core generation mechanism is a DDPM, mirroring the approach used in image generation.</p><ol><li><p><strong>Forward Process (Noise Addition):</strong> Starting with the true deck embeddings <code>x0</code>, Gaussian noise is progressively added over <code>T</code> timesteps (here, <code>T=1000</code>). This is analogous to how image diffusion models start with a clear image and gradually add noise until only static remains. The noise level at each step is determined by a predefined variance schedule, specifically a cosine schedule (<code>cosine_beta_schedule</code>).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># diffusion_model.py</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>cosine_beta_schedule</span>(T, s<span style=color:#f92672>=</span><span style=color:#ae81ff>0.008</span>):
</span></span><span style=display:flex><span>    steps <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>linspace(<span style=color:#ae81ff>0</span>, T, T <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float64)
</span></span><span style=display:flex><span>    alpha_bar <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cos(((steps <span style=color:#f92672>/</span> T) <span style=color:#f92672>+</span> s) <span style=color:#f92672>/</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> s) <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>pi <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.5</span>) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>    alpha_bar <span style=color:#f92672>=</span> alpha_bar <span style=color:#f92672>/</span> alpha_bar[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    betas <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> (alpha_bar[<span style=color:#ae81ff>1</span>:] <span style=color:#f92672>/</span> alpha_bar[:<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>clip(betas, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0.999</span>)<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Within DiffusionTrainer class:</span>
</span></span><span style=display:flex><span>beta <span style=color:#f92672>=</span> cosine_beta_schedule(T)<span style=color:#f92672>.</span>to(device) <span style=color:#75715e># T = TIMESTEPS (e.g., 1000)</span>
</span></span><span style=display:flex><span>alpha <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span> <span style=color:#f92672>-</span> beta
</span></span><span style=display:flex><span>alpha_bar <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cumprod(alpha, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># Precompute terms used in diffusion and sampling:</span>
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>register(<span style=color:#e6db74>&#34;sqrt_alpha_bar&#34;</span>, torch<span style=color:#f92672>.</span>sqrt(alpha_bar))
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>register(<span style=color:#e6db74>&#34;sqrt_one_minus_alpha_bar&#34;</span>, torch<span style=color:#f92672>.</span>sqrt(<span style=color:#ae81ff>1.0</span> <span style=color:#f92672>-</span> alpha_bar))
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>register(<span style=color:#e6db74>&#34;beta&#34;</span>, beta)
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>register(<span style=color:#e6db74>&#34;alpha&#34;</span>, alpha)
</span></span><span style=display:flex><span><span style=color:#75715e># ... (other registered buffers)</span>
</span></span></code></pre></div><p>The state <code>x_t</code> at timestep <code>t</code> can be sampled directly using the cumulative product <code>alpha_bar</code>:
<code>x_t = sqrt(alpha_bar_t) * x0 + sqrt(1 - alpha_bar_t) * noise</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># diffusion_model.py: DiffusionTrainer.q_sample</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Note: The actual implementation modifies x_t for known cards</span>
</span></span><span style=display:flex><span><span style=color:#75715e># based on the mask *after* the initial noise addition,</span>
</span></span><span style=display:flex><span><span style=color:#75715e># returning a mix of original x0 and noised x_t.</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>q_sample</span>(self, x0, t, mask):
</span></span><span style=display:flex><span>    mask_expanded <span style=color:#f92672>=</span> mask<span style=color:#f92672>.</span>expand_as(x0)
</span></span><span style=display:flex><span>    sqrt_ab <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_extract(self<span style=color:#f92672>.</span>sqrt_alpha_bar, t, x0<span style=color:#f92672>.</span>shape, x0<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>    sqrt_1mab <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_extract(self<span style=color:#f92672>.</span>sqrt_one_minus_alpha_bar, t, x0<span style=color:#f92672>.</span>shape, x0<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>    noise <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn_like(x0)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Calculate the fully noised version first</span>
</span></span><span style=display:flex><span>    x_t_noised <span style=color:#f92672>=</span> sqrt_ab <span style=color:#f92672>*</span> x0 <span style=color:#f92672>+</span> sqrt_1mab <span style=color:#f92672>*</span> noise
</span></span><span style=display:flex><span>    <span style=color:#75715e># Return original embeddings for known positions, noised for unknown</span>
</span></span><span style=display:flex><span>    x_t_masked <span style=color:#f92672>=</span> mask_expanded <span style=color:#f92672>*</span> x0 <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> mask_expanded) <span style=color:#f92672>*</span> x_t_noised
</span></span><span style=display:flex><span>    <span style=color:#75715e># Returns the masked noisy sample and the *original* noise (for loss)</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x_t_masked, noise
</span></span></code></pre></div></li><li><p><strong>Reverse Process (Denoising):</strong> The model learns to predict the noise <code>epsilon</code> added at timestep <code>t</code>. Starting from pure noise <code>x_T</code>, the model iteratively refines the embeddings by predicting the noise <code>epsilon_pred = model(x_t, x0, sb_x_t, t, mask, sb_mask)</code> and estimating <code>x_{t-1}</code> until <code>x0</code>, the original noise-free main deck, is reached. During inference, the actual denoising step involves sampling <code>x_{t-1}</code> by subtracting the predicted noise <code>epsilon_pred</code> from <code>x_t</code>, then adding a smaller amount of noise for the next timestep.</p></li></ol><h2 id=model-architecture-diffusion_modelpydiffusionmodel>Model Architecture (<code>diffusion_model.py:DiffusionModel</code>)<a hidden class=anchor aria-hidden=true href=#model-architecture-diffusion_modelpydiffusionmodel>#</a></h2><p>The model uses a transformer-based architecture, the same core building block behind Large Language Models like GPT and BERT, but adapted for set-based data rather than sequences. It lacks positional embeddings, treating decks as unordered sets, which differs from typical NLP or vision transformer usage where sequence order is crucial. It has distinct paths for main deck and sideboard processing. The internal model dimension is <code>model_dim=384</code>, and the embedding dimension is <code>EMB_DIM=128</code>.</p><ol><li><p><strong>Time Embeddings:</strong> Timestep <code>t</code> is encoded using standard sinusoidal embeddings, processed by separate MLPs for main deck and sideboard paths.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># diffusion_model.py</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sinusoidal_embedding</span>(t: torch<span style=color:#f92672>.</span>Tensor, dim: int <span style=color:#f92672>=</span> EMB_DIM):
</span></span><span style=display:flex><span>    half <span style=color:#f92672>=</span> dim <span style=color:#f92672>//</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>    freqs <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>exp(<span style=color:#f92672>-</span>math<span style=color:#f92672>.</span>log(<span style=color:#ae81ff>10000</span>) <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>arange(half, device<span style=color:#f92672>=</span>t<span style=color:#f92672>.</span>device) <span style=color:#f92672>/</span> (half <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    args <span style=color:#f92672>=</span> t[:, <span style=color:#66d9ef>None</span>] <span style=color:#f92672>*</span> freqs[<span style=color:#66d9ef>None</span>]
</span></span><span style=display:flex><span>    emb <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat((args<span style=color:#f92672>.</span>sin(), args<span style=color:#f92672>.</span>cos()), dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> dim <span style=color:#f92672>%</span> <span style=color:#ae81ff>2</span>:
</span></span><span style=display:flex><span>        emb <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>pad(emb, (<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> emb
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Within DiffusionModel.__init__</span>
</span></span><span style=display:flex><span>ff_dim <span style=color:#f92672>=</span> cfg[<span style=color:#e6db74>&#34;dim_feedforward&#34;</span>] <span style=color:#75715e># 3072</span>
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>main_time_mlp <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Linear(EMB_DIM, ff_dim), nn<span style=color:#f92672>.</span>SiLU(), nn<span style=color:#f92672>.</span>Linear(ff_dim, EMB_DIM),
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>sb_time_mlp <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential( <span style=color:#75715e># For Sideboard Decoder path</span>
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Linear(EMB_DIM, ff_dim), nn<span style=color:#f92672>.</span>SiLU(), nn<span style=color:#f92672>.</span>Linear(ff_dim, EMB_DIM),
</span></span><span style=display:flex><span>)
</span></span></code></pre></div></li><li><p><strong>Mask Embeddings:</strong> Binary masks (1.0 for known, 0.0 for unknown) are processed by separate MLPs.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Within DiffusionModel.__init__</span>
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>main_mask_mlp <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>1</span>, EMB_DIM), nn<span style=color:#f92672>.</span>SiLU(), nn<span style=color:#f92672>.</span>Linear(EMB_DIM, EMB_DIM),
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>sb_mask_mlp <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential( <span style=color:#75715e># For Sideboard Decoder path</span>
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>1</span>, ff_dim), nn<span style=color:#f92672>.</span>SiLU(), nn<span style=color:#f92672>.</span>Linear(ff_dim, EMB_DIM), <span style=color:#75715e># Note: intermediate dim is ff_dim here</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div></li><li><p><strong>Input Processing:</strong> Input embeddings <code>x_t</code> (main) or <code>sb_x_t</code> (sideboard) are combined via addition with their respective time and mask embeddings, then projected to <code>model_dim</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Within DiffusionModel.forward</span>
</span></span><span style=display:flex><span>sin_emb <span style=color:#f92672>=</span> sinusoidal_embedding(t, EMB_DIM) <span style=color:#75715e># Shape: [Batch, EMB_DIM]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Main Deck Input</span>
</span></span><span style=display:flex><span>main_t_emb_flat <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>main_time_mlp(sin_emb) <span style=color:#75715e># Shape: [Batch, EMB_DIM]</span>
</span></span><span style=display:flex><span>main_t_emb <span style=color:#f92672>=</span> main_t_emb_flat[:, <span style=color:#66d9ef>None</span>, :]<span style=color:#f92672>.</span>expand(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, DECK_SIZE, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>) <span style=color:#75715e># Shape: [Batch, DECK_SIZE, EMB_DIM]</span>
</span></span><span style=display:flex><span>main_mask_emb <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>main_mask_mlp(mask) <span style=color:#75715e># mask shape: [Batch, DECK_SIZE, 1] -&gt; Output: [Batch, DECK_SIZE, EMB_DIM]</span>
</span></span><span style=display:flex><span>h_main <span style=color:#f92672>=</span> x_t <span style=color:#f92672>+</span> main_t_emb <span style=color:#f92672>+</span> main_mask_emb <span style=color:#75715e># x_t shape: [Batch, DECK_SIZE, EMB_DIM]</span>
</span></span><span style=display:flex><span>h_main_proj <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>main_input_proj(h_main) <span style=color:#75715e># Linear(EMB_DIM, model_dim) -&gt; Shape: [Batch, DECK_SIZE, model_dim]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Sideboard Input</span>
</span></span><span style=display:flex><span>sb_decoder_t_emb_flat <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>sb_time_mlp(sin_emb) <span style=color:#75715e># Shape: [Batch, EMB_DIM]</span>
</span></span><span style=display:flex><span>sb_decoder_t_emb <span style=color:#f92672>=</span> sb_decoder_t_emb_flat[:, <span style=color:#66d9ef>None</span>, :]<span style=color:#f92672>.</span>expand(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, SIDEBOARD_SIZE, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>) <span style=color:#75715e># Shape: [Batch, SIDEBOARD_SIZE, EMB_DIM]</span>
</span></span><span style=display:flex><span>sb_decoder_mask_emb <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>sb_mask_mlp(sb_mask) <span style=color:#75715e># sb_mask shape: [Batch, SIDEBOARD_SIZE, 1] -&gt; Output: [Batch, SIDEBOARD_SIZE, EMB_DIM]</span>
</span></span><span style=display:flex><span>h_sb <span style=color:#f92672>=</span> sb_x_t <span style=color:#f92672>+</span> sb_decoder_t_emb <span style=color:#f92672>+</span> sb_decoder_mask_emb <span style=color:#75715e># sb_x_t shape: [Batch, SIDEBOARD_SIZE, EMB_DIM]</span>
</span></span><span style=display:flex><span>h_sb_proj <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>sb_input_proj(h_sb) <span style=color:#75715e># Linear(EMB_DIM, model_dim) -&gt; Shape: [Batch, SIDEBOARD_SIZE, model_dim]</span>
</span></span></code></pre></div></li><li><p><strong>Main Deck Path (Encoder):</strong> Processes <code>h_main_proj</code> through a standard <code>nn.TransformerEncoder</code> (<code>layers=8</code>, <code>nhead=8</code>). The output is projected back to <code>EMB_DIM</code> to predict main deck noise (<code>main_noise_pred</code>).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Within DiffusionModel.__init__</span>
</span></span><span style=display:flex><span>main_encoder_layer <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>TransformerEncoderLayer(d_model<span style=color:#f92672>=</span>model_dim, nhead<span style=color:#f92672>=</span>nhead, <span style=color:#f92672>...</span>)
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>main_transformer_encoder <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>TransformerEncoder(main_encoder_layer, num_layers<span style=color:#f92672>=</span>num_layers)
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>main_output_proj <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(model_dim, EMB_DIM)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Within DiffusionModel.forward</span>
</span></span><span style=display:flex><span>main_encoded <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>main_transformer_encoder(h_main_proj)
</span></span><span style=display:flex><span>main_noise_pred <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>main_output_proj(main_encoded)
</span></span></code></pre></div></li><li><p><strong>Sideboard Context Path (Encoder):</strong> Processes the <em>original</em> main deck embeddings <code>x0</code> (noise-free) through a separate, shallow <code>nn.TransformerEncoder</code> (<code>num_layers=1</code>) to create context (<code>sb_context_encoded</code>). This context is used by the sideboard decoder.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Within DiffusionModel.__init__</span>
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>sb_context_input_proj <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(EMB_DIM, model_dim)
</span></span><span style=display:flex><span>sb_context_encoder_layer <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>TransformerEncoderLayer(d_model<span style=color:#f92672>=</span>model_dim, nhead<span style=color:#f92672>=</span>nhead, <span style=color:#f92672>...</span>)
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>sideboard_context_encoder <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>TransformerEncoder(sb_context_encoder_layer, num_layers<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Within DiffusionModel.forward</span>
</span></span><span style=display:flex><span>h_sb_context <span style=color:#f92672>=</span> x0 <span style=color:#75715e># Original main deck embeddings</span>
</span></span><span style=display:flex><span>h_sb_context_proj <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>sb_context_input_proj(h_sb_context)
</span></span><span style=display:flex><span>sb_context_encoded <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>sideboard_context_encoder(h_sb_context_proj)
</span></span></code></pre></div></li><li><p><strong>Sideboard Path (Decoder):</strong> Processes projected sideboard embeddings <code>h_sb_proj</code> using a <code>nn.TransformerDecoder</code> (<code>num_layers=1</code>), conditioned on <code>sb_context_encoded</code> via cross-attention (<code>memory</code>). The decoder output is passed through <em>another</em> <code>nn.TransformerEncoder</code> (<code>sb_layers=8</code>) before the final projection back to <code>EMB_DIM</code> to predict sideboard noise (<code>sb_noise_pred</code>). The use of cross-attention to condition the sideboard generation on the main deck context is conceptually similar to how text-to-image models use cross-attention to condition image generation on a text prompt embedding.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Within DiffusionModel.__init__</span>
</span></span><span style=display:flex><span>sb_decoder_layer <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>TransformerDecoderLayer(d_model<span style=color:#f92672>=</span>model_dim, nhead<span style=color:#f92672>=</span>nhead, <span style=color:#f92672>...</span>)
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>sb_transformer_decoder <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>TransformerDecoder(sb_decoder_layer, num_layers<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># Uses the same sb_context_encoder_layer definition for the subsequent encoder</span>
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>sb_transformer_output <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>TransformerEncoder(sb_context_encoder_layer, num_layers<span style=color:#f92672>=</span>sb_num_layers)
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>sb_output_proj <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(model_dim, EMB_DIM)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Within DiffusionModel.forward</span>
</span></span><span style=display:flex><span>sb_decoded <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>sb_transformer_decoder(tgt<span style=color:#f92672>=</span>h_sb_proj, memory<span style=color:#f92672>=</span>sb_context_encoded)
</span></span><span style=display:flex><span>sb_decoded <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>sb_transformer_output(sb_decoded)
</span></span><span style=display:flex><span>sb_noise_pred <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>sb_output_proj(sb_decoded)
</span></span></code></pre></div></li></ol><h2 id=conditioning-masking-strategy>Conditioning: Masking Strategy<a hidden class=anchor aria-hidden=true href=#conditioning-masking-strategy>#</a></h2><p>Conditional generation (deck completion) is handled via masking. Known card embeddings are provided by the user (or determined during training). This mechanism is analogous to providing a starting image and a mask for inpainting in image generation models, or providing a text prompt to guide generation.</p><ul><li><p><strong>Training:</strong> For each training sample (<code>x0_embeddings</code>, <code>x0_indices</code>, etc.), multiple masks (<code>masks_per_deck</code>) are generated dynamically per deck.</p><ul><li>The number of known main deck cards <code>k_main</code> is sampled from partitioned ranges [1, 59] across the generated masks to ensure diverse <code>k</code> values are seen.</li><li>Sideboard <code>k_sb</code> is sampled randomly from [1, 14], with a 50% chance of being forced to 0. This is done so that the model performs well at generating sideboards from scratch, which I expect to be a common use case.</li></ul></li><li><p><strong>Masking Logic (<code>diffusion_model.py:DiffusionTrainer._create_mask_row</code>):</strong> This function generates a single mask row (shape <code>[deck_size, 1]</code>) for a target <code>k</code>. It identifies unique available card indices in the current deck (<code>current_deck_indices</code>). It samples these unique cards <em>without replacement</em> using weights derived from pre-calculated popularity scores (<code>self.card_popularity</code>), slightly favoring less popular cards (<code>0.5 + score</code>, where score is <code>1.0 - normalized_count</code>). It iterates through these weighted, shuffled unique cards. For each unique card, with 85% probability, it attempts to mask all available copies (up to <code>k</code> remaining); with 15% probability, it masks a random number of available copies (from 1 up to available, limited by <code>k</code> remaining). This process repeats until <code>k</code> positions are marked as known (1.0 in the mask). We typically want to mask all instances of a card to make the task harder, so that the model has to learn what cards go together and not just to add additional copies.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># diffusion_model.py: DiffusionTrainer._create_mask_row (Simplified Pseudocode)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_create_mask_row</span>(k_target, deck_size, current_deck_indices, popularity_scores):
</span></span><span style=display:flex><span>    mask_row <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros(deck_size, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    available_indices <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>ones(deck_size, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>bool)
</span></span><span style=display:flex><span>    masked_count <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> masked_count <span style=color:#f92672>&lt;</span> k_target <span style=color:#f92672>and</span> available_indices<span style=color:#f92672>.</span>any():
</span></span><span style=display:flex><span>        <span style=color:#75715e># 1. Get unique card indices from currently *available* positions</span>
</span></span><span style=display:flex><span>        unique_cards <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>unique(current_deck_indices[available_indices])
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> unique_cards: <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 2. Calculate sampling weights (favor less popular)</span>
</span></span><span style=display:flex><span>        weights <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>0.5</span> <span style=color:#f92672>+</span> popularity_scores<span style=color:#f92672>.</span>get(idx<span style=color:#f92672>.</span>item(), <span style=color:#ae81ff>1.0</span>) <span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> unique_cards])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 3. Sample unique cards without replacement based on weights</span>
</span></span><span style=display:flex><span>        perm_indices <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>multinomial(weights, num_samples<span style=color:#f92672>=</span>len(unique_cards), replacement<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        shuffled_unique_cards <span style=color:#f92672>=</span> unique_cards[perm_indices]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> card_idx <span style=color:#f92672>in</span> shuffled_unique_cards:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> masked_count <span style=color:#f92672>&gt;=</span> k_target: <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 4. Find available positions for this specific card_idx</span>
</span></span><span style=display:flex><span>            potential_pos <span style=color:#f92672>=</span> (current_deck_indices <span style=color:#f92672>==</span> card_idx)<span style=color:#f92672>.</span>nonzero(as_tuple<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>            available_pos <span style=color:#f92672>=</span> potential_pos[available_indices[potential_pos]] <span style=color:#75715e># Filter by available</span>
</span></span><span style=display:flex><span>            available_count <span style=color:#f92672>=</span> len(available_pos)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> available_count <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>: <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            needed <span style=color:#f92672>=</span> k_target <span style=color:#f92672>-</span> masked_count
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 5. Decide how many copies to mask (85% all available, 15% random count)</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> random<span style=color:#f92672>.</span>random() <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0.85</span>:
</span></span><span style=display:flex><span>                num_to_mask <span style=color:#f92672>=</span> min(available_count, needed)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                max_can_mask <span style=color:#f92672>=</span> min(available_count, needed)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> max_can_mask <span style=color:#f92672>&lt;=</span> <span style=color:#ae81ff>0</span>: <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>                num_to_mask <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>1</span>, max(<span style=color:#ae81ff>1</span>, max_can_mask))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 6. Select specific positions to mask and update mask_row/available_indices</span>
</span></span><span style=display:flex><span>            indices_to_mask <span style=color:#f92672>=</span> available_pos[torch<span style=color:#f92672>.</span>randperm(available_count)[:num_to_mask]]
</span></span><span style=display:flex><span>            mask_row[indices_to_mask] <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span>            available_indices[indices_to_mask] <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>            masked_count <span style=color:#f92672>+=</span> num_to_mask
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> mask_row
</span></span></code></pre></div></li><li><p><strong>Loss Calculation:</strong> The MSE loss is computed only between the predicted noise (<code>main_noise_pred</code>, <code>sb_noise_pred</code>) and the true noise (<code>noise</code>, <code>sb_noise</code> from <code>q_sample</code>) for the <em>unknown</em> (mask value 0.0) card slots. This focuses the model on learning to generate the missing parts.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># diffusion_model.py: DiffusionTrainer.p_losses</span>
</span></span><span style=display:flex><span>main_loss <span style=color:#f92672>=</span> ((noise <span style=color:#f92672>-</span> main_noise_pred) <span style=color:#f92672>*</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> mask<span style=color:#f92672>.</span>expand_as(noise)))<span style=color:#f92672>.</span>pow(<span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>sb_loss <span style=color:#f92672>=</span> ((sb_noise <span style=color:#f92672>-</span> sb_noise_pred) <span style=color:#f92672>*</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> sb_mask<span style=color:#f92672>.</span>expand_as(sb_noise)))<span style=color:#f92672>.</span>pow(<span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>total_loss <span style=color:#f92672>=</span> main_loss <span style=color:#f92672>+</span> sb_loss
</span></span></code></pre></div></li><li><p><strong>Inference:</strong> During the reverse diffusion process (sampling <code>x_{t-1}</code> from <code>x_t</code>), the known card embeddings <code>x0_known</code> (provided by the user) are reapplied at each step to guide the generation towards the desired completion. A common approach (simplified):</p><ol><li>Predict noise: <code>epsilon_pred = model(x_t, x0_context, sb_x_t, t, mask, sb_mask)</code></li><li>Calculate the parameters (mean, variance) of the distribution <code>p(x_{t-1} | x_t)</code> using <code>x_t</code>, <code>t</code>, and <code>epsilon_pred</code> according to the diffusion schedule.</li><li>Sample the potential next state <code>x_{t-1}_sample</code> from this distribution (adding noise if <code>t > 0</code>, otherwise using the mean).</li><li>Re-apply knowns to the sample: <code>x_{t-1}_conditioned = mask * x0_known + (1 - mask) * x_{t-1}_sample</code>.</li><li>Use <code>x_{t-1}_conditioned</code> as the input <code>x_t</code> for the next step (t-2).
The main deck context <code>sb_context_encoded</code> for the sideboard decoder is generated from the <em>final</em> denoised main deck embeddings (<code>x0_main_final</code>).</li></ol></li></ul><h2 id=training-diffusion_modelpydiffusiontrainer>Training (<code>diffusion_model.py:DiffusionTrainer</code>)<a hidden class=anchor aria-hidden=true href=#training-diffusion_modelpydiffusiontrainer>#</a></h2><p>The current model was trained using ~47,000 decks scraped from MTGTop8 and is format agnostic, with the training data covering Standard, Modern, Pioneer, Pauper, Legacy, and Vintage. The full model contains ~56 million parameters. Due to its small size, training was feasible on consumer hardware, specifically a single Nvidia 3050 Laptop GPU with 4GB of VRAM, taking roughly 4 days to complete 100 epochs.</p><ul><li><strong>Dataset:</strong> <code>DeckDataset</code> loads decks and filters for exact 60 main deck / 15 sideboard card counts. It converts card names to the pre-trained Doc2Vec embeddings (<code>card_embeddings.pkl</code>) and retrieves corresponding integer indices using the mapping from the trained classifier (<code>card_classifier.pt</code>). Decks with cards missing from embeddings or the classifier map are skipped. It also calculates card popularity scores based on deck frequency for the masking strategy.</li><li><strong>Optimizer:</strong> AdamW (<code>torch.optim.AdamW</code>) with learning rate (<code>lr=5e-6</code> default) and weight decay (<code>diff_weight_decay=1e-3</code> default).</li><li><strong>Objective:</strong> Minimize the combined MSE loss <code>total_loss</code> described above, calculated over <code>masks_per_deck</code> different masks for each deck in the batch.</li><li><strong>Process:</strong> Standard PyTorch training loop: iterates epochs, loads batches via DataLoader, calculates loss using <code>p_losses</code>, performs backpropagation (<code>total_loss.backward()</code>), clips gradients (<code>nn.utils.clip_grad_norm_</code>), updates optimizer (<code>opt.step()</code>). Checkpoints containing model state dict, epoch, and config are saved periodically (<code>torch.save</code>).</li></ul><h2 id=inference-enforcing-deck-rules-with-iterative-refinement>Inference: Enforcing Deck Rules with Iterative Refinement<a hidden class=anchor aria-hidden=true href=#inference-enforcing-deck-rules-with-iterative-refinement>#</a></h2><p>While the diffusion model learns the underlying patterns of deck construction from the training data, it doesn&rsquo;t inherently guarantee adherence to strict game rules like the 4-copy limit for non-basic cards or format legality during the raw generation process. To address this, the inference functions employ an iterative refinement strategy after the initial denoising pass:</p><ol><li><strong>Initial Generation:</strong> The standard reverse diffusion process is performed once to generate initial embeddings for all unknown card slots, conditioned on any user-provided cards.</li><li><strong>Classification & Rule Check:</strong> The resulting embeddings (both originally known and newly generated) are converted back to card names using the trained linear classifier (<code>CardClassifier</code>). The system then checks for violations:<ul><li><strong>4-Copy Limit:</strong> It counts occurrences of each non-basic card name. For sideboard generation, this count considers cards in both the main deck and the current sideboard iteration.</li><li><strong>Format Legality:</strong> Using preloaded card data (derived from MTGJSON&rsquo;s <code>AtomicCards.json</code>), it verifies if each <em>generated</em> card is legal in the specified format (e.g., &lsquo;Modern&rsquo;, &lsquo;Standard&rsquo;). Basic lands are exempt from this check.</li></ul></li><li><strong>Identify Violations:</strong> The system identifies the specific <em>generated</em> card slots that violate either the 4-copy limit or format legality. User-provided cards are never marked for regeneration.</li><li><strong>Mask Update & Regeneration:</strong> A new mask is created. User-provided cards and <em>valid</em> generated cards from the current iteration are marked as &ldquo;known&rdquo;. Slots corresponding to rule violations are marked as &ldquo;unknown&rdquo;.</li><li><strong>Re-run Diffusion:</strong> The reverse diffusion sampling process is run <em>again</em>, using the updated mask and the embeddings of the known cards (including the valid generated ones) as fixed context. The model only needs to generate new embeddings for the slots marked as unknown due to rule violations.</li><li><strong>Repeat:</strong> Steps 2-5 are repeated up to a fixed number of maximum refinement iterations (<code>MAX_REFINEMENT_ITERATIONS</code>). This loop continues until no rule violations are found among the generated cards or the iteration limit is reached.</li></ol><p>This refinement loop significantly improves the likelihood of producing valid and legal deck completions by correcting rule violations after the initial generation, leveraging the classifier and external card data to guide the process without needing to bake these complex constraints directly into the diffusion model&rsquo;s training objective. The final deck combines the original user input with the cards generated and refined through this process.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Manamorphosis applies diffusion models to Magic: The Gathering deck generation and was a fun, but time-consuming project. Developing this system, including embedding tuning, model architecture experiments, and refinement loops, was a substantial effort (two weeks of me procrastinating before finals).</p><p>While there&rsquo;s always more to explore, like testing different embeddings or format specializations, the current model provides a solid foundation for AI-driven deck completion.</p><p>The complete code is available on GitHub if you&rsquo;d like to run it yourself or see the nitty-gritty details: <a href=https://github.com/JakeBoggs/Manamorphosis>Manamorphosis GitHub repository</a>.</p><p>Twitter: <a href=https://x.com/JakeABoggs>@JakeABoggs</a></p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://boggs.tech/>Jake Boggs</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>