<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Daily Yap: A Synthetically Generated Conversational Audio Dataset | Jake Boggs</title>
<meta name=keywords content><meta name=description content="Training multimodal models often requires large, high-quality conversational audio datasets, which are currently scarce. This document details the creation of Daily Yap, a dataset developed to address this gap.
Existing conversational audio datasets present several limitations:
Content Scope: Many datasets focus on assistant-user interactions, lacking the breadth of topics found in general human dialogues. Audio-Text Alignment: Datasets with precise alignment between high-quality audio and accurate transcriptions are uncommon. Speaker Diversity: The use of few speakers limits the generalizability of models trained on these datasets."><meta name=author content="Jake Boggs"><link rel=canonical href=https://boggs.tech/posts/synthetic-conversational-audio-generation-daily-yap/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://boggs.tech/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://boggs.tech/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://boggs.tech/favicon-32x32.png><link rel=apple-touch-icon href=https://boggs.tech/apple-touch-icon.png><link rel=mask-icon href=https://boggs.tech/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://boggs.tech/posts/synthetic-conversational-audio-generation-daily-yap/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Daily Yap: A Synthetically Generated Conversational Audio Dataset"><meta property="og:description" content="Training multimodal models often requires large, high-quality conversational audio datasets, which are currently scarce. This document details the creation of Daily Yap, a dataset developed to address this gap.
Existing conversational audio datasets present several limitations:
Content Scope: Many datasets focus on assistant-user interactions, lacking the breadth of topics found in general human dialogues. Audio-Text Alignment: Datasets with precise alignment between high-quality audio and accurate transcriptions are uncommon. Speaker Diversity: The use of few speakers limits the generalizability of models trained on these datasets."><meta property="og:type" content="article"><meta property="og:url" content="https://boggs.tech/posts/synthetic-conversational-audio-generation-daily-yap/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-23T00:00:00+00:00"><meta property="article:modified_time" content="2024-06-23T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Daily Yap: A Synthetically Generated Conversational Audio Dataset"><meta name=twitter:description content="Training multimodal models often requires large, high-quality conversational audio datasets, which are currently scarce. This document details the creation of Daily Yap, a dataset developed to address this gap.
Existing conversational audio datasets present several limitations:
Content Scope: Many datasets focus on assistant-user interactions, lacking the breadth of topics found in general human dialogues. Audio-Text Alignment: Datasets with precise alignment between high-quality audio and accurate transcriptions are uncommon. Speaker Diversity: The use of few speakers limits the generalizability of models trained on these datasets."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://boggs.tech/posts/"},{"@type":"ListItem","position":2,"name":"Daily Yap: A Synthetically Generated Conversational Audio Dataset","item":"https://boggs.tech/posts/synthetic-conversational-audio-generation-daily-yap/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Daily Yap: A Synthetically Generated Conversational Audio Dataset","name":"Daily Yap: A Synthetically Generated Conversational Audio Dataset","description":"Training multimodal models often requires large, high-quality conversational audio datasets, which are currently scarce. This document details the creation of Daily Yap, a dataset developed to address this gap.\nExisting conversational audio datasets present several limitations:\nContent Scope: Many datasets focus on assistant-user interactions, lacking the breadth of topics found in general human dialogues. Audio-Text Alignment: Datasets with precise alignment between high-quality audio and accurate transcriptions are uncommon. Speaker Diversity: The use of few speakers limits the generalizability of models trained on these datasets.","keywords":[],"articleBody":"Training multimodal models often requires large, high-quality conversational audio datasets, which are currently scarce. This document details the creation of Daily Yap, a dataset developed to address this gap.\nExisting conversational audio datasets present several limitations:\nContent Scope: Many datasets focus on assistant-user interactions, lacking the breadth of topics found in general human dialogues. Audio-Text Alignment: Datasets with precise alignment between high-quality audio and accurate transcriptions are uncommon. Speaker Diversity: The use of few speakers limits the generalizability of models trained on these datasets. Scalability: Human recording is resource-intensive, hindering the creation of large-scale datasets. Daily Yap was created to mitigate these challenges by providing a synthetically generated conversational audio resource suitable for training real-time conversational audio models.\nThe following sections describe the methodology used to develop Daily Yap, resulting in a dataset comprising 9,758 audio samples with corresponding transcripts.\nFoundational Dataset Selection The Daily Dialog dataset was selected as the textual foundation due to its range of conversational topics. However, modifications were required to enhance its suitability for audio synthesis.\nText Preprocessing and Enhancement The Daily Dialog transcripts were initially filtered to remove conversations where any utterance was shorter than 10 characters. The remaining transcripts were then processed using GPT-4o via the OpenAI API with three main objectives:\nCorrecting grammatical and spelling errors. Reformatting text for improved compatibility with text-to-speech (TTS) engines. This included expanding abbreviations (e.g., “Mr.” to “Mister”, “.com” to “dot com”, “@” to “at”) so the text represented spoken language more accurately. Extending shorter conversations. GPT-4o was prompted to plausibly continue the dialogue until it reached a total of 10 utterances, increasing the average length and complexity. The prompt instructed GPT-4o to return the processed dialogue as a JSON object containing a list of strings, where each string represented one turn in the conversation (e.g., {\"segments\": [\"segment one\", \"segment two\", ...]}). This structured output facilitated programmatic handling. The processed transcripts were saved incrementally during the generation process.\ncompletion = client.chat.completions.create( model='gpt-4o', response_format={ 'type': 'json_object' }, messages=[ {'role': 'system', 'content': 'You are a helpful assistant who responds only with JSON.'}, {'role': 'user', 'content': 'Given the following dialog transcription, fix any formatting issues, grammar mistakes, or spelling mistakes if they exist. Replace any abbreviations like the \".? in \".com\" or the \"@\" in an email with their corresponding text, such as \"dot\" or \"at\" so that the text reads the way it would be spoken. Also replace other common abbreviations like \"Ms\" and \"Mr\". Additionally, if you think it is plausible that the conversation could continue, generate some additional lines of dialog until there are 10 total. Reply with a JSON object in the following format: {\"segments\": [\"segment one\", \"segment two\"]}\\n: ' + json.dumps(sample['dialog'])} ] ) completion_data = json.loads(completion.choices[0].message.content)['segments'] Audio Generation using TTS After evaluating several TTS engines, including ChatTTS, the Coqui TTS library’s xtts_v2 model (tts_models/multilingual/multi-dataset/xtts_v2) was selected using TTS.api, chosen for the perceived naturalness of its output compared to other open-source alternatives available at the time. Audio generation was performed using GPU acceleration.\nfrom TTS.api import TTS tts = TTS('tts_models/multilingual/multi-dataset/xtts_v2', gpu=True) To introduce speaker variability, eight distinct voices (four male, four female) were synthesized using recorded samples as inputs to generate latent representations. For each conversation transcript retrieved from the processed dataset, two distinct speaker voices were randomly selected from the available voice pool (voices directory).\nThe generation process iterated through each utterance (line) in the conversation’s segments. Each turn was assigned to one of the two selected voices in an alternating fashion (j % 2). The tts.tts_to_file function generated a temporary WAV file (current.wav) for the utterance using the appropriate speaker voice (speaker_wav=v1 or speaker_wav=v2) and language set to English (language='en').\nTo create the dual-channel output, the pydub library was employed. Two AudioSegment objects (track_one, track_two) were initialized. As each utterance’s audio was generated, it was appended to the corresponding speaker’s track. Simultaneously, silence of equivalent duration was appended to the other speaker’s track using AudioSegment.silent(). This ensured both tracks remained synchronized, representing the back-and-forth nature of the conversation with silence during the other speaker’s turn.\nfrom os import listdir from random import choice from pydub import AudioSegment voices = listdir('voices') # ... (dataset loading) for (i, sample) in enumerate(dataset): # Select two distinct random voices v1 = 'voices/' + choice(voices) v2 = 'voices/' + choice(voices) while v1 == v2: v2 = 'voices/' + choice(voices) track_one = AudioSegment.empty() track_two = AudioSegment.empty() for (j, line) in enumerate(sample['conversation']): if j % 2 == 1: # Speaker 1 tts.tts_to_file(text=line, file_path='current.wav', speaker_wav=v1, language='en') segment = AudioSegment.from_wav('current.wav') track_one += segment track_two += AudioSegment.silent(duration=len(segment)) else: # Speaker 0 tts.tts_to_file(text=line, file_path='current.wav', speaker_wav=v2, language='en') segment = AudioSegment.from_wav('current.wav') track_two += segment track_one += AudioSegment.silent(duration=len(segment)) # Combine tracks and export result = AudioSegment.from_mono_audiosegments( track_one[:min(len(track_one), len(track_two))], track_two[:min(len(track_one), len(track_two))] ) result.export('audio/' + str(i) + '.mp3', format='mp3') Finally, the two mono audio segments were combined into a single stereo audio file using AudioSegment.from_mono_audiosegments. The resulting segment was truncated to the length of the shorter track to handle any minor duration discrepancies and then exported as an MP3 file (e.g., audio/0.mp3, audio/1.mp3, …).\nDataset Characteristics The resulting Daily Yap dataset contains 9,758 samples, totaling approximately 90 hours of audio. The average sample duration is 33 seconds. This resource is intended for use in conversational AI, speech recognition, and related natural language processing tasks.\nEach sample consists of a JSON-formatted transcript and a corresponding dual-channel WAV audio file, facilitating speaker diarization and multimodal alignment.\nFuture Directions Future work may involve generating entirely synthetic dialogues, independent of the Daily Dialog source material, potentially increasing scalability and topical diversity.\nFurthermore, updates to the audio generation process will be considered as TTS technology progresses, incorporating newer methods for synthetic speech generation.\nConclusion The development of Daily Yap was undertaken to address the need for suitable conversational audio datasets and to investigate multimodal model requirements. This project involved exploring audio model architectures and current research in the field.\nDaily Yap provides a resource for developing and evaluating conversational AI systems. The dataset is available on HuggingFace: https://huggingface.co/datasets/jakeBoggs/DailyYap\nIf any academics want to cite this in a paper, I would be honored and extremely amused. Seeing “Daily Yap” in a works cited section would give me a good laugh and make all of this worth it.\n","wordCount":"1031","inLanguage":"en","datePublished":"2024-06-23T00:00:00Z","dateModified":"2024-06-23T00:00:00Z","author":{"@type":"Person","name":"Jake Boggs"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://boggs.tech/posts/synthetic-conversational-audio-generation-daily-yap/"},"publisher":{"@type":"Organization","name":"Jake Boggs","logo":{"@type":"ImageObject","url":"https://boggs.tech/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://boggs.tech/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://boggs.tech/startups title=Startups><span>Startups</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Daily Yap: A Synthetically Generated Conversational Audio Dataset</h1><div class=post-meta><span title='2024-06-23 00:00:00 +0000 UTC'>June 23, 2024</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;1031 words&nbsp;·&nbsp;Jake Boggs</div></header><div class=post-content><p>Training multimodal models often requires large, high-quality conversational audio datasets, which are currently scarce. This document details the creation of Daily Yap, a dataset developed to address this gap.</p><p>Existing conversational audio datasets present several limitations:</p><ol><li><strong>Content Scope:</strong> Many datasets focus on assistant-user interactions, lacking the breadth of topics found in general human dialogues.</li><li><strong>Audio-Text Alignment:</strong> Datasets with precise alignment between high-quality audio and accurate transcriptions are uncommon.</li><li><strong>Speaker Diversity:</strong> The use of few speakers limits the generalizability of models trained on these datasets.</li><li><strong>Scalability:</strong> Human recording is resource-intensive, hindering the creation of large-scale datasets.</li></ol><p>Daily Yap was created to mitigate these challenges by providing a synthetically generated conversational audio resource suitable for training real-time conversational audio models.</p><p>The following sections describe the methodology used to develop Daily Yap, resulting in a dataset comprising 9,758 audio samples with corresponding transcripts.</p><h2 id=foundational-dataset-selection>Foundational Dataset Selection<a hidden class=anchor aria-hidden=true href=#foundational-dataset-selection>#</a></h2><p>The Daily Dialog dataset was selected as the textual foundation due to its range of conversational topics. However, modifications were required to enhance its suitability for audio synthesis.</p><h2 id=text-preprocessing-and-enhancement>Text Preprocessing and Enhancement<a hidden class=anchor aria-hidden=true href=#text-preprocessing-and-enhancement>#</a></h2><p>The Daily Dialog transcripts were initially filtered to remove conversations where any utterance was shorter than 10 characters. The remaining transcripts were then processed using GPT-4o via the OpenAI API with three main objectives:</p><ol><li>Correcting grammatical and spelling errors.</li><li>Reformatting text for improved compatibility with text-to-speech (TTS) engines. This included expanding abbreviations (e.g., &ldquo;Mr.&rdquo; to &ldquo;Mister&rdquo;, &ldquo;.com&rdquo; to &ldquo;dot com&rdquo;, &ldquo;@&rdquo; to &ldquo;at&rdquo;) so the text represented spoken language more accurately.</li><li>Extending shorter conversations. GPT-4o was prompted to plausibly continue the dialogue until it reached a total of 10 utterances, increasing the average length and complexity.</li></ol><p>The prompt instructed GPT-4o to return the processed dialogue as a JSON object containing a list of strings, where each string represented one turn in the conversation (e.g., <code>{"segments": ["segment one", "segment two", ...]}</code>). This structured output facilitated programmatic handling. The processed transcripts were saved incrementally during the generation process.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>completion <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>chat<span style=color:#f92672>.</span>completions<span style=color:#f92672>.</span>create(
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;gpt-4o&#39;</span>,
</span></span><span style=display:flex><span>    response_format<span style=color:#f92672>=</span>{ <span style=color:#e6db74>&#39;type&#39;</span>: <span style=color:#e6db74>&#39;json_object&#39;</span> },
</span></span><span style=display:flex><span>    messages<span style=color:#f92672>=</span>[
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#39;role&#39;</span>: <span style=color:#e6db74>&#39;system&#39;</span>, <span style=color:#e6db74>&#39;content&#39;</span>: <span style=color:#e6db74>&#39;You are a helpful assistant who responds only with JSON.&#39;</span>},
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#39;role&#39;</span>: <span style=color:#e6db74>&#39;user&#39;</span>, <span style=color:#e6db74>&#39;content&#39;</span>: <span style=color:#e6db74>&#39;Given the following dialog transcription, fix any formatting issues, grammar mistakes, or spelling mistakes if they exist. Replace any abbreviations like the &#34;.? in &#34;.com&#34; or the &#34;@&#34; in an email with their corresponding text, such as &#34;dot&#34; or &#34;at&#34; so that the text reads the way it would be spoken. Also replace other common abbreviations like &#34;Ms&#34; and &#34;Mr&#34;. Additionally, if you think it is plausible that the conversation could continue, generate some additional lines of dialog until there are 10 total. Reply with a JSON object in the following format: {&#34;segments&#34;: [&#34;segment one&#34;, &#34;segment two&#34;]}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>: &#39;</span> <span style=color:#f92672>+</span> json<span style=color:#f92672>.</span>dumps(sample[<span style=color:#e6db74>&#39;dialog&#39;</span>])}
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>completion_data <span style=color:#f92672>=</span> json<span style=color:#f92672>.</span>loads(completion<span style=color:#f92672>.</span>choices[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>message<span style=color:#f92672>.</span>content)[<span style=color:#e6db74>&#39;segments&#39;</span>]
</span></span></code></pre></div><h2 id=audio-generation-using-tts>Audio Generation using TTS<a hidden class=anchor aria-hidden=true href=#audio-generation-using-tts>#</a></h2><p>After evaluating several TTS engines, including ChatTTS, the Coqui TTS library&rsquo;s <code>xtts_v2</code> model (<code>tts_models/multilingual/multi-dataset/xtts_v2</code>) was selected using <code>TTS.api</code>, chosen for the perceived naturalness of its output compared to other open-source alternatives available at the time. Audio generation was performed using GPU acceleration.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> TTS.api <span style=color:#f92672>import</span> TTS
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tts <span style=color:#f92672>=</span> TTS(<span style=color:#e6db74>&#39;tts_models/multilingual/multi-dataset/xtts_v2&#39;</span>, gpu<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><p>To introduce speaker variability, eight distinct voices (four male, four female) were synthesized using recorded samples as inputs to generate latent representations. For each conversation transcript retrieved from the processed dataset, two distinct speaker voices were randomly selected from the available voice pool (<code>voices</code> directory).</p><p>The generation process iterated through each utterance (<code>line</code>) in the conversation&rsquo;s <code>segments</code>. Each turn was assigned to one of the two selected voices in an alternating fashion (<code>j % 2</code>). The <code>tts.tts_to_file</code> function generated a temporary WAV file (<code>current.wav</code>) for the utterance using the appropriate speaker voice (<code>speaker_wav=v1</code> or <code>speaker_wav=v2</code>) and language set to English (<code>language='en'</code>).</p><p>To create the dual-channel output, the <code>pydub</code> library was employed. Two <code>AudioSegment</code> objects (<code>track_one</code>, <code>track_two</code>) were initialized. As each utterance&rsquo;s audio was generated, it was appended to the corresponding speaker&rsquo;s track. Simultaneously, silence of equivalent duration was appended to the <em>other</em> speaker&rsquo;s track using <code>AudioSegment.silent()</code>. This ensured both tracks remained synchronized, representing the back-and-forth nature of the conversation with silence during the other speaker&rsquo;s turn.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> os <span style=color:#f92672>import</span> listdir
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> random <span style=color:#f92672>import</span> choice
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pydub <span style=color:#f92672>import</span> AudioSegment
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>voices <span style=color:#f92672>=</span> listdir(<span style=color:#e6db74>&#39;voices&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># ... (dataset loading)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> (i, sample) <span style=color:#f92672>in</span> enumerate(dataset):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Select two distinct random voices</span>
</span></span><span style=display:flex><span>    v1 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;voices/&#39;</span> <span style=color:#f92672>+</span> choice(voices)
</span></span><span style=display:flex><span>    v2 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;voices/&#39;</span> <span style=color:#f92672>+</span> choice(voices)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> v1 <span style=color:#f92672>==</span> v2:
</span></span><span style=display:flex><span>        v2 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;voices/&#39;</span> <span style=color:#f92672>+</span> choice(voices)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    track_one <span style=color:#f92672>=</span> AudioSegment<span style=color:#f92672>.</span>empty()
</span></span><span style=display:flex><span>    track_two <span style=color:#f92672>=</span> AudioSegment<span style=color:#f92672>.</span>empty()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> (j, line) <span style=color:#f92672>in</span> enumerate(sample[<span style=color:#e6db74>&#39;conversation&#39;</span>]):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> j <span style=color:#f92672>%</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>: <span style=color:#75715e># Speaker 1</span>
</span></span><span style=display:flex><span>            tts<span style=color:#f92672>.</span>tts_to_file(text<span style=color:#f92672>=</span>line, file_path<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;current.wav&#39;</span>, speaker_wav<span style=color:#f92672>=</span>v1, language<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;en&#39;</span>)
</span></span><span style=display:flex><span>            segment <span style=color:#f92672>=</span> AudioSegment<span style=color:#f92672>.</span>from_wav(<span style=color:#e6db74>&#39;current.wav&#39;</span>)
</span></span><span style=display:flex><span>            track_one <span style=color:#f92672>+=</span> segment
</span></span><span style=display:flex><span>            track_two <span style=color:#f92672>+=</span> AudioSegment<span style=color:#f92672>.</span>silent(duration<span style=color:#f92672>=</span>len(segment))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>: <span style=color:#75715e># Speaker 0</span>
</span></span><span style=display:flex><span>            tts<span style=color:#f92672>.</span>tts_to_file(text<span style=color:#f92672>=</span>line, file_path<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;current.wav&#39;</span>, speaker_wav<span style=color:#f92672>=</span>v2, language<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;en&#39;</span>)
</span></span><span style=display:flex><span>            segment <span style=color:#f92672>=</span> AudioSegment<span style=color:#f92672>.</span>from_wav(<span style=color:#e6db74>&#39;current.wav&#39;</span>)
</span></span><span style=display:flex><span>            track_two <span style=color:#f92672>+=</span> segment
</span></span><span style=display:flex><span>            track_one <span style=color:#f92672>+=</span> AudioSegment<span style=color:#f92672>.</span>silent(duration<span style=color:#f92672>=</span>len(segment))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Combine tracks and export</span>
</span></span><span style=display:flex><span>    result <span style=color:#f92672>=</span> AudioSegment<span style=color:#f92672>.</span>from_mono_audiosegments(
</span></span><span style=display:flex><span>        track_one[:min(len(track_one), len(track_two))],
</span></span><span style=display:flex><span>        track_two[:min(len(track_one), len(track_two))]
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    result<span style=color:#f92672>.</span>export(<span style=color:#e6db74>&#39;audio/&#39;</span> <span style=color:#f92672>+</span> str(i) <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;.mp3&#39;</span>, format<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;mp3&#39;</span>)
</span></span></code></pre></div><p>Finally, the two mono audio segments were combined into a single stereo audio file using <code>AudioSegment.from_mono_audiosegments</code>. The resulting segment was truncated to the length of the shorter track to handle any minor duration discrepancies and then exported as an MP3 file (e.g., <code>audio/0.mp3</code>, <code>audio/1.mp3</code>, &mldr;).</p><h2 id=dataset-characteristics>Dataset Characteristics<a hidden class=anchor aria-hidden=true href=#dataset-characteristics>#</a></h2><p>The resulting Daily Yap dataset contains 9,758 samples, totaling approximately 90 hours of audio. The average sample duration is 33 seconds. This resource is intended for use in conversational AI, speech recognition, and related natural language processing tasks.</p><p>Each sample consists of a JSON-formatted transcript and a corresponding dual-channel WAV audio file, facilitating speaker diarization and multimodal alignment.</p><h2 id=future-directions>Future Directions<a hidden class=anchor aria-hidden=true href=#future-directions>#</a></h2><p>Future work may involve generating entirely synthetic dialogues, independent of the Daily Dialog source material, potentially increasing scalability and topical diversity.</p><p>Furthermore, updates to the audio generation process will be considered as TTS technology progresses, incorporating newer methods for synthetic speech generation.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>The development of Daily Yap was undertaken to address the need for suitable conversational audio datasets and to investigate multimodal model requirements. This project involved exploring audio model architectures and current research in the field.</p><p>Daily Yap provides a resource for developing and evaluating conversational AI systems. The dataset is available on HuggingFace: <a href=https://huggingface.co/datasets/jakeBoggs/DailyYap>https://huggingface.co/datasets/jakeBoggs/DailyYap</a></p><p>If any academics want to cite this in a paper, I would be honored and extremely amused. Seeing &ldquo;Daily Yap&rdquo; in a works
cited section would give me a good laugh and make all of this worth it.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://boggs.tech/>Jake Boggs</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>