<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=author content="Jake Boggs"><title>Daily Yap: A Synthetically Generated Conversational Audio Dataset | Jake Boggs</title><link rel=stylesheet href=/css/main.min.css><link rel=icon href=/favicon.png><link rel=canonical href=https://boggs.tech/posts/synthetic-conversational-audio-generation-daily-yap/></head><body><header><div class=container><a href=https://boggs.tech/ class=site-title>Jake Boggs</a><nav><a href=/about>About me</a><a href=/startups>Startups</a></nav></div></header><main><div class=container><article><h1>Daily Yap: A Synthetically Generated Conversational Audio Dataset</h1><div class=article-meta><span>June 23, 2024</span>
<span>5 min read</span></div><div class=article-content><p>Training multimodal models often requires large, high-quality conversational audio datasets, which are scarce at the time of writing.</p><p>Existing conversational audio datasets present several limitations:</p><ol><li><strong>Content Scope:</strong> Many datasets focus on assistant-user interactions, lacking the breadth of topics found in general human dialogues.</li><li><strong>Audio-Text Alignment:</strong> Datasets with precise alignment between high-quality audio and accurate transcriptions are uncommon.</li><li><strong>Speaker Diversity:</strong> The use of few speakers limits the generalizability of models trained on these datasets.</li><li><strong>Scalability:</strong> Human recording is resource-intensive, hindering the creation of large-scale datasets.</li></ol><p>Daily Yap was created to overcome these challenges by providing a synthetically generated conversational audio resource suitable for training real-time conversational audio models.</p><h2 id=foundational-conversation-dataset>Foundational Conversation Dataset</h2><p>The Daily Dialog dataset was selected as the textual foundation due to its range of conversational topics. However, modifications were required to enhance its suitability for audio synthesis.</p><h3 id=text-preprocessing-and-enhancement>Text Preprocessing and Enhancement</h3><p>The Daily Dialog transcripts were initially filtered to remove conversations where any utterance was shorter than 10 characters. The remaining transcripts were then processed using GPT-4o via the OpenAI API with three main objectives:</p><ol><li>Correcting grammatical and spelling errors.</li><li>Reformatting text for improved compatibility with text-to-speech (TTS) engines. This included expanding abbreviations (e.g., &ldquo;Mr.&rdquo; to &ldquo;Mister&rdquo;, &ldquo;.com&rdquo; to &ldquo;dot com&rdquo;, &ldquo;@&rdquo; to &ldquo;at&rdquo;) so the text represented spoken language more accurately.</li><li>Extending shorter conversations. GPT-4o was prompted to plausibly continue the dialogue until it reached a total of 10 utterances, increasing the average length and complexity.</li></ol><p>The prompt instructed GPT-4o to return the processed dialogue as a JSON object containing a list of strings, where each string represented one turn in the conversation (e.g., <code>{"segments": ["segment one", "segment two", ...]}</code>). This structured output facilitated programmatic handling. The processed transcripts were saved incrementally during the generation process.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>completion <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>chat<span style=color:#f92672>.</span>completions<span style=color:#f92672>.</span>create(
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;gpt-4o&#39;</span>,
</span></span><span style=display:flex><span>    response_format<span style=color:#f92672>=</span>{ <span style=color:#e6db74>&#39;type&#39;</span>: <span style=color:#e6db74>&#39;json_object&#39;</span> },
</span></span><span style=display:flex><span>    messages<span style=color:#f92672>=</span>[
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#39;role&#39;</span>: <span style=color:#e6db74>&#39;system&#39;</span>, <span style=color:#e6db74>&#39;content&#39;</span>: <span style=color:#e6db74>&#39;You are a helpful assistant who responds only with JSON.&#39;</span>},
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#39;role&#39;</span>: <span style=color:#e6db74>&#39;user&#39;</span>, <span style=color:#e6db74>&#39;content&#39;</span>: <span style=color:#e6db74>&#39;Given the following dialog transcription, fix any formatting issues, grammar mistakes, or spelling mistakes if they exist. Replace any abbreviations like the &#34;.? in &#34;.com&#34; or the &#34;@&#34; in an email with their corresponding text, such as &#34;dot&#34; or &#34;at&#34; so that the text reads the way it would be spoken. Also replace other common abbreviations like &#34;Ms&#34; and &#34;Mr&#34;. Additionally, if you think it is plausible that the conversation could continue, generate some additional lines of dialog until there are 10 total. Reply with a JSON object in the following format: {&#34;segments&#34;: [&#34;segment one&#34;, &#34;segment two&#34;]}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>: &#39;</span> <span style=color:#f92672>+</span> json<span style=color:#f92672>.</span>dumps(sample[<span style=color:#e6db74>&#39;dialog&#39;</span>])}
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>completion_data <span style=color:#f92672>=</span> json<span style=color:#f92672>.</span>loads(completion<span style=color:#f92672>.</span>choices[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>message<span style=color:#f92672>.</span>content)[<span style=color:#e6db74>&#39;segments&#39;</span>]
</span></span></code></pre></div><h2 id=audio-generation-using-tts>Audio Generation using TTS</h2><p>After evaluating several TTS engines, including ChatTTS, the Coqui TTS library&rsquo;s <code>xtts_v2</code> model (<code>tts_models/multilingual/multi-dataset/xtts_v2</code>) was selected using <code>TTS.api</code>, chosen for the perceived naturalness of its output compared to other open-source alternatives available at the time. Audio generation was performed using GPU acceleration.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> TTS.api <span style=color:#f92672>import</span> TTS
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tts <span style=color:#f92672>=</span> TTS(<span style=color:#e6db74>&#39;tts_models/multilingual/multi-dataset/xtts_v2&#39;</span>, gpu<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><p>To introduce speaker variability, eight distinct voices (four male, four female) were synthesized using recorded samples as inputs to generate latent representations. For each conversation transcript retrieved from the processed dataset, two distinct speaker voices were randomly selected from the available voice pool (<code>voices</code> directory).</p><p>The generation process iterated through each utterance (<code>line</code>) in the conversation&rsquo;s <code>segments</code>. Each turn was assigned to one of the two selected voices in an alternating fashion (<code>j % 2</code>). The <code>tts.tts_to_file</code> function generated a temporary WAV file (<code>current.wav</code>) for the utterance using the appropriate speaker voice (<code>speaker_wav=v1</code> or <code>speaker_wav=v2</code>) and language set to English (<code>language='en'</code>).</p><p>To create the dual-channel output, the <code>pydub</code> library was employed. Two <code>AudioSegment</code> objects (<code>track_one</code>, <code>track_two</code>) were initialized. As each utterance&rsquo;s audio was generated, it was appended to the corresponding speaker&rsquo;s track. Simultaneously, silence of equivalent duration was appended to the <em>other</em> speaker&rsquo;s track using <code>AudioSegment.silent()</code>. This ensured both tracks remained synchronized, representing the back-and-forth nature of the conversation with silence during the other speaker&rsquo;s turn.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> os <span style=color:#f92672>import</span> listdir
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> random <span style=color:#f92672>import</span> choice
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pydub <span style=color:#f92672>import</span> AudioSegment
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>voices <span style=color:#f92672>=</span> listdir(<span style=color:#e6db74>&#39;voices&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># ... (dataset loading)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> (i, sample) <span style=color:#f92672>in</span> enumerate(dataset):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Select two distinct random voices</span>
</span></span><span style=display:flex><span>    v1 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;voices/&#39;</span> <span style=color:#f92672>+</span> choice(voices)
</span></span><span style=display:flex><span>    v2 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;voices/&#39;</span> <span style=color:#f92672>+</span> choice(voices)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> v1 <span style=color:#f92672>==</span> v2:
</span></span><span style=display:flex><span>        v2 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;voices/&#39;</span> <span style=color:#f92672>+</span> choice(voices)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    track_one <span style=color:#f92672>=</span> AudioSegment<span style=color:#f92672>.</span>empty()
</span></span><span style=display:flex><span>    track_two <span style=color:#f92672>=</span> AudioSegment<span style=color:#f92672>.</span>empty()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> (j, line) <span style=color:#f92672>in</span> enumerate(sample[<span style=color:#e6db74>&#39;conversation&#39;</span>]):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> j <span style=color:#f92672>%</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>: <span style=color:#75715e># Speaker 1</span>
</span></span><span style=display:flex><span>            tts<span style=color:#f92672>.</span>tts_to_file(text<span style=color:#f92672>=</span>line, file_path<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;current.wav&#39;</span>, speaker_wav<span style=color:#f92672>=</span>v1, language<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;en&#39;</span>)
</span></span><span style=display:flex><span>            segment <span style=color:#f92672>=</span> AudioSegment<span style=color:#f92672>.</span>from_wav(<span style=color:#e6db74>&#39;current.wav&#39;</span>)
</span></span><span style=display:flex><span>            track_one <span style=color:#f92672>+=</span> segment
</span></span><span style=display:flex><span>            track_two <span style=color:#f92672>+=</span> AudioSegment<span style=color:#f92672>.</span>silent(duration<span style=color:#f92672>=</span>len(segment))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>: <span style=color:#75715e># Speaker 0</span>
</span></span><span style=display:flex><span>            tts<span style=color:#f92672>.</span>tts_to_file(text<span style=color:#f92672>=</span>line, file_path<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;current.wav&#39;</span>, speaker_wav<span style=color:#f92672>=</span>v2, language<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;en&#39;</span>)
</span></span><span style=display:flex><span>            segment <span style=color:#f92672>=</span> AudioSegment<span style=color:#f92672>.</span>from_wav(<span style=color:#e6db74>&#39;current.wav&#39;</span>)
</span></span><span style=display:flex><span>            track_two <span style=color:#f92672>+=</span> segment
</span></span><span style=display:flex><span>            track_one <span style=color:#f92672>+=</span> AudioSegment<span style=color:#f92672>.</span>silent(duration<span style=color:#f92672>=</span>len(segment))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Combine tracks and export</span>
</span></span><span style=display:flex><span>    result <span style=color:#f92672>=</span> AudioSegment<span style=color:#f92672>.</span>from_mono_audiosegments(
</span></span><span style=display:flex><span>        track_one[:min(len(track_one), len(track_two))],
</span></span><span style=display:flex><span>        track_two[:min(len(track_one), len(track_two))]
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    result<span style=color:#f92672>.</span>export(<span style=color:#e6db74>&#39;audio/&#39;</span> <span style=color:#f92672>+</span> str(i) <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;.mp3&#39;</span>, format<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;mp3&#39;</span>)
</span></span></code></pre></div><p>Finally, the two mono audio segments were combined into a single stereo audio file using <code>AudioSegment.from_mono_audiosegments</code>. The resulting segment was truncated to the length of the shorter track to handle any minor duration discrepancies and then exported as an MP3 file (e.g., <code>audio/0.mp3</code>, <code>audio/1.mp3</code>, &mldr;).</p><h2 id=dataset-characteristics>Dataset Characteristics</h2><p>The resulting Daily Yap dataset contains 9,758 samples, totaling approximately 90 hours of audio. Each sample consists of a JSON-formatted transcript and a corresponding dual-channel WAV audio file. The average sample duration is 33 seconds.</p><p>The dataset is available on HuggingFace: <a href=https://huggingface.co/datasets/jakeBoggs/DailyYap>https://huggingface.co/datasets/jakeBoggs/DailyYap</a></p><p>If any researchers want to cite this in a paper, I would be both honored and amused. Seeing &ldquo;Daily Yap&rdquo; in a works cited section would give me a good laugh.</p></div></article></div></main><footer><div class=container><p>&copy; 2026 Jake Boggs</p></div></footer></body></html>