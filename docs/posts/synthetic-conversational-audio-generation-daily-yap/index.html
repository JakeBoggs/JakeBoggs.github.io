<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Open-sourcing 100 Hours of Conversational Audio (Daily Yap) | Jake Boggs</title>
<meta name=keywords content><meta name=description content="In the rapidly evolving field of artificial intelligence, one critical gap became apparent to me as I was helping my friend&rsquo;s startup locate data for training their multimodal model: the scarcity of high-quality conversational audio datasets.
Existing datasets often fall short in several key areas:
Content of conversations: Most available datasets consist assistant-user exchanges that don&rsquo;t capture the complexity and subject matter of real-world dialogues. Audio-text alignment: There&rsquo;s a lack of datasets that provide both high-quality audio and accurately transcribed text, perfectly aligned."><meta name=author content="Jake Boggs"><link rel=canonical href=https://boggs.tech/posts/synthetic-conversational-audio-generation-daily-yap/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://boggs.tech/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://boggs.tech/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://boggs.tech/favicon-32x32.png><link rel=apple-touch-icon href=https://boggs.tech/apple-touch-icon.png><link rel=mask-icon href=https://boggs.tech/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://boggs.tech/posts/synthetic-conversational-audio-generation-daily-yap/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Open-sourcing 100 Hours of Conversational Audio (Daily Yap)"><meta property="og:description" content="In the rapidly evolving field of artificial intelligence, one critical gap became apparent to me as I was helping my friend&rsquo;s startup locate data for training their multimodal model: the scarcity of high-quality conversational audio datasets.
Existing datasets often fall short in several key areas:
Content of conversations: Most available datasets consist assistant-user exchanges that don&rsquo;t capture the complexity and subject matter of real-world dialogues. Audio-text alignment: There&rsquo;s a lack of datasets that provide both high-quality audio and accurately transcribed text, perfectly aligned."><meta property="og:type" content="article"><meta property="og:url" content="https://boggs.tech/posts/synthetic-conversational-audio-generation-daily-yap/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-23T00:00:00+00:00"><meta property="article:modified_time" content="2024-06-23T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Open-sourcing 100 Hours of Conversational Audio (Daily Yap)"><meta name=twitter:description content="In the rapidly evolving field of artificial intelligence, one critical gap became apparent to me as I was helping my friend&rsquo;s startup locate data for training their multimodal model: the scarcity of high-quality conversational audio datasets.
Existing datasets often fall short in several key areas:
Content of conversations: Most available datasets consist assistant-user exchanges that don&rsquo;t capture the complexity and subject matter of real-world dialogues. Audio-text alignment: There&rsquo;s a lack of datasets that provide both high-quality audio and accurately transcribed text, perfectly aligned."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://boggs.tech/posts/"},{"@type":"ListItem","position":2,"name":"Open-sourcing 100 Hours of Conversational Audio (Daily Yap)","item":"https://boggs.tech/posts/synthetic-conversational-audio-generation-daily-yap/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Open-sourcing 100 Hours of Conversational Audio (Daily Yap)","name":"Open-sourcing 100 Hours of Conversational Audio (Daily Yap)","description":"In the rapidly evolving field of artificial intelligence, one critical gap became apparent to me as I was helping my friend\u0026rsquo;s startup locate data for training their multimodal model: the scarcity of high-quality conversational audio datasets.\nExisting datasets often fall short in several key areas:\nContent of conversations: Most available datasets consist assistant-user exchanges that don\u0026rsquo;t capture the complexity and subject matter of real-world dialogues. Audio-text alignment: There\u0026rsquo;s a lack of datasets that provide both high-quality audio and accurately transcribed text, perfectly aligned.","keywords":[],"articleBody":"In the rapidly evolving field of artificial intelligence, one critical gap became apparent to me as I was helping my friend’s startup locate data for training their multimodal model: the scarcity of high-quality conversational audio datasets.\nExisting datasets often fall short in several key areas:\nContent of conversations: Most available datasets consist assistant-user exchanges that don’t capture the complexity and subject matter of real-world dialogues. Audio-text alignment: There’s a lack of datasets that provide both high-quality audio and accurately transcribed text, perfectly aligned. Diversity of speakers: Many datasets use a limited number of voices, which can lead to AI models that don’t generalize well across different speakers. Scalability: The labor-intensive nature of creating human-recorded conversational audio data limits the size and scope of many existing datasets. It was in response to these specific challenges that I embarked on the journey to create Daily Yap. This dataset aims to provide a starting point to address the shortage of comprehensive conversational audio data, offering researchers and developers a resource for training real-time conversational audio models.\nIn the following sections, I’ll detail the process of developing Daily Yap, from its conceptual roots to its final form as a dataset of nearly 10,000 audio samples with matching transcripts.\nChoosing a Foundation After extensive research, I selected the Daily Dialog dataset as the foundation for this project. Daily Dialog offered a solid base of conversational topics, providing a springboard from which to build. However, I quickly realized that to create a truly valuable resource, significant enhancements would be necessary.\nRefining the Raw Material The first step in transforming Daily Dialog into Daily Yap involved leveraging the power of GPT-4. I tasked this advanced language model with three primary objectives:\nCorrecting grammatical and spelling errors in the original transcripts. Reformatting the text to be more compatible with text-to-speech (TTS) engines. This included expanding abbreviations (e.g., changing “Mr.” to “Mister”) to ensure clearer audio output. Extending conversations that were deemed too brief, adding depth and complexity to the dialogues. This process was crucial in preparing the data for the next stage: audio generation.\nBringing Conversations to Life Selecting the right text-to-speech engine was a critical decision. After experimenting with several options, including ChatTTS, I ultimately chose XTTSv2. This engine stood out for its superior quality, producing more natural-sounding speech than other open-source options.\nTo enhance the dataset’s versatility, I decided to use a total of eight distinct voices - four male and four female. These were generated by mixing latent representations of my own voice and those of some friends. This diversity was intentional, aimed at creating a balanced dataset that would allow AI models to generalize better across different speaker characteristics.\nThe Final Product After weeks of development, refinement, and quality assurance, Daily Yap emerged as a robust dataset consisting of 9,758 samples. With approximately 90 hours of audio and an average sample length of 33 seconds, it offers a rich resource for researchers and developers working on conversational AI, speech recognition, and natural language processing tasks.\nEach sample in the dataset includes a JSON-formatted transcript paired with a dual-channel WAV file, allowing for easy speaker separation and multimodal alignment.\nLooking to the Future While Daily Yap represents a significant step forward, I see it as just the beginning. Future iterations could potentially include fully synthetic dialogues, moving beyond the constraints of the original Daily Dialog dataset. This approach could allow for even greater scalability and diversity in the conversations.\nAdditionally, as text-to-speech technology continues to advance, I plan to explore upgrading the audio generation process to incorporate the latest breakthroughs in synthetic speech.\nConclusion The creation of Daily Yap was driven by a desire to address the lack of high-quality conversational audio datasets and gain a better understanding of multimodal models. This research has taught me a lot about the architecture of audio models and helped familiarize me with the latest research in the field.\nDaily Yap represents a significant step forward in providing researchers and developers with the tools they need to create more natural, more engaging, and more capable conversational AI systems. I’m excited to see how the community will leverage this resource to push the boundaries of what’s possible in multimodal AI and speech recognition.\nFor those interested in exploring or using the Daily Yap dataset, you can find it on HuggingFace: https://huggingface.co/datasets/jakeBoggs/DailyYap\nIf any academics want to cite this in a paper, I would be honored and extremely amused. Seeing “Daily Yap” in a works cited section would give me a good laugh and make all of this worth it.\n","wordCount":"759","inLanguage":"en","datePublished":"2024-06-23T00:00:00Z","dateModified":"2024-06-23T00:00:00Z","author":{"@type":"Person","name":"Jake Boggs"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://boggs.tech/posts/synthetic-conversational-audio-generation-daily-yap/"},"publisher":{"@type":"Organization","name":"Jake Boggs","logo":{"@type":"ImageObject","url":"https://boggs.tech/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://boggs.tech/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://boggs.tech/startups title=Startups><span>Startups</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Open-sourcing 100 Hours of Conversational Audio (Daily Yap)</h1><div class=post-meta><span title='2024-06-23 00:00:00 +0000 UTC'>June 23, 2024</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;759 words&nbsp;·&nbsp;Jake Boggs</div></header><div class=post-content><p>In the rapidly evolving field of artificial intelligence, one critical gap became apparent to me as I was helping my friend&rsquo;s startup locate data for training their multimodal model: the scarcity of high-quality conversational audio datasets.</p><p>Existing datasets often fall short in several key areas:</p><ol><li>Content of conversations: Most available datasets consist assistant-user exchanges that don&rsquo;t capture the complexity and subject matter of real-world dialogues.</li><li>Audio-text alignment: There&rsquo;s a lack of datasets that provide both high-quality audio and accurately transcribed text, perfectly aligned.</li><li>Diversity of speakers: Many datasets use a limited number of voices, which can lead to AI models that don&rsquo;t generalize well across different speakers.</li><li>Scalability: The labor-intensive nature of creating human-recorded conversational audio data limits the size and scope of many existing datasets.</li></ol><p>It was in response to these specific challenges that I embarked on the journey to create Daily Yap. This dataset aims to provide a starting point to address the shortage of comprehensive conversational audio data, offering researchers and developers a resource for training real-time conversational audio models.</p><p>In the following sections, I&rsquo;ll detail the process of developing Daily Yap, from its conceptual roots to its final form as a dataset of nearly 10,000 audio samples with matching transcripts.</p><h2 id=choosing-a-foundation>Choosing a Foundation<a hidden class=anchor aria-hidden=true href=#choosing-a-foundation>#</a></h2><p>After extensive research, I selected the Daily Dialog dataset as the foundation for this project. Daily Dialog offered a solid base of conversational topics, providing a springboard from which to build. However, I quickly realized that to create a truly valuable resource, significant enhancements would be necessary.</p><h2 id=refining-the-raw-material>Refining the Raw Material<a hidden class=anchor aria-hidden=true href=#refining-the-raw-material>#</a></h2><p>The first step in transforming Daily Dialog into Daily Yap involved leveraging the power of GPT-4. I tasked this advanced language model with three primary objectives:</p><ol><li>Correcting grammatical and spelling errors in the original transcripts.</li><li>Reformatting the text to be more compatible with text-to-speech (TTS) engines. This included expanding abbreviations (e.g., changing &ldquo;Mr.&rdquo; to &ldquo;Mister&rdquo;) to ensure clearer audio output.</li><li>Extending conversations that were deemed too brief, adding depth and complexity to the dialogues.</li></ol><p>This process was crucial in preparing the data for the next stage: audio generation.</p><h2 id=bringing-conversations-to-life>Bringing Conversations to Life<a hidden class=anchor aria-hidden=true href=#bringing-conversations-to-life>#</a></h2><p>Selecting the right text-to-speech engine was a critical decision. After experimenting with several options, including ChatTTS, I ultimately chose XTTSv2. This engine stood out for its superior quality, producing more natural-sounding speech than other open-source options.</p><p>To enhance the dataset&rsquo;s versatility, I decided to use a total of eight distinct voices - four male and four female. These were generated by mixing latent representations of my own voice and those of some friends. This diversity was intentional, aimed at creating a balanced dataset that would allow AI models to generalize better across different speaker characteristics.</p><h2 id=the-final-product>The Final Product<a hidden class=anchor aria-hidden=true href=#the-final-product>#</a></h2><p>After weeks of development, refinement, and quality assurance, Daily Yap emerged as a robust dataset consisting of 9,758 samples. With approximately 90 hours of audio and an average sample length of 33 seconds, it offers a rich resource for researchers and developers working on conversational AI, speech recognition, and natural language processing tasks.</p><p>Each sample in the dataset includes a JSON-formatted transcript paired with a dual-channel WAV file, allowing for easy speaker separation and multimodal alignment.</p><h2 id=looking-to-the-future>Looking to the Future<a hidden class=anchor aria-hidden=true href=#looking-to-the-future>#</a></h2><p>While Daily Yap represents a significant step forward, I see it as just the beginning. Future iterations could potentially include fully synthetic dialogues, moving beyond the constraints of the original Daily Dialog dataset. This approach could allow for even greater scalability and diversity in the conversations.</p><p>Additionally, as text-to-speech technology continues to advance, I plan to explore upgrading the audio generation process to incorporate the latest breakthroughs in synthetic speech.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>The creation of Daily Yap was driven by a desire to address the lack of high-quality conversational audio datasets and gain a better understanding of multimodal models. This research has taught me a lot about the architecture of audio models and helped familiarize me with the latest research in the field.</p><p>Daily Yap represents a significant step forward in providing researchers and developers with the tools they need to create more natural, more engaging, and more capable conversational AI systems. I&rsquo;m excited to see how the community will leverage this resource to push the boundaries of what&rsquo;s possible in multimodal AI and speech recognition.</p><p>For those interested in exploring or using the Daily Yap dataset, you can find it on HuggingFace: <a href=https://huggingface.co/datasets/jakeBoggs/DailyYap>https://huggingface.co/datasets/jakeBoggs/DailyYap</a></p><p>If any academics want to cite this in a paper, I would be honored and extremely amused. Seeing &ldquo;Daily Yap&rdquo; in a works cited section would give me a good laugh and make all of this worth it.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://boggs.tech/>Jake Boggs</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>