<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Evaluating Reasoning in LLMs Through MTG Deck Building | Jake Boggs</title><meta name=keywords content><meta name=description content="
  Update (2026-01-14): Gemini 3 Pro, Gemini 3 Flash, GPT 5.2 (medium), Claude Opus 4.5, and Grok 4.1 Fast added.


  View Older Updates
  
    
    Update (2025-08-08): GPT-5, GPT-5 Mini, GPT-5 Nano, GPT-4 Turbo 11-06, and GPT-3.5 Turbo added.
    
    
      Update (2025-08-05): Kimi K2, GPT OSS 120B (low), and GPT OSS 120B (high) added.
    
    
      Update (2025-07-13): Grok 4, Grok 3, Gemini 2.5 Flash, Claude Sonnet 4 (thinking), and Command A added.
    
    
      Update (2025-06-11): o3 (high) added after API cost reduction.
    
    
      Update (2025-06-06): Deepseek R1 05-28 and Gemini 2.5 Pro 06-05 added.
    
    
      Update (2025-05-22): Claude Sonnet 4 and Opus 4 added.
    
    
      Update (2025-05-14): Human Baseline, Gemini 2.5 Pro 03-25, Gemini 1.5 Flash, Deepseek V3 03-24, Qwen3 30B 3A added.
    
  

Introduction
I have an obsession with applying AI models to my favorite card game, so I&rsquo;ve created ManaBench, a benchmark designed to probe an LLM&rsquo;s capacity for reasoning using the collectible card game Magic: The Gathering (MTG). With its intricate interactions and deep strategy, MTG serves as an ideal micro-world to test an LLM&rsquo;s ability to process contextual information, identify patterns, and make judgments that align with expert human choices. This post provides an overview of the benchmark&rsquo;s construction and the methodology used for evaluation."><meta name=author content="Jake Boggs"><link rel=canonical href=https://boggs.tech/posts/evaluating-llm-reasoning-with-mtg-deck-building/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://boggs.tech/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://boggs.tech/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://boggs.tech/favicon-32x32.png><link rel=apple-touch-icon href=https://boggs.tech/apple-touch-icon.png><link rel=mask-icon href=https://boggs.tech/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://boggs.tech/posts/evaluating-llm-reasoning-with-mtg-deck-building/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><script>!function(e){if(window.reb2b)return;window.reb2b={loaded:!0};var t=document.createElement("script");t.async=!0,t.src="https://ddwl4m2hdecbv.cloudfront.net/b/"+e+"/"+e+".js.gz",document.getElementsByTagName("script")[0].parentNode.insertBefore(t,document.getElementsByTagName("script")[0])}("GOYPYHQMLEOX")</script><meta property="og:url" content="https://boggs.tech/posts/evaluating-llm-reasoning-with-mtg-deck-building/"><meta property="og:site_name" content="Jake Boggs"><meta property="og:title" content="Evaluating Reasoning in LLMs Through MTG Deck Building"><meta property="og:description" content="Update (2026-01-14): Gemini 3 Pro, Gemini 3 Flash, GPT 5.2 (medium), Claude Opus 4.5, and Grok 4.1 Fast added.View Older UpdatesUpdate (2025-08-08): GPT-5, GPT-5 Mini, GPT-5 Nano, GPT-4 Turbo 11-06, and GPT-3.5 Turbo added.Update (2025-08-05): Kimi K2, GPT OSS 120B (low), and GPT OSS 120B (high) added.Update (2025-07-13): Grok 4, Grok 3, Gemini 2.5 Flash, Claude Sonnet 4 (thinking), and Command A added.Update (2025-06-11): o3 (high) added after API cost reduction.Update (2025-06-06): Deepseek R1 05-28 and Gemini 2.5 Pro 06-05 added.Update (2025-05-22): Claude Sonnet 4 and Opus 4 added.Update (2025-05-14): Human Baseline, Gemini 2.5 Pro 03-25, Gemini 1.5 Flash, Deepseek V3 03-24, Qwen3 30B 3A added.Introduction I have an obsession with applying AI models to my favorite card game, so I’ve created ManaBench, a benchmark designed to probe an LLM’s capacity for reasoning using the collectible card game Magic: The Gathering (MTG). With its intricate interactions and deep strategy, MTG serves as an ideal micro-world to test an LLM’s ability to process contextual information, identify patterns, and make judgments that align with expert human choices. This post provides an overview of the benchmark’s construction and the methodology used for evaluation."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-09T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-09T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Evaluating Reasoning in LLMs Through MTG Deck Building"><meta name=twitter:description content="
  Update (2026-01-14): Gemini 3 Pro, Gemini 3 Flash, GPT 5.2 (medium), Claude Opus 4.5, and Grok 4.1 Fast added.


  View Older Updates
  
    
    Update (2025-08-08): GPT-5, GPT-5 Mini, GPT-5 Nano, GPT-4 Turbo 11-06, and GPT-3.5 Turbo added.
    
    
      Update (2025-08-05): Kimi K2, GPT OSS 120B (low), and GPT OSS 120B (high) added.
    
    
      Update (2025-07-13): Grok 4, Grok 3, Gemini 2.5 Flash, Claude Sonnet 4 (thinking), and Command A added.
    
    
      Update (2025-06-11): o3 (high) added after API cost reduction.
    
    
      Update (2025-06-06): Deepseek R1 05-28 and Gemini 2.5 Pro 06-05 added.
    
    
      Update (2025-05-22): Claude Sonnet 4 and Opus 4 added.
    
    
      Update (2025-05-14): Human Baseline, Gemini 2.5 Pro 03-25, Gemini 1.5 Flash, Deepseek V3 03-24, Qwen3 30B 3A added.
    
  

Introduction
I have an obsession with applying AI models to my favorite card game, so I&rsquo;ve created ManaBench, a benchmark designed to probe an LLM&rsquo;s capacity for reasoning using the collectible card game Magic: The Gathering (MTG). With its intricate interactions and deep strategy, MTG serves as an ideal micro-world to test an LLM&rsquo;s ability to process contextual information, identify patterns, and make judgments that align with expert human choices. This post provides an overview of the benchmark&rsquo;s construction and the methodology used for evaluation."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://boggs.tech/posts/"},{"@type":"ListItem","position":2,"name":"Evaluating Reasoning in LLMs Through MTG Deck Building","item":"https://boggs.tech/posts/evaluating-llm-reasoning-with-mtg-deck-building/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Evaluating Reasoning in LLMs Through MTG Deck Building","name":"Evaluating Reasoning in LLMs Through MTG Deck Building","description":"\rUpdate (2026-01-14): Gemini 3 Pro, Gemini 3 Flash, GPT 5.2 (medium), Claude Opus 4.5, and Grok 4.1 Fast added.\rView Older Updates\rUpdate (2025-08-08): GPT-5, GPT-5 Mini, GPT-5 Nano, GPT-4 Turbo 11-06, and GPT-3.5 Turbo added.\rUpdate (2025-08-05): Kimi K2, GPT OSS 120B (low), and GPT OSS 120B (high) added.\rUpdate (2025-07-13): Grok 4, Grok 3, Gemini 2.5 Flash, Claude Sonnet 4 (thinking), and Command A added.\rUpdate (2025-06-11): o3 (high) added after API cost reduction.\rUpdate (2025-06-06): Deepseek R1 05-28 and Gemini 2.5 Pro 06-05 added.\rUpdate (2025-05-22): Claude Sonnet 4 and Opus 4 added.\rUpdate (2025-05-14): Human Baseline, Gemini 2.5 Pro 03-25, Gemini 1.5 Flash, Deepseek V3 03-24, Qwen3 30B 3A added.\rIntroduction I have an obsession with applying AI models to my favorite card game, so I\u0026rsquo;ve created ManaBench, a benchmark designed to probe an LLM\u0026rsquo;s capacity for reasoning using the collectible card game Magic: The Gathering (MTG). With its intricate interactions and deep strategy, MTG serves as an ideal micro-world to test an LLM\u0026rsquo;s ability to process contextual information, identify patterns, and make judgments that align with expert human choices. This post provides an overview of the benchmark\u0026rsquo;s construction and the methodology used for evaluation.\n","keywords":[],"articleBody":"\rUpdate (2026-01-14): Gemini 3 Pro, Gemini 3 Flash, GPT 5.2 (medium), Claude Opus 4.5, and Grok 4.1 Fast added.\rView Older Updates\rUpdate (2025-08-08): GPT-5, GPT-5 Mini, GPT-5 Nano, GPT-4 Turbo 11-06, and GPT-3.5 Turbo added.\rUpdate (2025-08-05): Kimi K2, GPT OSS 120B (low), and GPT OSS 120B (high) added.\rUpdate (2025-07-13): Grok 4, Grok 3, Gemini 2.5 Flash, Claude Sonnet 4 (thinking), and Command A added.\rUpdate (2025-06-11): o3 (high) added after API cost reduction.\rUpdate (2025-06-06): Deepseek R1 05-28 and Gemini 2.5 Pro 06-05 added.\rUpdate (2025-05-22): Claude Sonnet 4 and Opus 4 added.\rUpdate (2025-05-14): Human Baseline, Gemini 2.5 Pro 03-25, Gemini 1.5 Flash, Deepseek V3 03-24, Qwen3 30B 3A added.\rIntroduction I have an obsession with applying AI models to my favorite card game, so I’ve created ManaBench, a benchmark designed to probe an LLM’s capacity for reasoning using the collectible card game Magic: The Gathering (MTG). With its intricate interactions and deep strategy, MTG serves as an ideal micro-world to test an LLM’s ability to process contextual information, identify patterns, and make judgments that align with expert human choices. This post provides an overview of the benchmark’s construction and the methodology used for evaluation.\nThe core task in ManaBench is as follows: Given a 59-card main deck from a specific MTG constructed format (e.g., Modern, Legacy) - a deck originally constructed by a human player and sourced from tournament results - the LLM must choose the most suitable 60th card from a list of six options. One of these options is the “golden” card - the card that was originally in that slot in the human-designed decklist - while the other five are plausible alternatives generated by Manamorphosis, a diffusion model I trained specifically for completing MTG decks.\nThis task is difficult because it demands more than just factual recall about individual cards. To score well, models must be able to perform:\nStrategic Coherence Evaluation: The chosen card must align with the deck’s overall strategy (e.g., aggro, control, combo), a judgment that requires understanding the interplay of the existing 59 cards. System-Wide Optimization: The card should fit the deck’s mana curve and resource development plan, demonstrating an understanding of resource management within the game system. Complex Interaction Analysis (Card Synergies): Effective MTG play relies heavily on card interactions. The LLM needs to identify cards that synergize well with the existing 59 cards, showcasing an ability to reason about emergent properties of combined elements. Contextual Awareness (Format Knowledge): Different MTG formats have distinct card pools and power levels. The choice must be legal and relevant within the specified format, testing the LLM’s ability to operate within defined constraints. Discernment Against Plausible Alternatives: The alternatives are not entirely random but are generated by a model trained for the task, making them potentially attractive but likely sub-optimal choices. Successfully identifying the golden card requires fine-grained distinction based on strategic fit. Benchmark Leaderboard The human baseline has an admittedly small sample size (just myself, it’s difficult to find skilled MTG players who want to sit through a 200 question test), but is still a useful reference. As the creator of the benchmark and a player of the game for 10+ years, I scored 68% agreement with the other human deck builders. This could be is a skill issue on my end, but I think it is more likely due to the somewhat subjective nature of questions, along with the presentation format. To provide a fair comparison, I made a script that presents that questions to me exactly as they are shown to the LLMs, but I think I could do better using a deck editor. When I first released this benchmark, I beat most of the models by a wide margin. Several months later, GPT-5 essentially matched my score with 67% accuracy, followed closely by o3 and Gemini 2.5 Pro.\nMore recently, I evaluated Gemini 3 Pro, which reached 71.50%. This is impressive and it will be interesting to watch and see how performance continues improving in the near term. My expection is that scores will plateau somewhere in the 80-90% range, as the “most competitive” card for a deck can vary depending on the exact metagame at a specific time. While there many obviously wrong choices given the limited selection of 6 cards, I strongly suspect at least some of the questions in the dataset do not have a clear correct answer. If a model were to score 95%+, I would immediately question if it was overfit.\nThe results also highlight a discernible gap in performance between the leading American models (o3, Gemini 2.5 Pro, Claude) and Chinese models like Deepseek R1 (43.5%) and Kimi K2 (41%). While these models are undoubtedly powerful, their performance on here suggests that their reasoning capabilities may not be as developed as some of their US counterparts.\nPerformance Over Time This chart tracks the performance of the best scoring model on ManaBench available at each point in time.\nCost Over Time This chart shows the cheapest cost to reach GPT-4-level accuracy over time.\nAccuracy vs. Cost This chart visualizes the model cost vs performance. The x-axis represents a blended cost per million tokens, calculated with a 3:1 weighting of input to output costs. The y-axis shows the accuracy on ManaBench. Models on the red line represent the Pareto frontier.\nBenchmark Construction Methodology The creation of ManaBench involved several stages, from curating source decks to generating challenging multiple-choice questions. The process is designed to be rigorous and reproducible, with a focus on capturing instances of human expert judgment as reflected in competitive deck choices.\nSource Data \u0026 Deck Curation The foundation of the benchmark is a large corpus of human-constructed MTG decklists scraped from MTGTop8 (a public database of MTG tournament results and decklists). The curation process involves:\nFormat Filtering: Decks are categorized by major constructed formats: Modern, Pioneer, Legacy, and Vintage. The benchmark currently focuses on these eternal formats. Standard is excluded due to its rotating nature, which does not align with the goal of measuring reasoning abilities and minimizing the impact of knowledge cutoff dates, thereby keeping the focus on reasoning skills rather than rapidly changing format knowledge. Strict Validation: Only decks containing exactly 60 main deck cards and 15 sideboard cards are considered. This ensures that they obey conventional deck-building wisdom and that the creator carefully considered the metagame when constructing the deck. Format Legality Check: Using the card database from MTGJSON, every card in a potential source deck is verified for legality within its designated format. This step is crucial for ensuring that the decks and subsequent questions are valid within the game’s rules. Sampling: From the pool of validated decks, 25 decks are randomly sampled for each of the four target formats, resulting in a set of 100 unique, curated decklists for benchmark generation. These decks represent successful strategies designed and tested by human players in tournament play. Question Generation For each of the 100 sampled human-constructed decks, two unique questions are generated. This involves selecting a “golden” card to remove and then using the Manamorphosis model to propose alternatives.\nGolden Card Selection:\nThe “golden” card represents the correct answer for the deck completion task, reflecting a choice consistent with the original human-designed, tournament-sourced deck. It is chosen by randomly selecting one unique card name present in the original 60-card main deck. For instance, if a deck contains four copies of “Lightning Bolt,” “Lightning Bolt” is one possible unique card name that could be selected as the golden card. To ensure variety in the questions derived from a single deck, the two golden cards selected from the same source deck must be different card names. Partial Deck Creation:\nOnce a golden card name is selected, one instance of this card is removed from the original 60-card main deck list. This creates the 59-card partial deck that will be presented to the LLM. For example, if “Island” is chosen as the golden card from a deck containing 10 Islands, the partial deck will contain 9 Islands. Generating Plausible Alternatives:\nThe five incorrect-but-plausible alternatives are generated using a Manamorphosis, a Transformer-based diffusion model custom-trained trained on a vast corpus of MTG decks. It learns to represent cards as high-dimensional embeddings and understands patterns of card co-occurrence and deck structure. For benchmark generation, this model takes the 59-card partial deck and, through a reverse diffusion process conditioned on these known cards, predicts embeddings for the missing card. These embeddings are then mapped back to specific card names. This process is designed to generate alternatives that are contextually relevant yet distinct from the golden card. For a detailed technical explanation of the diffusion model’s architecture and training process, please refer to the Manamorphosis repository and the accompanying blog post. This generation process is repeated to obtain 5 unique card names that are different from the chosen golden card and from each other, serving as challenging distractors for the LLM. Question Structure (JSON): Each generated question is stored in a JSON object with the following structure:\n{ \"id\": \"question_001\", \"deck\": [\"Card Name 1\", \"Card Name 2\", \"Card Name 3\", \"... list of 59 card names ...\"], \"golden\": \"Actual Missing Card Name\", \"alternatives\": [ \"Alternative Card Name A\", \"Alternative Card Name B\", \"Alternative Card Name C\", \"Alternative Card Name D\", \"Alternative Card Name E\" ], \"format\": \"modern\" } Evaluation Protocol Prompting Strategy Careful prompt engineering is employed to provide the LLM with sufficient context to make a reasoned judgment, minimizing the need for memorized MTG card knowledge and emphasizing analytical skill:\nRole Assigment: The LLM is instructed: \"You are an expert Magic: The Gathering player.\" Task Definition: The prompt explains that a decklist from a specific format is missing one card and asked to choose the answer that best completes the deck. Decklist Presentation: The 59-card partial deck is provided. Each card entry includes: Its count in the partial deck. Its full name. Its detailed rules text, including type line, mana cost, power/toughness (for creatures), loyalty (for planeswalkers), and Oracle text. This information is sourced from MTGJSON. The explicit provision of full rules text for all cards in the deck and choices is a key design element, intended to reduce the task’s reliance on the LLM’s pre-existing knowledge of specific cards and instead focus on its ability to reason based on the provided information. Example Card Presentation in Prompt: 2x Snapcaster Mage - Creature - Human Wizard - Cost: {1}{U} - P/T: 2/1 - Rules: Flash. When Snapcaster Mage enters the battlefield, target instant or sorcery card in your graveyard gains flashback until end of turn. The flashback cost is equal to its mana cost. Multiple-Choice Options: The six choices (the golden card and the five alternatives) are presented, shuffled randomly to avoid positional bias, and labeled A through F. Each choice is also presented with its full name and detailed rules text (sourced from MTGJSON, same as deck cards). Example Choice Presentation in Prompt: A) Brainstorm - Instant - Cost: {U} - Rules: Draw three cards, then put two cards from your hand on top of your library in any order. Answer Format and Extraction To standardize evaluation, LLMs are instructed to output their final answer in a specific format: \"Respond with only the letter of your choice in the format: ANSWER: [LETTER]\"\nThe evaluation script parses this response using the following logic:\nPrimary Method: Looks for the “ANSWER: [LETTER]” pattern (case-insensitive for “ANSWER:”, extracts the letter). Fallback 1 (Single Letter): If the primary method fails, it checks if the entire response is a single letter from A to F (e.g., a response like “C”). Fallback 2 (Boxed Letter): If both above fail, it looks for the pattern $\\boxed{LETTER}$ (e.g., “The answer is $\\boxed{A}$”) using a regular expression. Metrics Accuracy: The primary metric is the percentage of questions the LLM answers correctly by selecting the golden card. Why ManaBench is a Strong Benchmark for Reasoning The preliminary results and the design of ManaBench demonstrate several key strengths for evaluating an LLM’s reasoning capabilities:\nMeasures Alignment with Human Expert Judgment: The “golden” answers are derived from decks designed by human players and sourced from MTGTop8, a database of tournament decklists. Success on this benchmark therefore indicates an LLM’s ability to make choices that align with established human expertise.\nClear Performance Differentiation: Compared to other popular evaluation metrics like LMArena ELO ratings, ManaBench has a significantly stronger ability to differentiate between models. While a general positive correlation is observed (as indicated by the trendline in the chart below), ManaBench provides a much wider relative spread in scores. For the models included in the comparison, ManaBench accuracies range from 19.5% to 67% (a spread of 47.5 percentage points, representing an increase of approximately 244% from the minimum observed score to the maximum). In contrast, their LMArena ELO scores range from 1257 to 1481 (a spread of 224 ELO points, representing an increase of approximately 17.8% from the minimum observed ELO score to the maximum). The significantly larger proportional range in ManaBench allows for a more granular distinction between models.\nChallenge for Frontier Models: GPT-5 now reaches 67%, just 1 point shy of the 68% human baseline, with o3 (high) at 65% and o3 (low) at 63%. Only GPT-5 and o3 exceed 60%, indicating the benchmark remains unsaturated and continues to present a significant challenge for frontier models.\nTest of Generalization vs. Benchmark Overfitting: The complexity of MTG deck construction, the private nature of the benchmark questions, and the fact that MTG strategy is unlikely to be a direct optimization target for most AI labs, collectively make ManaBench a strong test of generalized reasoning. Performance on this benchmark may reveal whether models are truly capable of applying reasoning to novel, complex systems, or if their high scores on common academic benchmarks (like MMLU or MATH) are partly due to overfitting. For example, a model series like Llama 4, which demonstrated strong performance on many standard benchmarks, gave a much weaker showing here. This aligns with the experiences of many users who reported that Llama 4 struggled with real tasks and underperformed expectations.\nCost-Effectiveness and Efficiency: With 200 questions, the benchmark is relatively small compared to some larger evaluation suites. This allows for more rapid and cost-effective evaluation cycles, making it feasible to test a wider array of models or fine-tuned variants without incurring prohibitive API costs or excessive computation time, while still providing a strong differentiating signal.\nConclusion The initial results demonstrate ManaBench’s potential as a benchmark, with even frontier models finding the task challenging. To maintain the integrity and long-term utility of the benchmark, the specific questions are not being publicly released at this time. This is to prevent them from being inadvertently included in the training data of future LLMs, which would compromise its validity as an unseen test set.\nIf you would me to add other models to the leaderboard or just think it’s a cool project, consider checking out my other work or following me on Twitter\n","wordCount":"2494","inLanguage":"en","datePublished":"2025-05-09T00:00:00Z","dateModified":"2025-05-09T00:00:00Z","author":{"@type":"Person","name":"Jake Boggs"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://boggs.tech/posts/evaluating-llm-reasoning-with-mtg-deck-building/"},"publisher":{"@type":"Organization","name":"Jake Boggs","logo":{"@type":"ImageObject","url":"https://boggs.tech/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://boggs.tech/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://boggs.tech/about title="About me"><span>About me</span></a></li><li><a href=https://boggs.tech/startups title=Startups><span>Startups</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Evaluating Reasoning in LLMs Through MTG Deck Building</h1><div class=post-meta><span title='2025-05-09 00:00:00 +0000 UTC'>May 9, 2025</span>&nbsp;·&nbsp;<span>12 min</span>&nbsp;·&nbsp;<span>2494 words</span>&nbsp;·&nbsp;<span>Jake Boggs</span></div></header><div class=post-content><div style="background:#e0f7fa;color:#006064;padding:12px 18px;border-radius:6px;margin-bottom:18px;font-size:1.08em;font-weight:500"><strong>Update (2026-01-14):</strong> Gemini 3 Pro, Gemini 3 Flash, GPT 5.2 (medium), Claude Opus 4.5, and Grok 4.1 Fast added.</div><details style=margin-bottom:18px><summary style=cursor:pointer;font-weight:500>View Older Updates</summary><div style=padding-top:10px><div style="background:#e0f7fa;color:#006064;padding:12px 18px;border-radius:6px;margin-bottom:18px;font-size:1.08em;font-weight:500"><strong>Update (2025-08-08):</strong> GPT-5, GPT-5 Mini, GPT-5 Nano, GPT-4 Turbo 11-06, and GPT-3.5 Turbo added.</div><div style="background:#e0f7fa;color:#006064;padding:12px 18px;border-radius:6px;margin-bottom:18px;font-size:1.08em;font-weight:500"><strong>Update (2025-08-05):</strong> Kimi K2, GPT OSS 120B (low), and GPT OSS 120B (high) added.</div><div style="background:#e0f7fa;color:#006064;padding:12px 18px;border-radius:6px;margin-bottom:18px;font-size:1.08em;font-weight:500"><strong>Update (2025-07-13):</strong> Grok 4, Grok 3, Gemini 2.5 Flash, Claude Sonnet 4 (thinking), and Command A added.</div><div style="background:#e0f7fa;color:#006064;padding:12px 18px;border-radius:6px;margin-bottom:18px;font-size:1.08em;font-weight:500"><strong>Update (2025-06-11):</strong> o3 (high) added after API cost reduction.</div><div style="background:#e0f7fa;color:#006064;padding:12px 18px;border-radius:6px;margin-bottom:18px;font-size:1.08em;font-weight:500"><strong>Update (2025-06-06):</strong> Deepseek R1 05-28 and Gemini 2.5 Pro 06-05 added.</div><div style="background:#e0f7fa;color:#006064;padding:12px 18px;border-radius:6px;margin-bottom:18px;font-size:1.08em;font-weight:500"><strong>Update (2025-05-22):</strong> Claude Sonnet 4 and Opus 4 added.</div><div style="background:#e0f7fa;color:#006064;padding:12px 18px;border-radius:6px;margin-bottom:0;font-size:1.08em;font-weight:500"><strong>Update (2025-05-14):</strong> Human Baseline, Gemini 2.5 Pro 03-25, Gemini 1.5 Flash, Deepseek V3 03-24, Qwen3 30B 3A added.</div></div></details><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>I have an obsession with applying AI models to my favorite card game, so I&rsquo;ve created ManaBench, a benchmark designed to probe an LLM&rsquo;s capacity for reasoning using the collectible card game Magic: The Gathering (MTG). With its intricate interactions and deep strategy, MTG serves as an ideal micro-world to test an LLM&rsquo;s ability to process contextual information, identify patterns, and make judgments that align with expert human choices. This post provides an overview of the benchmark&rsquo;s construction and the methodology used for evaluation.</p><p>The core task in ManaBench is as follows: Given a 59-card main deck from a specific MTG constructed format (e.g., Modern, Legacy) - a deck originally constructed by a human player and sourced from tournament results - the LLM must choose the most suitable 60th card from a list of six options. One of these options is the &ldquo;golden&rdquo; card - the card that was originally in that slot in the human-designed decklist - while the other five are plausible alternatives generated by <a href=https://github.com/JakeBoggs/Manamorphosis>Manamorphosis</a>, a diffusion model I trained specifically for completing MTG decks.</p><p>This task is difficult because it demands more than just factual recall about individual cards. To score well, models must be able to perform:</p><ul><li><strong>Strategic Coherence Evaluation:</strong> The chosen card must align with the deck&rsquo;s overall strategy (e.g., aggro, control, combo), a judgment that requires understanding the interplay of the existing 59 cards.</li><li><strong>System-Wide Optimization:</strong> The card should fit the deck&rsquo;s mana curve and resource development plan, demonstrating an understanding of resource management within the game system.</li><li><strong>Complex Interaction Analysis (Card Synergies):</strong> Effective MTG play relies heavily on card interactions. The LLM needs to identify cards that synergize well with the existing 59 cards, showcasing an ability to reason about emergent properties of combined elements.</li><li><strong>Contextual Awareness (Format Knowledge):</strong> Different MTG formats have distinct card pools and power levels. The choice must be legal and relevant within the specified format, testing the LLM&rsquo;s ability to operate within defined constraints.</li><li><strong>Discernment Against Plausible Alternatives:</strong> The alternatives are not entirely random but are generated by a model trained for the task, making them potentially attractive but likely sub-optimal choices. Successfully identifying the golden card requires fine-grained distinction based on strategic fit.</li></ul><h2 id=benchmark-leaderboard>Benchmark Leaderboard<a hidden class=anchor aria-hidden=true href=#benchmark-leaderboard>#</a></h2><script src=https://cdn.jsdelivr.net/npm/chart.js@3.9.1/dist/chart.min.js></script><script src=https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns@2.0.0/dist/chartjs-adapter-date-fns.bundle.min.js></script><script src=https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2.2.0/dist/chartjs-plugin-datalabels.min.js></script><div style="width:90%;margin:20px auto"><canvas id=leaderboardChart></canvas></div><script>document.addEventListener("DOMContentLoaded",()=>{const e=document.getElementById("leaderboardChart"),s=e.getContext("2d"),t=["Gemini 3 Pro","Gemini 3 Flash","Human Baseline","GPT-5","o3 (high)","o3 (low)","GPT 5.2 (medium)","Gemini 2.5 Pro 06-05","Claude Opus 4.5","Gemini 2.5 Pro 03-25","Grok 4","Gemini 2.5 Flash","Claude 3.7 Sonnet (no thinking)","Grok 4.1 Fast","GPT-5 Mini","o4 Mini (low)","Claude Sonnet 4 (thinking)","Deepseek R1","Deepseek R1 05-28","Grok 3","GPT-4o 08-06","Kimi K2","Claude Opus 4 (no thinking)","GPT OSS 120B (high)","GPT-4 Turbo 11-06","Claude Sonnet 4 (no thinking)","Deepseek V3 03-24","Qwen3 235B A22B (thinking)","Grok 3 Mini (low)","GPT-5 Nano","GPT OSS 120B (low)","Gemini 2.0 Flash","Mistral 3 Medium","Qwen3 30B A3B (thinking)","Gemini 1.5 Pro","Llama 4 Maverick","GPT-3.5 Turbo","Command A","Gemini 1.5 Flash","GPT-4.1 Nano","Llama 3.3 70B","Random Guessing"],o=t.length,i=15,a=100,r=o*i+a;e.style.height=r+"px";const c=[71.5,70.5,68,67,65,63,61,57.5,54.5,53,52.5,50,49.5,46,45.5,45,44,43.5,43,41.5,41,41,40.5,40,39.5,38,37.5,37,37,36,35.5,35,31.5,28.5,27.5,26.5,26,25,22.5,19.5,19.5,16.67],n=["rgba(0, 150, 255, 0.85)","rgba(0, 200, 150, 0.85)","rgba(34, 139, 34, 0.8)","rgba(0, 114, 178, 0.85)","rgba(70, 130, 220, 0.8)","rgba(75, 192, 192, 0.8)","rgba(0, 80, 160, 0.85)","rgba(0, 220, 220, 0.8)","rgba(210, 120, 80, 0.85)","rgba(0, 200, 255, 0.8)","rgba(220, 20, 60, 0.8)","rgba(50, 205, 50, 0.8)","rgba(54, 162, 235, 0.8)","rgba(255, 115, 0, 0.85)","rgba(204, 121, 167, 0.85)","rgba(255, 206, 86, 0.8)","rgba(138, 43, 226, 0.8)","rgba(255, 99, 132, 0.8)","rgba(255, 99, 100, 0.8)","rgba(255, 140, 0, 0.8)","rgba(153, 102, 255, 0.8)","rgba(0, 100, 0, 0.8)","rgba(205, 133, 63, 0.8)","rgba(255, 215, 0, 0.8)","rgba(90, 90, 90, 0.8)","rgba(188, 143, 143, 0.8)","rgba(128, 128, 0, 0.8)","rgba(255, 159, 64, 0.8)","rgba(101, 143, 74, 0.8)","rgba(0, 158, 115, 0.85)","rgba(255, 192, 203, 0.8)","rgba(210, 105, 30, 0.8)","rgba(0, 128, 128, 0.8)","rgba(0, 0, 205, 0.8)","rgba(60, 179, 113, 0.8)","rgba(165, 42, 42, 0.8)","rgba(106, 90, 205, 0.8)","rgba(139, 69, 19, 0.8)","rgba(255, 105, 180, 0.8)","rgba(70, 130, 180, 0.8)","rgba(128, 0, 128, 0.8)","rgba(150, 150, 150, 0.8)"],l=n.map(e=>e.replace("0.8","1").replace("0.85","1"));new Chart(s,{type:"bar",data:{labels:t,datasets:[{label:"Accuracy",data:c,backgroundColor:n,borderColor:l,borderWidth:1}]},options:{indexAxis:"y",responsive:!0,maintainAspectRatio:!1,plugins:{legend:{display:!1},title:{display:!0,text:"ManaBench LLM Leaderboard (Accuracy %)",font:{size:18}},tooltip:{callbacks:{label:function(e){let t=e.dataset.label||"";return t&&(t+=": "),e.parsed.x!==null&&(t+=e.parsed.x+"%"),t}}}},scales:{x:{beginAtZero:!0,title:{display:!0,text:"Accuracy (%)"}},y:{ticks:{autoSkip:!1}}}}})})</script><p>The human baseline has an admittedly small sample size (just myself, it&rsquo;s difficult to find skilled MTG players who want to sit through a 200 question test), but is still a useful reference. As the creator of the benchmark and a player of the game for 10+ years, I scored 68% agreement with the other human deck builders. This could be is a skill issue on my end, but I think it is more likely due to the somewhat subjective nature of questions, along with the presentation format. To provide a fair comparison, I made a script that presents that questions to me exactly as they are shown to the LLMs, but I think I could do better using a deck editor. When I first released this benchmark, I beat most of the models by a wide margin. Several months later, GPT-5 essentially matched my score with 67% accuracy, followed closely by o3 and Gemini 2.5 Pro.</p><p>More recently, I evaluated Gemini 3 Pro, which reached 71.50%. This is impressive and it will be interesting to watch and see how performance continues improving in the near term. My expection is that scores will plateau somewhere in the 80-90% range, as the &ldquo;most competitive&rdquo; card for a deck can vary depending on the exact metagame at a specific time. While there many obviously wrong choices given the limited selection of 6 cards, I strongly suspect at least some of the questions in the dataset do not have a clear correct answer. If a model were to score 95%+, I would immediately question if it was overfit.</p><p>The results also highlight a discernible gap in performance between the leading American models (o3, Gemini 2.5 Pro, Claude) and Chinese models like Deepseek R1 (43.5%) and Kimi K2 (41%). While these models are undoubtedly powerful, their performance on here suggests that their reasoning capabilities may not be as developed as some of their US counterparts.</p><h2 id=performance-over-time>Performance Over Time<a hidden class=anchor aria-hidden=true href=#performance-over-time>#</a></h2><p>This chart tracks the performance of the best scoring model on ManaBench available at each point in time.</p><div style="width:90%;margin:20px auto;min-height:600px"><canvas id=timeSeriesChart></canvas></div><script src=https://cdn.jsdelivr.net/npm/chart.js@3.9.1/dist/chart.min.js></script><script src=https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns@2.0.0/dist/chartjs-adapter-date-fns.bundle.min.js></script><script src=https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2.2.0/dist/chartjs-plugin-datalabels.min.js></script><script>document.addEventListener("DOMContentLoaded",()=>{const p=document.getElementById("timeSeriesChart").getContext("2d"),m=[{name:"Gemini 3 Pro",score:71.5,releaseDate:"2026-01-14",color:"rgba(0, 150, 255, 0.85)"},{name:"Gemini 3 Flash",score:70.5,releaseDate:"2026-01-14",color:"rgba(0, 200, 150, 0.85)"},{name:"GPT 5.2 (medium)",score:61,releaseDate:"2026-01-14",color:"rgba(0, 80, 160, 0.85)"},{name:"Claude Opus 4.5",score:54.5,releaseDate:"2026-01-14",color:"rgba(210, 120, 80, 0.85)"},{name:"Grok 4.1 Fast",score:46,releaseDate:"2026-01-14",color:"rgba(255, 115, 0, 0.85)"},{name:"GPT-5",score:67,releaseDate:"2025-08-07",color:"rgba(0, 114, 178, 0.85)"},{name:"GPT-5 Mini",score:45.5,releaseDate:"2025-08-07",color:"rgba(204, 121, 167, 0.85)"},{name:"GPT-5 Nano",score:36,releaseDate:"2025-08-07",color:"rgba(0, 158, 115, 0.85)"},{name:"o3 (high)",score:65,releaseDate:"2025-04-16",color:"rgba(70, 130, 220, 0.8)"},{name:"o3 (low)",score:63,releaseDate:"2025-04-16",color:"rgba(75, 192, 192, 0.8)"},{name:"Gemini 2.5 Pro 06-05",score:57.5,releaseDate:"2025-06-05",color:"rgba(0, 220, 220, 0.8)"},{name:"Gemini 2.5 Pro 03-25",score:53,releaseDate:"2025-03-28",color:"rgba(0, 200, 255, 0.8)"},{name:"Grok 4",score:52.5,releaseDate:"2025-07-09",color:"rgba(220, 20, 60, 0.8)"},{name:"Gemini 2.5 Flash",score:50,releaseDate:"2025-04-17",color:"rgba(50, 205, 50, 0.8)"},{name:"Claude 3.7 Sonnet (no thinking)",score:49.5,releaseDate:"2025-02-24",color:"rgba(54, 162, 235, 0.8)"},{name:"o4 Mini (low)",score:45,releaseDate:"2025-04-16",color:"rgba(255, 206, 86, 0.8)"},{name:"Claude Sonnet 4 (thinking)",score:44,releaseDate:"2025-05-22",color:"rgba(138, 43, 226, 0.8)"},{name:"Deepseek R1",score:43.5,releaseDate:"2025-01-20",color:"rgba(255, 99, 132, 0.8)"},{name:"Deepseek R1 05-28",score:43,releaseDate:"2025-05-28",color:"rgba(255, 99, 100, 0.8)"},{name:"Grok 3",score:41.5,releaseDate:"2025-02-17",color:"rgba(255, 140, 0, 0.8)"},{name:"GPT-4o 08-06",score:41,releaseDate:"2024-08-06",color:"rgba(153, 102, 255, 0.8)"},{name:"Kimi K2",score:41,releaseDate:"2025-07-11",color:"rgba(0, 100, 0, 0.8)"},{name:"Claude Opus 4 (no thinking)",score:40.5,releaseDate:"2025-05-22",color:"rgba(205, 133, 63, 0.8)"},{name:"GPT OSS 120B (high)",score:40,releaseDate:"2025-08-05",color:"rgba(255, 215, 0, 0.8)"},{name:"GPT-4 Turbo 11-06",score:39.5,releaseDate:"2023-11-06",color:"rgba(90, 90, 90, 0.8)"},{name:"Claude Sonnet 4 (no thinking)",score:38,releaseDate:"2025-05-22",color:"rgba(188, 143, 143, 0.8)"},{name:"Deepseek V3 03-24",score:37.5,releaseDate:"2025-03-24",color:"rgba(128, 128, 0, 0.8)"},{name:"Qwen3 235B A22B (thinking)",score:37,releaseDate:"2025-04-28",color:"rgba(255, 159, 64, 0.8)"},{name:"Grok 3 Mini (low)",score:37,releaseDate:"2025-02-17",color:"rgba(101, 143, 74, 0.8)"},{name:"GPT OSS 120B (low)",score:35.5,releaseDate:"2025-08-05",color:"rgba(255, 192, 203, 0.8)"},{name:"Gemini 2.0 Flash",score:35,releaseDate:"2025-02-05",color:"rgba(210, 105, 30, 0.8)"},{name:"Mistral 3 Medium",score:31.5,releaseDate:"2025-06-26",color:"rgba(0, 128, 128, 0.8)"},{name:"Qwen3 30B A3B (thinking)",score:28.5,releaseDate:"2025-07-31",color:"rgba(0, 0, 205, 0.8)"},{name:"Llama 4 Maverick",score:26.5,releaseDate:"2025-04-05",color:"rgba(165, 42, 42, 0.8)"},{name:"Gemini 1.5 Pro",score:27.5,releaseDate:"2024-04-09",color:"rgba(60, 179, 113, 0.8)"},{name:"GPT-3.5 Turbo",score:26,releaseDate:"2023-05-28",color:"rgba(106, 90, 205, 0.8)"},{name:"Command A",score:25,releaseDate:"2025-03-13",color:"rgba(139, 69, 19, 0.8)"},{name:"Gemini 1.5 Flash",score:22.5,releaseDate:"2024-05-14",color:"rgba(255, 105, 180, 0.8)"},{name:"GPT-4.1 Nano",score:19.5,releaseDate:"2025-04-14",color:"rgba(70, 130, 180, 0.8)"},{name:"Llama 3.3 70B",score:19.5,releaseDate:"2024-12-05",color:"rgba(128, 0, 128, 0.8)"}];window.modelDataWithRelease=m;const j=m.sort((e,t)=>new Date(e.releaseDate)-new Date(t.releaseDate)||t.score-e.score);let h=-1;const b=j.filter(e=>e.score>h&&(h=e.score,!0)),t=b.map(e=>({x:new Date(e.releaseDate),y:e.score,label:e.name,color:e.color})),u=t.map(e=>e.y),g=Math.min(...u),f=Math.max(...u),c=t.map(e=>e.x.getTime()),r=Math.min(...c),l=Math.max(...c),d=Math.max((l-r)*.05,1e3*60*60*24*7),n=new Date(r-d),a=new Date(l+d),v=new Date((n.getTime()+a.getTime())/2),e=t.map(e=>({x:e.x.getTime(),y:e.y}));let i={};if(e.length>=3){const d=e.length;let r=0,u=0,o=0,a=0,m=0,h=0,l=0;for(let n=0;n<d;n++){const t=e[n];r+=t.x,u+=t.y,o+=t.x*t.x,a+=t.x*t.x*t.x,m+=t.x*t.x*t.x*t.x,h+=t.x*t.y,l+=t.x*t.x*t.y}const t=[[m,a,o],[a,o,r],[o,r,d]],n=[l,h,u];for(let e=0;e<3;e++){let s=e;for(let n=e+1;n<3;n++)Math.abs(t[n][e])>Math.abs(t[s][e])&&(s=n);[t[e],t[s]]=[t[s],t[e]],[n[e],n[s]]=[n[s],n[e]];for(let s=e+1;s<3;s++){const o=t[s][e]/t[e][e];for(let n=e;n<3;n++)t[s][n]-=o*t[e][n];n[s]-=o*n[e]}}const s=new Array(3);for(let e=2;e>=0;e--){s[e]=n[e];for(let n=e+1;n<3;n++)s[e]-=t[e][n]*s[n];s[e]/=t[e][e]}const[g,v,b]=s,c=Math.min(...e.map(e=>e.x)),j=Math.max(...e.map(e=>e.x)),f=[],p=30;for(let t=0;t<=p;t++){const e=c+(j-c)*t/p,n=g*e*e+v*e+b;f.push({x:new Date(e),y:n})}i={type:"line",label:"Trendline",data:f,borderColor:"rgba(255, 0, 0, 0.7)",borderWidth:2,fill:!1,pointRadius:0,tension:.1,datalabels:{display:!1}}}const o=[{label:"Models",type:"scatter",data:t,backgroundColor:t.map(e=>e.color),pointRadius:6,pointHoverRadius:8,datalabels:{formatter:(e,t)=>{const n=t.dataset.data[t.dataIndex];return t.active?`${n.label}: ${n.y}% on ${n.x.toISOString().split("T")[0]}`:n.label},font:function(e){return{size:9,weight:e.active?"bold":"500"}},backgroundColor:function(e){return e.active?e.dataset.backgroundColor[e.dataIndex].replace("0.8","1"):null},borderColor:function(e){return e.active?"white":null},borderRadius:4,borderWidth:1,color:function(e){return e.active?"white":"#333"},padding:4,align:"top",offset:8,anchor:"center"}}];Object.keys(i).length>0&&o.push(i);const s=68;o.push({type:"line",label:"Human Baseline",data:[{x:n,y:s},{x:v,y:s},{x:a,y:s}],borderColor:"rgba(34, 139, 34, 0.8)",borderDash:[6,4],borderWidth:2,fill:!1,pointRadius:0,datalabels:{formatter:(e,t)=>t.dataIndex===1?"Human Baseline":null,align:"top",anchor:"center",offset:6,color:"rgba(34, 139, 34, 0.95)",backgroundColor:"rgba(255, 255, 255, 0.9)",borderColor:"rgba(34, 139, 34, 0.95)",borderWidth:1,borderRadius:4,padding:3,font:{size:9,weight:"bold"}}}),new Chart(p,{type:"scatter",plugins:[ChartDataLabels],data:{datasets:o},options:{responsive:!0,maintainAspectRatio:!1,plugins:{legend:{display:!1},title:{display:!0,text:"ManaBench High Score",font:{size:18}},tooltip:{enabled:!1}},scales:{x:{type:"time",time:{unit:"month",tooltipFormat:"yyyy-MM-dd",displayFormats:{month:"MMM yyyy"}},min:n,max:a,title:{display:!0,text:"Model Release Date"}},y:{min:Math.max(g-4,0),max:Math.min(f+4,100),title:{display:!0,text:"Accuracy (%)"}}}}})})</script><h2 id=cost-over-time>Cost Over Time<a hidden class=anchor aria-hidden=true href=#cost-over-time>#</a></h2><p>This chart shows the cheapest cost to reach GPT-4-level accuracy over time.</p><div style="width:90%;margin:20px auto;min-height:600px"><canvas id=costOverTimeChart></canvas></div><script>document.addEventListener("DOMContentLoaded",()=>{const d=document.getElementById("costOverTimeChart").getContext("2d"),i=(window.modelDataWithRelease||[]).slice(),a={"Gemini 3 Pro":{in:2,out:12},"Gemini 3 Flash":{in:.5,out:3},"GPT 5.2 (medium)":{in:1.75,out:14},"Claude Opus 4.5":{in:5,out:25},"Grok 4.1 Fast":{in:.2,out:.5},"GPT-5":{in:1.25,out:10},"GPT-5 Mini":{in:.25,out:2},"GPT-5 Nano":{in:.05,out:.4},"o3 (high)":{in:2,out:8},"o3 (low)":{in:2,out:8},"Gemini 2.5 Pro 06-05":{in:2.5,out:15},"Gemini 2.5 Pro 03-25":{in:2.5,out:15},"Grok 4":{in:3,out:15},"Gemini 2.5 Flash":{in:.3,out:2.5},"Claude 3.7 Sonnet (no thinking)":{in:3,out:15},"o4 Mini (low)":{in:1.1,out:4.4},"Claude Sonnet 4 (thinking)":{in:3,out:15},"Deepseek R1":{in:.45,out:2.15},"Deepseek R1 05-28":{in:.5,out:2.15},"Grok 3":{in:3,out:15},"GPT-4o 08-06":{in:2.5,out:10},"Kimi K2":{in:.55,out:2.2},"Claude Opus 4 (no thinking)":{in:15,out:75},"GPT OSS 120B (high)":{in:.09,out:.45},"GPT-4 Turbo 11-06":{in:10,out:30},"Claude Sonnet 4 (no thinking)":{in:3,out:15},"Deepseek V3 03-24":{in:.27,out:1.1},"Qwen3 235B A22B (thinking)":{in:.13,out:.6},"Grok 3 Mini (low)":{in:.3,out:.5},"GPT OSS 120B (low)":{in:.09,out:.45},"Gemini 2.0 Flash":{in:.1,out:.4},"Mistral 3 Medium":{in:.4,out:2},"Qwen3 30B A3B (thinking)":{in:.08,out:.29},"Llama 4 Maverick":{in:.15,out:.6},"Gemini 1.5 Pro":{in:1.25,out:5},"GPT-3.5 Turbo":{in:.5,out:1.5},"Command A":{in:2,out:8},"Gemini 1.5 Flash":{in:.075,out:.3},"GPT-4.1 Nano":{in:.1,out:.4},"Llama 3.3 70B":{in:.038,out:.12}};function r(e){const t=a[e];return t?(t.in*3+t.out*1)/4:null}const t=[{key:"gpt4",label:"GPT-4-level",minAccuracy:39.5,color:"rgba(70, 130, 180, 0.9)"},{key:"gpt5",label:"GPT-5-level",minAccuracy:67,color:"rgba(0, 114, 178, 0.85)"}],n={};t.forEach(e=>{const a=i.filter(t=>t.score>=e.minAccuracy).map(e=>({...e,date:new Date(e.releaseDate),cost:r(e.name)})).filter(e=>e.cost!=null).sort((e,t)=>e.date.getTime()-t.date.getTime());let t=1/0,s=null;const o=[];a.forEach(e=>{e.cost<t-1e-9&&(t=e.cost,s=e.name,o.push({x:e.date,y:t,label:s}))}),n[e.key]=o});const s=t.flatMap(e=>n[e.key].map(e=>e.y)).filter(e=>e!=null),c=s.length?Math.max(Math.min(...s)*.9,.001):.001,l=s.length?Math.max(...s)*1.1:100,e=t.flatMap(e=>n[e.key].map(e=>e.x.getTime())),o=1e3*60*60*24*45,u=e.length?new Date(Math.min(...e)-o):0[0],h=e.length?new Date(Math.max(...e)+o):0[0];new Chart(d,{type:"line",plugins:[ChartDataLabels],data:{datasets:t.map(e=>({label:e.label,data:n[e.key],borderColor:e.color,backgroundColor:"transparent",pointBackgroundColor:e.color,borderWidth:2,pointRadius:5,pointHoverRadius:7,tension:0,stepped:"before",fill:!1,datalabels:{formatter:(e,t)=>{const n=t.dataset.data[t.dataIndex];if(t.active){const e=n.x instanceof Date?n.x.toISOString().split("T")[0]:"";return`${n.label}: $${Number(n.y).toFixed(2)} on ${e}`}return n.label},font:e=>({size:9,weight:e.active?"bold":"500"}),backgroundColor:t=>t.active?e.color.replace("0.8","1").replace("0.85","1"):null,borderColor:e=>e.active?"white":null,borderRadius:4,borderWidth:1,color:e=>e.active?"white":"#333",padding:4,align:"top",offset:8,anchor:"center",clip:!1}}))},options:{responsive:!0,maintainAspectRatio:!1,plugins:{title:{display:!0,text:"Cost of fixed intelligence level over time",font:{size:18}},tooltip:{enabled:!1},legend:{display:!0}},scales:{x:{type:"time",time:{unit:"month",tooltipFormat:"yyyy-MM-dd",displayFormats:{month:"MMM yyyy"}},title:{display:!0,text:"Date"},min:u,max:h,grid:{drawBorder:!1}},y:{type:"logarithmic",min:c,max:l,title:{display:!0,text:"Blended Cost ($ per 1M tokens)"},ticks:{callback:(e,t,n)=>{try{const o=n&&n[t];if(!o||!o.major)return"";const s=Number(e);return isFinite(s)?s>=10?`$${s.toFixed(0)}`:s>=1?`$${s.toFixed(1)}`:`$${s.toPrecision(1)}`:""}catch{return""}}},grid:{drawBorder:!1,color:e=>e.tick?.major?"rgba(0,0,0,0.15)":"rgba(0,0,0,0.06)",lineWidth:e=>e.tick?.major?1:.5}}},layout:{padding:{right:12,top:8,bottom:8}}}})})</script><h2 id=accuracy-vs-cost>Accuracy vs. Cost<a hidden class=anchor aria-hidden=true href=#accuracy-vs-cost>#</a></h2><p>This chart visualizes the model cost vs performance. The x-axis represents a blended cost per million tokens, calculated with a 3:1 weighting of input to output costs. The y-axis shows the accuracy on ManaBench. Models on the red line represent the Pareto frontier.</p><div style="width:90%;margin:20px auto;min-height:600px"><canvas id=paretoChart></canvas></div><script>document.addEventListener("DOMContentLoaded",()=>{const c=document.getElementById("paretoChart").getContext("2d"),r=[{name:"Gemini 3 Pro",accuracy:71.5,inputCost:2,outputCost:12,color:"rgba(0, 150, 255, 0.85)"},{name:"Gemini 3 Flash",accuracy:70.5,inputCost:.5,outputCost:3,color:"rgba(0, 200, 150, 0.85)"},{name:"GPT 5.2 (medium)",accuracy:61,inputCost:1.75,outputCost:14,color:"rgba(0, 80, 160, 0.85)"},{name:"Claude Opus 4.5",accuracy:54.5,inputCost:5,outputCost:25,color:"rgba(210, 120, 80, 0.85)"},{name:"Grok 4.1 Fast",accuracy:46,inputCost:.2,outputCost:.5,color:"rgba(255, 115, 0, 0.85)"},{name:"GPT-5",accuracy:67,inputCost:1.25,outputCost:10,color:"rgba(0, 114, 178, 0.85)"},{name:"GPT-5 Mini",accuracy:45.5,inputCost:.25,outputCost:2,color:"rgba(204, 121, 167, 0.85)"},{name:"GPT-5 Nano",accuracy:36,inputCost:.05,outputCost:.4,color:"rgba(0, 158, 115, 0.85)"},{name:"o3 (high)",accuracy:65,inputCost:2,outputCost:8,color:"rgba(70, 130, 220, 0.8)"},{name:"o3 (low)",accuracy:63,inputCost:2,outputCost:8,color:"rgba(75, 192, 192, 0.8)"},{name:"Gemini 2.5 Pro 06-05",accuracy:57.5,inputCost:2.5,outputCost:15,color:"rgba(0, 220, 220, 0.8)"},{name:"Gemini 2.5 Pro 03-25",accuracy:53,inputCost:2.5,outputCost:15,color:"rgba(0, 200, 255, 0.8)"},{name:"Grok 4",accuracy:52.5,inputCost:3,outputCost:15,color:"rgba(220, 20, 60, 0.8)"},{name:"Gemini 2.5 Flash",accuracy:50,inputCost:.3,outputCost:2.5,color:"rgba(50, 205, 50, 0.8)"},{name:"Claude 3.7 Sonnet (no thinking)",accuracy:49.5,inputCost:3,outputCost:15,color:"rgba(54, 162, 235, 0.8)"},{name:"o4 Mini (low)",accuracy:45,inputCost:1.1,outputCost:4.4,color:"rgba(255, 206, 86, 0.8)"},{name:"Claude Sonnet 4 (thinking)",accuracy:44,inputCost:3,outputCost:15,color:"rgba(138, 43, 226, 0.8)"},{name:"Deepseek R1",accuracy:43.5,inputCost:.45,outputCost:2.15,color:"rgba(255, 99, 132, 0.8)"},{name:"Deepseek R1 05-28",accuracy:43,inputCost:.5,outputCost:2.15,color:"rgba(255, 99, 100, 0.8)"},{name:"Grok 3",accuracy:41.5,inputCost:3,outputCost:15,color:"rgba(255, 140, 0, 0.8)"},{name:"GPT-4o 08-06",accuracy:41,inputCost:2.5,outputCost:10,color:"rgba(153, 102, 255, 0.8)"},{name:"Kimi K2",accuracy:41,inputCost:.55,outputCost:2.2,color:"rgba(0, 100, 0, 0.8)"},{name:"Claude Opus 4 (no thinking)",accuracy:40.5,inputCost:15,outputCost:75,color:"rgba(205, 133, 63, 0.8)"},{name:"GPT OSS 120B (high)",accuracy:40,inputCost:.09,outputCost:.45,color:"rgba(255, 215, 0, 0.8)"},{name:"GPT-4 Turbo 11-06",accuracy:39.5,inputCost:10,outputCost:30,color:"rgba(90, 90, 90, 0.8)"},{name:"Claude Sonnet 4 (no thinking)",accuracy:38,inputCost:3,outputCost:15,color:"rgba(188, 143, 143, 0.8)"},{name:"Deepseek V3 03-24",accuracy:37.5,inputCost:.27,outputCost:1.1,color:"rgba(128, 128, 0, 0.8)"},{name:"Qwen3 235B A22B (thinking)",accuracy:37,inputCost:.13,outputCost:.6,color:"rgba(255, 159, 64, 0.8)"},{name:"Grok 3 Mini (low)",accuracy:37,inputCost:.3,outputCost:.5,color:"rgba(101, 143, 74, 0.8)"},{name:"GPT OSS 120B (low)",accuracy:35.5,inputCost:.09,outputCost:.45,color:"rgba(255, 192, 203, 0.8)"},{name:"Gemini 2.0 Flash",accuracy:35,inputCost:.1,outputCost:.4,color:"rgba(210, 105, 30, 0.8)"},{name:"Mistral 3 Medium",accuracy:31.5,inputCost:.4,outputCost:2,color:"rgba(0, 128, 128, 0.8)"},{name:"Qwen3 30B A3B (thinking)",accuracy:28.5,inputCost:.08,outputCost:.29,color:"rgba(0, 0, 205, 0.8)"},{name:"Llama 4 Maverick",accuracy:26.5,inputCost:.15,outputCost:.6,color:"rgba(165, 42, 42, 0.8)"},{name:"Gemini 1.5 Pro",accuracy:27.5,inputCost:1.25,outputCost:5,color:"rgba(60, 179, 113, 0.8)"},{name:"GPT-3.5 Turbo",accuracy:26,inputCost:.5,outputCost:1.5,color:"rgba(106, 90, 205, 0.8)"},{name:"Command A",accuracy:25,inputCost:2,outputCost:8,color:"rgba(139, 69, 19, 0.8)"},{name:"Gemini 1.5 Flash",accuracy:22.5,inputCost:.075,outputCost:.3,color:"rgba(255, 105, 180, 0.8)"},{name:"GPT-4.1 Nano",accuracy:19.5,inputCost:.1,outputCost:.4,color:"rgba(70, 130, 180, 0.8)"},{name:"Llama 3.3 70B",accuracy:19.5,inputCost:.038,outputCost:.12,color:"rgba(128, 0, 128, 0.8)"}],e=r.map(e=>{const t=(e.inputCost*3+e.outputCost*1)/4;return{x:t,y:e.accuracy,label:e.name,color:e.color}});e.sort((e,t)=>t.y-e.y||e.x-t.x);const t=[];let m=-1,n=1/0;for(const s of e){if(t.length===0){t.push(s),n=s.x;continue}s.x<n&&(t.push(s),n=s.x)}t.sort((e,t)=>e.x-t.x);const s=e.map(e=>e.x),o=Math.min(...s),i=Math.max(...s),l=(o*i)**.5,a=e.map(e=>e.y),d=Math.min(...a),u=Math.max(...a),h={id:"paretoOverlay",beforeDatasetsDraw(e){const{ctx:t,chartArea:i}=e;if(!i)return;const{left:n,top:s,right:a,bottom:r}=i,o=t.createLinearGradient(n,s,a,r);o.addColorStop(0,"rgba(46, 125, 50, 0.2)"),o.addColorStop(1,"rgba(255, 255, 255, 0.06)"),t.save(),t.fillStyle=o,t.fillRect(n,s,a-n,r-s),t.restore()},afterDraw(e){const{ctx:t,chartArea:n}=e;if(!n)return;const{left:o,top:i,right:a,bottom:r}=n;t.save(),t.font="bold 12px sans-serif",t.fillStyle="rgba(46, 125, 50, 0.9)",t.textBaseline="top",t.fillText("Most favorable",o+8,i+8);const s="Least favorable",c=t.measureText(s).width;t.fillStyle="rgba(183, 28, 28, 0.9)",t.textBaseline="alphabetic",t.fillText(s,a-8-c,r-8),t.restore()}};new Chart(c,{type:"scatter",plugins:[ChartDataLabels,h],data:{datasets:[{label:"Models",data:e,backgroundColor:e.map(e=>e.color),pointRadius:6,pointHoverRadius:8,datalabels:{formatter:(e,t)=>{const n=t.dataset.data[t.dataIndex];return t.active?`${n.label}: (${n.x.toFixed(2)}, ${n.y}%)`:n.label},font:function(e){return{size:9,weight:e.active?"bold":"500"}},backgroundColor:function(e){return e.active?e.dataset.backgroundColor[e.dataIndex].replace("0.8","1"):null},borderColor:function(e){return e.active?"white":null},borderRadius:4,borderWidth:1,color:function(e){return e.active?"white":"#333"},padding:4,align:e=>e.dataset.data[e.dataIndex].x<l?"right":"left",offset:8,anchor:"center"}},{label:"Pareto Frontier",data:t,type:"line",borderColor:"rgba(255, 0, 0, 0.7)",borderWidth:2,fill:!1,showLine:!0,pointRadius:0,tension:.1,datalabels:{display:!1}}]},options:{responsive:!0,maintainAspectRatio:!1,plugins:{title:{display:!0,text:"ManaBench Pareto Frontier (Accuracy vs. Blended Cost)",font:{size:18}},tooltip:{enabled:!1},legend:{display:!1}},scales:{x:{type:"logarithmic",min:o*.9,max:i*1.1,title:{display:!0,text:"Blended Cost ($ per 1M Tokens, 3:1 Input/Output Ratio) - Log Scale"}},y:{min:Math.max(d-4,0),max:Math.min(u+4,100),title:{display:!0,text:"Accuracy (%)"}}}}})})</script><h2 id=benchmark-construction-methodology>Benchmark Construction Methodology<a hidden class=anchor aria-hidden=true href=#benchmark-construction-methodology>#</a></h2><p>The creation of ManaBench involved several stages, from curating source decks to generating challenging multiple-choice questions. The process is designed to be rigorous and reproducible, with a focus on capturing instances of human expert judgment as reflected in competitive deck choices.</p><h3 id=source-data--deck-curation>Source Data & Deck Curation<a hidden class=anchor aria-hidden=true href=#source-data--deck-curation>#</a></h3><p>The foundation of the benchmark is a large corpus of human-constructed MTG decklists scraped from <a href=https://www.mtgtop8.com/>MTGTop8</a> (a public database of MTG tournament results and decklists). The curation process involves:</p><ol><li><strong>Format Filtering:</strong> Decks are categorized by major constructed formats: Modern, Pioneer, Legacy, and Vintage. The benchmark currently focuses on these eternal formats. Standard is excluded due to its rotating nature, which does not align with the goal of measuring reasoning abilities and minimizing the impact of knowledge cutoff dates, thereby keeping the focus on reasoning skills rather than rapidly changing format knowledge.</li><li><strong>Strict Validation:</strong> Only decks containing exactly 60 main deck cards and 15 sideboard cards are considered. This ensures that they obey conventional deck-building wisdom and that the creator carefully considered the metagame when constructing the deck.</li><li><strong>Format Legality Check:</strong> Using the card database from <a href=https://mtgjson.com/>MTGJSON</a>, every card in a potential source deck is verified for legality within its designated format. This step is crucial for ensuring that the decks and subsequent questions are valid within the game&rsquo;s rules.</li><li><strong>Sampling:</strong> From the pool of validated decks, 25 decks are randomly sampled for each of the four target formats, resulting in a set of 100 unique, curated decklists for benchmark generation. These decks represent successful strategies designed and tested by human players in tournament play.</li></ol><h3 id=question-generation>Question Generation<a hidden class=anchor aria-hidden=true href=#question-generation>#</a></h3><p>For each of the 100 sampled human-constructed decks, two unique questions are generated. This involves selecting a &ldquo;golden&rdquo; card to remove and then using the Manamorphosis model to propose alternatives.</p><ol><li><p><strong>Golden Card Selection:</strong></p><ul><li>The &ldquo;golden&rdquo; card represents the correct answer for the deck completion task, reflecting a choice consistent with the original human-designed, tournament-sourced deck.</li><li>It is chosen by randomly selecting one <em>unique card name</em> present in the original 60-card main deck. For instance, if a deck contains four copies of &ldquo;Lightning Bolt,&rdquo; &ldquo;Lightning Bolt&rdquo; is one possible unique card name that could be selected as the golden card.</li><li>To ensure variety in the questions derived from a single deck, the two golden cards selected from the same source deck must be different card names.</li></ul></li><li><p><strong>Partial Deck Creation:</strong></p><ul><li>Once a golden card name is selected, one instance of this card is removed from the original 60-card main deck list. This creates the 59-card partial deck that will be presented to the LLM.</li><li>For example, if &ldquo;Island&rdquo; is chosen as the golden card from a deck containing 10 Islands, the partial deck will contain 9 Islands.</li></ul></li><li><p><strong>Generating Plausible Alternatives:</strong></p><ul><li>The five incorrect-but-plausible alternatives are generated using a Manamorphosis, a Transformer-based diffusion model custom-trained trained on a vast corpus of MTG decks. It learns to represent cards as high-dimensional embeddings and understands patterns of card co-occurrence and deck structure.</li><li>For benchmark generation, this model takes the 59-card partial deck and, through a reverse diffusion process conditioned on these known cards, predicts embeddings for the missing card. These embeddings are then mapped back to specific card names. This process is designed to generate alternatives that are contextually relevant yet distinct from the golden card.</li><li>For a detailed technical explanation of the diffusion model&rsquo;s architecture and training process, please refer to the <a href=https://github.com/JakeBoggs/Manamorphosis>Manamorphosis repository</a> and the <a href=https://boggs.tech/posts/manamorphosis>accompanying blog post</a>.</li><li>This generation process is repeated to obtain 5 unique card names that are different from the chosen golden card and from each other, serving as challenging distractors for the LLM.</li></ul></li><li><p><strong>Question Structure (JSON):</strong>
Each generated question is stored in a JSON object with the following structure:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#e6db74>&#34;question_001&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;deck&#34;</span>: [<span style=color:#e6db74>&#34;Card Name 1&#34;</span>, <span style=color:#e6db74>&#34;Card Name 2&#34;</span>, <span style=color:#e6db74>&#34;Card Name 3&#34;</span>, <span style=color:#e6db74>&#34;... list of 59 card names ...&#34;</span>],
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;golden&#34;</span>: <span style=color:#e6db74>&#34;Actual Missing Card Name&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;alternatives&#34;</span>: [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Alternative Card Name A&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Alternative Card Name B&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Alternative Card Name C&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Alternative Card Name D&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Alternative Card Name E&#34;</span>
</span></span><span style=display:flex><span>  ],
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;format&#34;</span>: <span style=color:#e6db74>&#34;modern&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></li></ol><h2 id=evaluation-protocol>Evaluation Protocol<a hidden class=anchor aria-hidden=true href=#evaluation-protocol>#</a></h2><h3 id=prompting-strategy>Prompting Strategy<a hidden class=anchor aria-hidden=true href=#prompting-strategy>#</a></h3><p>Careful prompt engineering is employed to provide the LLM with sufficient context to make a reasoned judgment, minimizing the need for memorized MTG card knowledge and emphasizing analytical skill:</p><ol><li><strong>Role Assigment:</strong> The LLM is instructed: <code>"You are an expert Magic: The Gathering player."</code></li><li><strong>Task Definition:</strong> The prompt explains that a decklist from a specific format is missing one card and asked to choose the answer that best completes the deck.</li><li><strong>Decklist Presentation:</strong> The 59-card partial deck is provided. Each card entry includes:<ul><li>Its count in the partial deck.</li><li>Its full name.</li><li>Its detailed rules text, including type line, mana cost, power/toughness (for creatures), loyalty (for planeswalkers), and Oracle text. This information is sourced from MTGJSON. <em>The explicit provision of full rules text for all cards in the deck and choices is a key design element, intended to reduce the task&rsquo;s reliance on the LLM&rsquo;s pre-existing knowledge of specific cards and instead focus on its ability to reason based on the provided information.</em></li><li><em>Example Card Presentation in Prompt:</em>
<code>2x Snapcaster Mage - Creature - Human Wizard - Cost: {1}{U} - P/T: 2/1 - Rules: Flash. When Snapcaster Mage enters the battlefield, target instant or sorcery card in your graveyard gains flashback until end of turn. The flashback cost is equal to its mana cost.</code></li></ul></li><li><strong>Multiple-Choice Options:</strong><ul><li>The six choices (the golden card and the five alternatives) are presented, shuffled randomly to avoid positional bias, and labeled A through F.</li><li>Each choice is also presented with its full name and detailed rules text (sourced from MTGJSON, same as deck cards).</li><li><em>Example Choice Presentation in Prompt:</em>
<code>A) Brainstorm - Instant - Cost: {U} - Rules: Draw three cards, then put two cards from your hand on top of your library in any order.</code></li></ul></li></ol><h3 id=answer-format-and-extraction>Answer Format and Extraction<a hidden class=anchor aria-hidden=true href=#answer-format-and-extraction>#</a></h3><p>To standardize evaluation, LLMs are instructed to output their final answer in a specific format:
<code>"Respond with only the letter of your choice in the format: ANSWER: [LETTER]"</code></p><p>The evaluation script parses this response using the following logic:</p><ol><li><strong>Primary Method:</strong> Looks for the &ldquo;ANSWER: [LETTER]&rdquo; pattern (case-insensitive for &ldquo;ANSWER:&rdquo;, extracts the letter).</li><li><strong>Fallback 1 (Single Letter):</strong> If the primary method fails, it checks if the entire response is a single letter from A to F (e.g., a response like &ldquo;C&rdquo;).</li><li><strong>Fallback 2 (Boxed Letter):</strong> If both above fail, it looks for the pattern <code>$\boxed{LETTER}$</code> (e.g., &ldquo;The answer is $\boxed{A}$&rdquo;) using a regular expression.</li></ol><h3 id=metrics>Metrics<a hidden class=anchor aria-hidden=true href=#metrics>#</a></h3><ul><li><strong>Accuracy:</strong> The primary metric is the percentage of questions the LLM answers correctly by selecting the golden card.</li></ul><h2 id=why-manabench-is-a-strong-benchmark-for-reasoning>Why ManaBench is a Strong Benchmark for Reasoning<a hidden class=anchor aria-hidden=true href=#why-manabench-is-a-strong-benchmark-for-reasoning>#</a></h2><p>The preliminary results and the design of ManaBench demonstrate several key strengths for evaluating an LLM&rsquo;s reasoning capabilities:</p><ul><li><p><strong>Measures Alignment with Human Expert Judgment:</strong> The &ldquo;golden&rdquo; answers are derived from decks designed by human players and sourced from MTGTop8, a database of tournament decklists. Success on this benchmark therefore indicates an LLM&rsquo;s ability to make choices that align with established human expertise.</p></li><li><p><strong>Clear Performance Differentiation:</strong> Compared to other popular evaluation metrics like LMArena ELO ratings, ManaBench has a significantly stronger ability to differentiate between models. While a general positive correlation is observed (as indicated by the trendline in the chart below), ManaBench provides a much wider relative spread in scores. For the models included in the comparison, ManaBench accuracies range from 19.5% to 67% (a spread of 47.5 percentage points, representing an increase of approximately 244% from the minimum observed score to the maximum). In contrast, their LMArena ELO scores range from 1257 to 1481 (a spread of 224 ELO points, representing an increase of approximately 17.8% from the minimum observed ELO score to the maximum). The significantly larger proportional range in ManaBench allows for a more granular distinction between models.</p></li></ul><div style="width:90%;margin:20px auto;min-height:600px"><canvas id=correlationChart></canvas></div><script>document.addEventListener("DOMContentLoaded",()=>{const d=document.getElementById("correlationChart").getContext("2d"),h=["GPT-5","o3 (low)","Gemini 2.5 Pro 03-25","Gemini 2.5 Flash","Claude 3.7 Sonnet (no thinking)","o4 Mini (low)","Deepseek R1","Grok 3","GPT-4o 08-06","GPT-4 Turbo 11-06","Qwen3 235B A22B (thinking)","Grok 3 Mini (low)","Gemini 2.0 Flash","Mistral 3 Medium","Llama 4 Maverick","Command A","GPT-4.1 Nano","Llama 3.3 70B","Gemini 1.5 Flash","Deepseek V3 03-24","Gemini 2.5 Pro 06-05","Deepseek R1 05-28","Kimi K2","Gemini 1.5 Pro","GPT-3.5 Turbo"],s=[67,63,53,50,49.5,45,43.5,41.5,41,39.5,37,37,35,31.5,26.5,25,19.5,19.5,22.5,37.5,57.5,43,41,27.5,26],u={"GPT-5":1481,"Gemini 2.5 Pro 03-25":1448,"Gemini 2.5 Flash":1416,"o3 (low)":1411,"Claude 3.7 Sonnet (no thinking)":1291,"o4 Mini (low)":1351,"Deepseek R1":1359,"Grok 3":1409,"GPT-4o 08-06":1265,"Qwen3 235B A22B (thinking)":1342,"Gemini 2.0 Flash":1355,"Llama 4 Maverick":1269,"Command A":1346,"GPT-4.1 Nano":1270,"Llama 3.3 70B":1257,"Gemini 1.5 Flash":1271,"Deepseek V3 03-24":1372,"Gemini 2.5 Pro 06-05":1470,"Kimi K2":1420},t=["rgba(75, 192, 192, 0.8)","rgba(0, 200, 255, 0.8)","rgba(50, 205, 50, 0.8)","rgba(54, 162, 235, 0.8)","rgba(255, 206, 86, 0.8)","rgba(255, 99, 132, 0.8)","rgba(255, 140, 0, 0.8)","rgba(153, 102, 255, 0.8)","rgba(255, 159, 64, 0.8)","rgba(101, 143, 74, 0.8)","rgba(210, 105, 30, 0.8)","rgba(0, 128, 128, 0.8)","rgba(165, 42, 42, 0.8)","rgba(139, 69, 19, 0.8)","rgba(70, 130, 180, 0.8)","rgba(128, 0, 128, 0.8)","rgba(255, 105, 180, 0.8)","rgba(128, 128, 0, 0.8)","rgba(0, 220, 220, 0.8)","rgba(255, 99, 100, 0.8)","rgba(0, 100, 0, 0.8)"],i=t.map(e=>e.replace("0.8","1")),o=[],a=[],r=[],e=[],c=[];h.forEach((n,l)=>{const h=u[n],d=s[l];if(h!==0[0]&&d!==0[0]&&d!==null){const s={x:h,y:d};o.push(s),e.push(s),c.push(n),a.push(t[l%t.length]||"rgba(150, 150, 150, 0.8)"),r.push(i[l%i.length]||"rgba(150, 150, 150, 1)")}});let n={};if(e.length>=2){let t=0,s=0,a=0,r=0;const o=e.length;e.forEach(e=>{t+=e.x,s+=e.y,a+=e.x*e.y,r+=e.x*e.x});const i=(o*a-t*s)/(o*r-t*t),c=(s-i*t)/o,l=Math.min(...e.map(e=>e.x)),d=Math.max(...e.map(e=>e.x));n={type:"line",label:"Trendline",data:[{x:l,y:i*l+c},{x:d,y:i*d+c}],borderColor:"rgba(255, 0, 0, 0.7)",borderWidth:2,fill:!1,pointRadius:0,tension:0,datalabels:{display:!1}}}const l=[{label:"Models",type:"scatter",data:o,backgroundColor:a,borderColor:r,pointRadius:8,pointHoverRadius:10,datalabels:{formatter:(e,t)=>{const n=c[t.dataIndex],s=t.dataset.data[t.dataIndex];return t.active?`${n}: (ELO: ${s.x}, ManaBench: ${s.y}%)`:n},font:function(e){return{size:9,weight:e.active?"bold":"500"}},backgroundColor:function(e){return e.active?e.dataset.backgroundColor[e.dataIndex].replace("0.8","1"):null},borderColor:function(e){return e.active?"white":null},borderRadius:4,borderWidth:1,color:function(e){return e.active?"white":"#333"},padding:4,align:"top",offset:8,anchor:"center"}}];Object.keys(n).length>0&&l.push(n),new Chart(d,{type:"scatter",plugins:[ChartDataLabels],data:{datasets:l},options:{responsive:!0,maintainAspectRatio:!1,plugins:{legend:{display:!1},title:{display:!0,text:"ManaBench Accuracy vs. LMArena ELO",font:{size:18}},tooltip:{enabled:!1}},scales:{x:{type:"linear",position:"bottom",title:{display:!0,text:"LMArena ELO (May 2025)"}},y:{min:Math.max(Math.min(...s)-5,0),max:Math.min(Math.max(...s)+5,100),title:{display:!0,text:"ManaBench Accuracy (%)"}}}}})})</script><ul><li><p><strong>Challenge for Frontier Models:</strong> GPT-5 now reaches 67%, just 1 point shy of the 68% human baseline, with o3 (high) at 65% and o3 (low) at 63%. Only GPT-5 and o3 exceed 60%, indicating the benchmark remains unsaturated and continues to present a significant challenge for frontier models.</p></li><li><p><strong>Test of Generalization vs. Benchmark Overfitting:</strong> The complexity of MTG deck construction, the private nature of the benchmark questions, and the fact that MTG strategy is unlikely to be a direct optimization target for most AI labs, collectively make ManaBench a strong test of generalized reasoning. Performance on this benchmark may reveal whether models are truly capable of applying reasoning to novel, complex systems, or if their high scores on common academic benchmarks (like MMLU or MATH) are partly due to overfitting. For example, a model series like Llama 4, which demonstrated strong performance on many standard benchmarks, gave a much weaker showing here. This aligns with the experiences of many users who reported that Llama 4 struggled with real tasks and underperformed expectations.</p></li><li><p><strong>Cost-Effectiveness and Efficiency:</strong> With 200 questions, the benchmark is relatively small compared to some larger evaluation suites. This allows for more rapid and cost-effective evaluation cycles, making it feasible to test a wider array of models or fine-tuned variants without incurring prohibitive API costs or excessive computation time, while still providing a strong differentiating signal.</p></li></ul><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>The initial results demonstrate ManaBench&rsquo;s potential as a benchmark, with even frontier models finding the task challenging. To maintain the integrity and long-term utility of the benchmark, the specific questions are <strong>not being publicly released at this time.</strong> This is to prevent them from being inadvertently included in the training data of future LLMs, which would compromise its validity as an unseen test set.</p><p>If you would me to add other models to the leaderboard or just think it&rsquo;s a cool project, consider checking out my other work or following me on <a href=https://x.com/JakeABoggs>Twitter</a></p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://boggs.tech/>Jake Boggs</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>