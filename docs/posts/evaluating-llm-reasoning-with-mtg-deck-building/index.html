<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Evaluating Reasoning in LLMs Through MTG Deck Building | Jake Boggs</title>
<meta name=keywords content><meta name=description content="Update (2025-06-06): Deepseek R1 05-28 and Gemini 2.5 Pro 06-05 added. Update (2025-05-22): Claude Sonnet 4 and Opus 4 added. Update (2025-05-14): Human Baseline, Gemini 2.5 Pro 03-25, Gemini 1.5 Flash, Deepseek V3 03-24, Qwen3 30B 3A added. Introduction Evaluating the advanced reasoning capabilities of Large Language Models (LLMs) requires specialized benchmarks that move beyond surface-level NLP tasks. Building upon my obsession applying AI models to my favorite card game, I&rsquo;ve created ManaBench, a benchmark designed to probe an LLM&rsquo;s capacity for reasoning using the collectible card game Magic: The Gathering (MTG) as a proxy."><meta name=author content="Jake Boggs"><link rel=canonical href=https://boggs.tech/posts/evaluating-llm-reasoning-with-mtg-deck-building/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://boggs.tech/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://boggs.tech/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://boggs.tech/favicon-32x32.png><link rel=apple-touch-icon href=https://boggs.tech/apple-touch-icon.png><link rel=mask-icon href=https://boggs.tech/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://boggs.tech/posts/evaluating-llm-reasoning-with-mtg-deck-building/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Evaluating Reasoning in LLMs Through MTG Deck Building"><meta property="og:description" content="Update (2025-06-06): Deepseek R1 05-28 and Gemini 2.5 Pro 06-05 added. Update (2025-05-22): Claude Sonnet 4 and Opus 4 added. Update (2025-05-14): Human Baseline, Gemini 2.5 Pro 03-25, Gemini 1.5 Flash, Deepseek V3 03-24, Qwen3 30B 3A added. Introduction Evaluating the advanced reasoning capabilities of Large Language Models (LLMs) requires specialized benchmarks that move beyond surface-level NLP tasks. Building upon my obsession applying AI models to my favorite card game, I&rsquo;ve created ManaBench, a benchmark designed to probe an LLM&rsquo;s capacity for reasoning using the collectible card game Magic: The Gathering (MTG) as a proxy."><meta property="og:type" content="article"><meta property="og:url" content="https://boggs.tech/posts/evaluating-llm-reasoning-with-mtg-deck-building/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-09T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-09T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Evaluating Reasoning in LLMs Through MTG Deck Building"><meta name=twitter:description content="Update (2025-06-06): Deepseek R1 05-28 and Gemini 2.5 Pro 06-05 added. Update (2025-05-22): Claude Sonnet 4 and Opus 4 added. Update (2025-05-14): Human Baseline, Gemini 2.5 Pro 03-25, Gemini 1.5 Flash, Deepseek V3 03-24, Qwen3 30B 3A added. Introduction Evaluating the advanced reasoning capabilities of Large Language Models (LLMs) requires specialized benchmarks that move beyond surface-level NLP tasks. Building upon my obsession applying AI models to my favorite card game, I&rsquo;ve created ManaBench, a benchmark designed to probe an LLM&rsquo;s capacity for reasoning using the collectible card game Magic: The Gathering (MTG) as a proxy."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://boggs.tech/posts/"},{"@type":"ListItem","position":2,"name":"Evaluating Reasoning in LLMs Through MTG Deck Building","item":"https://boggs.tech/posts/evaluating-llm-reasoning-with-mtg-deck-building/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Evaluating Reasoning in LLMs Through MTG Deck Building","name":"Evaluating Reasoning in LLMs Through MTG Deck Building","description":"Update (2025-06-06): Deepseek R1 05-28 and Gemini 2.5 Pro 06-05 added. Update (2025-05-22): Claude Sonnet 4 and Opus 4 added. Update (2025-05-14): Human Baseline, Gemini 2.5 Pro 03-25, Gemini 1.5 Flash, Deepseek V3 03-24, Qwen3 30B 3A added. Introduction Evaluating the advanced reasoning capabilities of Large Language Models (LLMs) requires specialized benchmarks that move beyond surface-level NLP tasks. Building upon my obsession applying AI models to my favorite card game, I\u0026rsquo;ve created ManaBench, a benchmark designed to probe an LLM\u0026rsquo;s capacity for reasoning using the collectible card game Magic: The Gathering (MTG) as a proxy.","keywords":[],"articleBody":" Update (2025-06-06): Deepseek R1 05-28 and Gemini 2.5 Pro 06-05 added. Update (2025-05-22): Claude Sonnet 4 and Opus 4 added. Update (2025-05-14): Human Baseline, Gemini 2.5 Pro 03-25, Gemini 1.5 Flash, Deepseek V3 03-24, Qwen3 30B 3A added. Introduction Evaluating the advanced reasoning capabilities of Large Language Models (LLMs) requires specialized benchmarks that move beyond surface-level NLP tasks. Building upon my obsession applying AI models to my favorite card game, I’ve created ManaBench, a benchmark designed to probe an LLM’s capacity for reasoning using the collectible card game Magic: The Gathering (MTG) as a proxy. MTG, with its intricate interactions and deep strategic layer, serves as an ideal micro-world to test an LLM’s ability to process extensive contextual information, identify sophisticated patterns, and make judgments that align with expert human strategic choices. This post provides a technical overview of the benchmark’s construction and the methodology used for evaluating LLMs, not as a test of MTG-specific card knowledge, but as a measure of their broader reasoning and problem-solving faculties when faced with a constrained, strategic challenge.\nThe Deck Completion Task The core task in ManaBench is as follows: Given a 59-card main deck from a specific MTG constructed format (e.g., Modern, Legacy) - a deck originally constructed by a human player and sourced from tournament results - the LLM must choose the most suitable 60th card from a list of six options. One of these options is the “golden” card - the card that was originally in that slot in the human-designed decklist - while the other five are plausible alternatives generated by Manamorphosis, a diffusion model I trained specifically for completing MTG decks.\nThis task is non-trivial for an LLM because it demands more than just factual recall about individual cards; it requires:\nStrategic Coherence Evaluation: The chosen card must align with the deck’s overall strategy (e.g., aggro, control, combo), a judgment that requires understanding the interplay of the existing 59 cards. System-Wide Optimization: The card should fit the deck’s mana curve and resource development plan, demonstrating an understanding of resource management within the game system. Complex Interaction Analysis (Card Synergies): Effective MTG play relies heavily on card interactions. The LLM needs to identify cards that synergize well with the existing 59 cards, showcasing an ability to reason about emergent properties of combined elements. Contextual Awareness (Format Knowledge): Different MTG formats have distinct card pools and power levels. The choice must be legal and relevant within the specified format, testing the LLM’s ability to operate within defined constraints. Discernment Against Plausible Alternatives: The alternatives are not entire random but are generated by a model trained for the task, making them potentially attractive but incorrect choices. Successfully identifying the golden card requires fine-grained distinction based on strategic fit. Benchmark Leaderboard The human baseline has an admittedly small sample size (just myself, it’s difficult to find skilled MTG players who want to sit through a 200 question test), but still provides a valuable reference. As the creator of the benchmark and a player of the game for 10+ years, I scored 68% agreement with the other human deck builders. This could be a skill issue on my end, but I think it is more likely due to the somewhat subjective nature of question, along with the presentation format. To provide a fair comparison, I made a script that presents that questions to me exactly as they are shown to the LLMs, but I think I could do better using a deck editor. Despite my less than perfect score, I still beat most of the models by a wide margin. Among LLMs, o3 (low) comes closest with 63% accuracy, followed by Gemini 2.5 Pro 06-05 at 57.5%, and the older Gemini 2.5 Pro 03-25 at 53%. This aligns with qualitative assessments and other benchmarks where these models often excel in real-world tasks. Their success here suggests that ManaBench is effectively capturing a similar type of reasoning aptitude.\nThe results also highlight a discernible gap in performance between the leading American models (o3, Gemini 2.5 Pro, Claude) and prominent Chinese models like Deepseek R1 (43.5%) and Qwen3 235B A22B (37%). While these models are undoubtedly powerful, their performance on ManaBench suggests that their reasoning capabilities may not be as developed as some of their US counterparts. This observation underscores the utility of specialized benchmarks like ManaBench in revealing subtle but significant differences in model capabilities that might be obscured by broader, more generalized benchmarks.\nBenchmark Construction Methodology The creation of ManaBench involves several stages, from curating source decks to generating challenging multiple-choice questions. The process is designed to be rigorous and reproducible, with a focus on capturing instances of human expert judgment as reflected in competitive deck choices.\nSource Data \u0026 Deck Curation The foundation of the benchmark is a large corpus of human-constructed MTG decklists scraped from MTGTop8 (a public database of MTG tournament results and decklists). The curation process involves:\nFormat Filtering: Decks are categorized by major constructed formats: Modern, Pioneer, Legacy, and Vintage. The benchmark currently focuses on these eternal formats. Standard is excluded due to its rotating nature, which does not align with the goal of measuring reasoning abilities and minimizing the impact of knowledge cutoff dates, thereby keeping the focus on reasoning skills rather than rapidly changing format knowledge. Strict Validation: Only decks containing exactly 60 main deck cards and 15 sideboard cards are considered. This ensures that they obey conventional deck-building wisdom and that the creator carefully considered the metagame when constructing the deck. Format Legality Check: Using the card database from MTGJSON, every card in a potential source deck is verified for legality within its designated format. This step is crucial for ensuring that the decks and subsequent questions are valid within the game’s rules. Sampling: From the pool of validated decks, 25 decks are randomly sampled for each of the four target formats, resulting in a set of 100 unique, curated decklists for benchmark generation. These decks represent successful strategies designed and tested by human players in tournament play. Question Generation For each of the 100 sampled human-constructed decks, two unique questions are generated. This involves selecting a “golden” card to remove and then using the Manamorphosis model to propose alternatives.\nGolden Card Selection:\nThe “golden” card represents the correct answer for the deck completion task, reflecting a choice consistent with the original human-designed, tournament-sourced deck. It is chosen by randomly selecting one unique card name present in the original 60-card main deck. For instance, if a deck contains four copies of “Lightning Bolt,” “Lightning Bolt” is one possible unique card name that could be selected as the golden card. To ensure variety in the questions derived from a single deck, the two golden cards selected from the same source deck must be different card names. Partial Deck Creation:\nOnce a golden card name is selected, one instance of this card is removed from the original 60-card main deck list. This creates the 59-card partial deck that will be presented to the LLM. For example, if “Island” is chosen as the golden card from a deck containing 10 Islands, the partial deck will contain 9 Islands. Generating Plausible Alternatives:\nThe five incorrect-but-plausible alternatives are generated using a Manamorphosis, a Transformer-based diffusion model custom-trained trained on a vast corpus of MTG decks. It learns to represent cards as high-dimensional embeddings and understands patterns of card co-occurrence and deck structure. For benchmark generation, this model takes the 59-card partial deck and, through a reverse diffusion process conditioned on these known cards, predicts embeddings for the missing card. These embeddings are then mapped back to specific card names. This process is designed to generate alternatives that are contextually relevant yet distinct from the golden card. For a detailed technical explanation of the diffusion model’s architecture and training process, please refer to the Manamorphosis repository and the accompanying blog post. This generation process is repeated to obtain 5 unique card names that are different from the chosen golden card and from each other, serving as challenging distractors for the LLM. Question Structure (JSON): Each generated question is stored in a JSON object with the following structure:\n{ \"id\": \"question_001\", \"deck\": [\"Card Name 1\", \"Card Name 2\", \"Card Name 3\", \"... list of 59 card names ...\"], \"golden\": \"Actual Missing Card Name\", \"alternatives\": [ \"Alternative Card Name A\", \"Alternative Card Name B\", \"Alternative Card Name C\", \"Alternative Card Name D\", \"Alternative Card Name E\" ], \"format\": \"modern\" } Evaluation Protocol Prompting Strategy Careful prompt engineering is employed to provide the LLM with sufficient context to make a reasoned judgment, minimizing the need for memorized MTG card knowledge and emphasizing analytical skill:\nRole Assigment: The LLM is instructed: \"You are an expert Magic: The Gathering player.\" Task Definition: The prompt explains that a decklist from a specific format is missing one card and asked to choose the answer that best completes the deck. Decklist Presentation: The 59-card partial deck is provided. Each card entry includes: Its count in the partial deck. Its full name. Its detailed rules text, including type line, mana cost, power/toughness (for creatures), loyalty (for planeswalkers), and Oracle text. This information is sourced from MTGJSON. The explicit provision of full rules text for all cards in the deck and choices is a key design element, intended to reduce the task’s reliance on the LLM’s pre-existing knowledge of specific cards and instead focus on its ability to reason based on the provided information. Example Card Presentation in Prompt: 2x Snapcaster Mage - Creature - Human Wizard - Cost: {1}{U} - P/T: 2/1 - Rules: Flash. When Snapcaster Mage enters the battlefield, target instant or sorcery card in your graveyard gains flashback until end of turn. The flashback cost is equal to its mana cost. Multiple-Choice Options: The six choices (the golden card and the five alternatives) are presented, shuffled randomly to avoid positional bias, and labeled A through F. Each choice is also presented with its full name and detailed rules text (sourced from MTGJSON, same as deck cards). Example Choice Presentation in Prompt: A) Brainstorm - Instant - Cost: {U} - Rules: Draw three cards, then put two cards from your hand on top of your library in any order. Answer Format and Extraction To standardize evaluation, LLMs are instructed to output their final answer in a specific format: \"Respond with only the letter of your choice in the format: ANSWER: [LETTER]\"\nThe evaluation script parses this response using the following logic:\nPrimary Method: Looks for the “ANSWER: [LETTER]” pattern (case-insensitive for “ANSWER:”, extracts the letter). Fallback 1 (Single Letter): If the primary method fails, it checks if the entire response is a single letter from A to F (e.g., a response like “C”). Fallback 2 (Boxed Letter): If both above fail, it looks for the pattern $\\boxed{LETTER}$ (e.g., “The answer is $\\boxed{A}$”) using a regular expression. Metrics Accuracy: The primary metric is the percentage of questions the LLM answers correctly by selecting the golden card. Why ManaBench is a Strong Benchmark for Reasoning The preliminary results and the design of ManaBench highlight several key strengths for evaluating an LLM’s reasoning capabilities:\nMeasures Alignment with Human Expert Judgment: The “golden” answers are derived from decks designed by human players and sourced from MTGTop8, a database of tournament decklists. Success on this benchmark therefore indicates an LLM’s ability to make choices that align with established human expertise and strategic consensus.\nClear Performance Differentiation: When comparing ManaBench scores to other established LLM evaluation metrics like LMArena ELO ratings, ManaBench demonstrates a significantly stronger ability to differentiate between models. While a general positive correlation is observed (as indicated by the trendline in the chart below), ManaBench provides a much wider relative spread in scores. For the models included in the comparison, ManaBench accuracies range from 19.5% to 63% (a spread of 43.5 percentage points, representing an increase of approximately 223% from the minimum observed score to the maximum). In contrast, their LMArena ELO scores range from 1257 to 1470 (a spread of 213 ELO points, representing an increase of approximately 17% from the minimum observed ELO score to the maximum). The significantly larger proportional range in ManaBench allows for a more granular distinction between models.\nChallenge for Frontier Models: With the exception of o3, most of the models are far from matching human performance, let alone exceeding it. There is a 5.5 point gap between the top models (o3 at 63% and Gemini 2.5 Pro 06-05 at 57.5%) and no model reaches the human baseline, showing that the benchmark remains unsaturated and clearly presents a significant challenge for frontier models.\nTest of Generalization vs. Benchmark Overfitting: The complexity of MTG deck construction, the private nature of the benchmark questions, and the fact that MTG strategy is unlikely to be a direct optimization target for most LLM labs, collectively make ManaBench a strong test of generalized reasoning. Performance on this benchmark may reveal whether models are truly capable of applying reasoning to novel, complex systems, or if their high scores on common academic benchmarks (like MMLU or MATH) are partly due to overfitting or memorization of those specific test distributions. For example, a model series like Llama 4, which demonstrated strong performance on many standard benchmarks, gave a much weaker showing here, highlighting the value of diverse, specialized evaluations like ManaBench in assessing true generalization. This aligns with the experiences of many users who reported that Llama 4 struggled with real tasks and underperformed expectations.\nCost-Effectiveness and Efficiency: With 200 questions, the benchmark is relatively concise compared to some larger evaluation suites. This allows for more rapid and cost-effective evaluation cycles, making it feasible to test a wider array of models or fine-tuned variants without incurring prohibitive API costs or excessive computation time, while still providing strong differentiating signals as seen in the results.\nBenchmark Integrity To maintain the integrity and long-term utility of ManaBench as an evaluation tool, the specific benchmark questions and code are not being publicly released at this time. This measure is taken to prevent the benchmark from being inadvertently included in the training data of future LLMs, which would compromise its validity as an unseen test set. If you are a researcher and would like private access, please reach out.\nConclusion ManaBench offers a novel approach to evaluating the sophisticated reasoning capabilities of Large Language Models by leveraging the strategic depth of Magic: The Gathering. The benchmark’s core “deck completion task” – choosing the optimal 60th card for a 59-card deck - demands an understanding of strategic coherence, system-wide optimization, and complex interactions.\nThe initial results demonstrate ManaBench’s potential as a strong differentiator of LLM reasoning abilities, with even frontier models finding the task challenging. If you would me to add other models to the leaderboard or just think it’s a cool project, consider checking out my other related work or following me on Twitter\n","wordCount":"2477","inLanguage":"en","datePublished":"2025-05-09T00:00:00Z","dateModified":"2025-05-09T00:00:00Z","author":{"@type":"Person","name":"Jake Boggs"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://boggs.tech/posts/evaluating-llm-reasoning-with-mtg-deck-building/"},"publisher":{"@type":"Organization","name":"Jake Boggs","logo":{"@type":"ImageObject","url":"https://boggs.tech/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://boggs.tech/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://boggs.tech/startups title=Startups><span>Startups</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Evaluating Reasoning in LLMs Through MTG Deck Building</h1><div class=post-meta><span title='2025-05-09 00:00:00 +0000 UTC'>May 9, 2025</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;2477 words&nbsp;·&nbsp;Jake Boggs</div></header><div class=post-content><div style="background:#e0f7fa;color:#006064;padding:12px 18px;border-radius:6px;margin-bottom:18px;font-size:1.08em;font-weight:500"><strong>Update (2025-06-06):</strong> Deepseek R1 05-28 and Gemini 2.5 Pro 06-05 added.</div><div style="background:#e0f7fa;color:#006064;padding:12px 18px;border-radius:6px;margin-bottom:18px;font-size:1.08em;font-weight:500"><strong>Update (2025-05-22):</strong> Claude Sonnet 4 and Opus 4 added.</div><div style="background:#e0f7fa;color:#006064;padding:12px 18px;border-radius:6px;margin-bottom:18px;font-size:1.08em;font-weight:500"><strong>Update (2025-05-14):</strong> Human Baseline, Gemini 2.5 Pro 03-25, Gemini 1.5 Flash, Deepseek V3 03-24, Qwen3 30B 3A added.</div><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Evaluating the advanced reasoning capabilities of Large Language Models (LLMs) requires specialized benchmarks that move beyond surface-level NLP tasks. Building upon my obsession applying AI models to my favorite card game, I&rsquo;ve created ManaBench, a benchmark designed to probe an LLM&rsquo;s capacity for reasoning using the collectible card game Magic: The Gathering (MTG) as a proxy. MTG, with its intricate interactions and deep strategic layer, serves as an ideal micro-world to test an LLM&rsquo;s ability to process extensive contextual information, identify sophisticated patterns, and make judgments that align with expert human strategic choices. This post provides a technical overview of the benchmark&rsquo;s construction and the methodology used for evaluating LLMs, not as a test of MTG-specific card knowledge, but as a measure of their broader reasoning and problem-solving faculties when faced with a constrained, strategic challenge.</p><h2 id=the-deck-completion-task>The Deck Completion Task<a hidden class=anchor aria-hidden=true href=#the-deck-completion-task>#</a></h2><p>The core task in ManaBench is as follows: Given a 59-card main deck from a specific MTG constructed format (e.g., Modern, Legacy) - a deck originally constructed by a human player and sourced from tournament results - the LLM must choose the most suitable 60th card from a list of six options. One of these options is the &ldquo;golden&rdquo; card - the card that was originally in that slot in the human-designed decklist - while the other five are plausible alternatives generated by <a href=https://github.com/JakeBoggs/Manamorphosis>Manamorphosis</a>, a diffusion model I trained specifically for completing MTG decks.</p><p>This task is non-trivial for an LLM because it demands more than just factual recall about individual cards; it requires:</p><ul><li><strong>Strategic Coherence Evaluation:</strong> The chosen card must align with the deck&rsquo;s overall strategy (e.g., aggro, control, combo), a judgment that requires understanding the interplay of the existing 59 cards.</li><li><strong>System-Wide Optimization:</strong> The card should fit the deck&rsquo;s mana curve and resource development plan, demonstrating an understanding of resource management within the game system.</li><li><strong>Complex Interaction Analysis (Card Synergies):</strong> Effective MTG play relies heavily on card interactions. The LLM needs to identify cards that synergize well with the existing 59 cards, showcasing an ability to reason about emergent properties of combined elements.</li><li><strong>Contextual Awareness (Format Knowledge):</strong> Different MTG formats have distinct card pools and power levels. The choice must be legal and relevant within the specified format, testing the LLM&rsquo;s ability to operate within defined constraints.</li><li><strong>Discernment Against Plausible Alternatives:</strong> The alternatives are not entire random but are generated by a model trained for the task, making them potentially attractive but incorrect choices. Successfully identifying the golden card requires fine-grained distinction based on strategic fit.</li></ul><h2 id=benchmark-leaderboard>Benchmark Leaderboard<a hidden class=anchor aria-hidden=true href=#benchmark-leaderboard>#</a></h2><div style="width:90%;margin:20px auto"><canvas id=leaderboardChart></canvas></div><script src=https://cdn.jsdelivr.net/npm/chart.js@3.9.1/dist/chart.min.js></script><script>document.addEventListener("DOMContentLoaded",()=>{const e=document.getElementById("leaderboardChart"),s=e.getContext("2d"),t=["Human Baseline","o3 (low)","Gemini 2.5 Pro 06-05","Gemini 2.5 Pro 03-25","Claude 3.7 Sonnet (no thinking)","o4 Mini (low)","Deepseek R1","Deepseek R1 05-28","GPT-4o 08-06","Claude Opus 4 (no thinking)","Claude Sonnet 4 (no thinking)","Deepseek V3 03-24","Qwen3 235B A22B (thinking)","Grok 3 Mini (low)","Gemini 2.0 Flash","Mistral 3 Medium","Qwen3 30B A3B (thinking)","Llama 4 Maverick","Gemini 1.5 Flash","GPT-4.1 Nano","Llama 3.3 70B","Random Guessing"],o=t.length,i=15,a=100,r=o*i+a;e.style.height=r+"px";const c=[68,63,57.5,53,49.5,45,43.5,43,41,40.5,38,37.5,37,37,35,31.5,28.5,26.5,22.5,19.5,19.5,16.67],n=["rgba(34, 139, 34, 0.8)","rgba(75, 192, 192, 0.8)","rgba(0, 220, 220, 0.8)","rgba(0, 200, 255, 0.8)","rgba(54, 162, 235, 0.8)","rgba(255, 206, 86, 0.8)","rgba(255, 99, 132, 0.8)","rgba(255, 99, 100, 0.8)","rgba(153, 102, 255, 0.8)","rgba(205, 133, 63, 0.8)","rgba(188, 143, 143, 0.8)","rgba(128, 128, 0, 0.8)","rgba(255, 159, 64, 0.8)","rgba(101, 143, 74, 0.8)","rgba(210, 105, 30, 0.8)","rgba(0, 128, 128, 0.8)","rgba(0, 0, 205, 0.8)","rgba(165, 42, 42, 0.8)","rgba(255, 105, 180, 0.8)","rgba(70, 130, 180, 0.8)","rgba(128, 0, 128, 0.8)","rgba(150, 150, 150, 0.8)"],l=n.map(e=>e.replace("0.8","1"));new Chart(s,{type:"bar",data:{labels:t,datasets:[{label:"Accuracy",data:c,backgroundColor:n,borderColor:l,borderWidth:1}]},options:{indexAxis:"y",responsive:!0,maintainAspectRatio:!1,plugins:{legend:{display:!1},title:{display:!0,text:"ManaBench LLM Leaderboard (Accuracy %)",font:{size:18}},tooltip:{callbacks:{label:function(e){let t=e.dataset.label||"";return t&&(t+=": "),e.parsed.x!==null&&(t+=e.parsed.x+"%"),t}}}},scales:{x:{beginAtZero:!0,title:{display:!0,text:"Accuracy (%)"}},y:{ticks:{autoSkip:!1}}}}})})</script><p>The human baseline has an admittedly small sample size (just myself, it&rsquo;s difficult to find skilled MTG players who want to sit through a 200 question test), but still provides a valuable reference. As the creator of the benchmark and a player of the game for 10+ years, I scored 68% agreement with the other human deck builders. This could be a skill issue on my end, but I think it is more likely due to the somewhat subjective nature of question, along with the presentation format. To provide a fair comparison, I made a script that presents that questions to me exactly as they are shown to the LLMs, but I think I could do better using a deck editor. Despite my less than perfect score, I still beat most of the models by a wide margin. Among LLMs, o3 (low) comes closest with 63% accuracy, followed by Gemini 2.5 Pro 06-05 at 57.5%, and the older Gemini 2.5 Pro 03-25 at 53%. This aligns with qualitative assessments and other benchmarks where these models often excel in real-world tasks. Their success here suggests that ManaBench is effectively capturing a similar type of reasoning aptitude.</p><p>The results also highlight a discernible gap in performance between the leading American models (o3, Gemini 2.5 Pro, Claude) and prominent Chinese models like Deepseek R1 (43.5%) and Qwen3 235B A22B (37%). While these models are undoubtedly powerful, their performance on ManaBench suggests that their reasoning capabilities may not be as developed as some of their US counterparts. This observation underscores the utility of specialized benchmarks like ManaBench in revealing subtle but significant differences in model capabilities that might be obscured by broader, more generalized benchmarks.</p><h2 id=benchmark-construction-methodology>Benchmark Construction Methodology<a hidden class=anchor aria-hidden=true href=#benchmark-construction-methodology>#</a></h2><p>The creation of ManaBench involves several stages, from curating source decks to generating challenging multiple-choice questions. The process is designed to be rigorous and reproducible, with a focus on capturing instances of human expert judgment as reflected in competitive deck choices.</p><h3 id=source-data--deck-curation>Source Data & Deck Curation<a hidden class=anchor aria-hidden=true href=#source-data--deck-curation>#</a></h3><p>The foundation of the benchmark is a large corpus of human-constructed MTG decklists scraped from <a href=https://www.mtgtop8.com/>MTGTop8</a> (a public database of MTG tournament results and decklists). The curation process involves:</p><ol><li><strong>Format Filtering:</strong> Decks are categorized by major constructed formats: Modern, Pioneer, Legacy, and Vintage. The benchmark currently focuses on these eternal formats. Standard is excluded due to its rotating nature, which does not align with the goal of measuring reasoning abilities and minimizing the impact of knowledge cutoff dates, thereby keeping the focus on reasoning skills rather than rapidly changing format knowledge.</li><li><strong>Strict Validation:</strong> Only decks containing exactly 60 main deck cards and 15 sideboard cards are considered. This ensures that they obey conventional deck-building wisdom and that the creator carefully considered the metagame when constructing the deck.</li><li><strong>Format Legality Check:</strong> Using the card database from <a href=https://mtgjson.com/>MTGJSON</a>, every card in a potential source deck is verified for legality within its designated format. This step is crucial for ensuring that the decks and subsequent questions are valid within the game&rsquo;s rules.</li><li><strong>Sampling:</strong> From the pool of validated decks, 25 decks are randomly sampled for each of the four target formats, resulting in a set of 100 unique, curated decklists for benchmark generation. These decks represent successful strategies designed and tested by human players in tournament play.</li></ol><h3 id=question-generation>Question Generation<a hidden class=anchor aria-hidden=true href=#question-generation>#</a></h3><p>For each of the 100 sampled human-constructed decks, two unique questions are generated. This involves selecting a &ldquo;golden&rdquo; card to remove and then using the Manamorphosis model to propose alternatives.</p><ol><li><p><strong>Golden Card Selection:</strong></p><ul><li>The &ldquo;golden&rdquo; card represents the correct answer for the deck completion task, reflecting a choice consistent with the original human-designed, tournament-sourced deck.</li><li>It is chosen by randomly selecting one <em>unique card name</em> present in the original 60-card main deck. For instance, if a deck contains four copies of &ldquo;Lightning Bolt,&rdquo; &ldquo;Lightning Bolt&rdquo; is one possible unique card name that could be selected as the golden card.</li><li>To ensure variety in the questions derived from a single deck, the two golden cards selected from the same source deck must be different card names.</li></ul></li><li><p><strong>Partial Deck Creation:</strong></p><ul><li>Once a golden card name is selected, one instance of this card is removed from the original 60-card main deck list. This creates the 59-card partial deck that will be presented to the LLM.</li><li>For example, if &ldquo;Island&rdquo; is chosen as the golden card from a deck containing 10 Islands, the partial deck will contain 9 Islands.</li></ul></li><li><p><strong>Generating Plausible Alternatives:</strong></p><ul><li>The five incorrect-but-plausible alternatives are generated using a Manamorphosis, a Transformer-based diffusion model custom-trained trained on a vast corpus of MTG decks. It learns to represent cards as high-dimensional embeddings and understands patterns of card co-occurrence and deck structure.</li><li>For benchmark generation, this model takes the 59-card partial deck and, through a reverse diffusion process conditioned on these known cards, predicts embeddings for the missing card. These embeddings are then mapped back to specific card names. This process is designed to generate alternatives that are contextually relevant yet distinct from the golden card.</li><li>For a detailed technical explanation of the diffusion model&rsquo;s architecture and training process, please refer to the <a href=https://github.com/JakeBoggs/Manamorphosis>Manamorphosis repository</a> and the <a href=https://boggs.tech/posts/manamorphosis>accompanying blog post</a>.</li><li>This generation process is repeated to obtain 5 unique card names that are different from the chosen golden card and from each other, serving as challenging distractors for the LLM.</li></ul></li><li><p><strong>Question Structure (JSON):</strong>
Each generated question is stored in a JSON object with the following structure:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#e6db74>&#34;question_001&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;deck&#34;</span>: [<span style=color:#e6db74>&#34;Card Name 1&#34;</span>, <span style=color:#e6db74>&#34;Card Name 2&#34;</span>, <span style=color:#e6db74>&#34;Card Name 3&#34;</span>, <span style=color:#e6db74>&#34;... list of 59 card names ...&#34;</span>],
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;golden&#34;</span>: <span style=color:#e6db74>&#34;Actual Missing Card Name&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;alternatives&#34;</span>: [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Alternative Card Name A&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Alternative Card Name B&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Alternative Card Name C&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Alternative Card Name D&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Alternative Card Name E&#34;</span>
</span></span><span style=display:flex><span>  ],
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;format&#34;</span>: <span style=color:#e6db74>&#34;modern&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></li></ol><h2 id=evaluation-protocol>Evaluation Protocol<a hidden class=anchor aria-hidden=true href=#evaluation-protocol>#</a></h2><h3 id=prompting-strategy>Prompting Strategy<a hidden class=anchor aria-hidden=true href=#prompting-strategy>#</a></h3><p>Careful prompt engineering is employed to provide the LLM with sufficient context to make a reasoned judgment, minimizing the need for memorized MTG card knowledge and emphasizing analytical skill:</p><ol><li><strong>Role Assigment:</strong> The LLM is instructed: <code>"You are an expert Magic: The Gathering player."</code></li><li><strong>Task Definition:</strong> The prompt explains that a decklist from a specific format is missing one card and asked to choose the answer that best completes the deck.</li><li><strong>Decklist Presentation:</strong> The 59-card partial deck is provided. Each card entry includes:<ul><li>Its count in the partial deck.</li><li>Its full name.</li><li>Its detailed rules text, including type line, mana cost, power/toughness (for creatures), loyalty (for planeswalkers), and Oracle text. This information is sourced from MTGJSON. <em>The explicit provision of full rules text for all cards in the deck and choices is a key design element, intended to reduce the task&rsquo;s reliance on the LLM&rsquo;s pre-existing knowledge of specific cards and instead focus on its ability to reason based on the provided information.</em></li><li><em>Example Card Presentation in Prompt:</em>
<code>2x Snapcaster Mage - Creature - Human Wizard - Cost: {1}{U} - P/T: 2/1 - Rules: Flash. When Snapcaster Mage enters the battlefield, target instant or sorcery card in your graveyard gains flashback until end of turn. The flashback cost is equal to its mana cost.</code></li></ul></li><li><strong>Multiple-Choice Options:</strong><ul><li>The six choices (the golden card and the five alternatives) are presented, shuffled randomly to avoid positional bias, and labeled A through F.</li><li>Each choice is also presented with its full name and detailed rules text (sourced from MTGJSON, same as deck cards).</li><li><em>Example Choice Presentation in Prompt:</em>
<code>A) Brainstorm - Instant - Cost: {U} - Rules: Draw three cards, then put two cards from your hand on top of your library in any order.</code></li></ul></li></ol><h3 id=answer-format-and-extraction>Answer Format and Extraction<a hidden class=anchor aria-hidden=true href=#answer-format-and-extraction>#</a></h3><p>To standardize evaluation, LLMs are instructed to output their final answer in a specific format:
<code>"Respond with only the letter of your choice in the format: ANSWER: [LETTER]"</code></p><p>The evaluation script parses this response using the following logic:</p><ol><li><strong>Primary Method:</strong> Looks for the &ldquo;ANSWER: [LETTER]&rdquo; pattern (case-insensitive for &ldquo;ANSWER:&rdquo;, extracts the letter).</li><li><strong>Fallback 1 (Single Letter):</strong> If the primary method fails, it checks if the entire response is a single letter from A to F (e.g., a response like &ldquo;C&rdquo;).</li><li><strong>Fallback 2 (Boxed Letter):</strong> If both above fail, it looks for the pattern <code>$\boxed{LETTER}$</code> (e.g., &ldquo;The answer is $\boxed{A}$&rdquo;) using a regular expression.</li></ol><h3 id=metrics>Metrics<a hidden class=anchor aria-hidden=true href=#metrics>#</a></h3><ul><li><strong>Accuracy:</strong> The primary metric is the percentage of questions the LLM answers correctly by selecting the golden card.</li></ul><h2 id=why-manabench-is-a-strong-benchmark-for-reasoning>Why ManaBench is a Strong Benchmark for Reasoning<a hidden class=anchor aria-hidden=true href=#why-manabench-is-a-strong-benchmark-for-reasoning>#</a></h2><p>The preliminary results and the design of ManaBench highlight several key strengths for evaluating an LLM&rsquo;s reasoning capabilities:</p><ul><li><p><strong>Measures Alignment with Human Expert Judgment:</strong> The &ldquo;golden&rdquo; answers are derived from decks designed by human players and sourced from MTGTop8, a database of tournament decklists. Success on this benchmark therefore indicates an LLM&rsquo;s ability to make choices that align with established human expertise and strategic consensus.</p></li><li><p><strong>Clear Performance Differentiation:</strong> When comparing ManaBench scores to other established LLM evaluation metrics like LMArena ELO ratings, ManaBench demonstrates a significantly stronger ability to differentiate between models. While a general positive correlation is observed (as indicated by the trendline in the chart below), ManaBench provides a much wider relative spread in scores. For the models included in the comparison, ManaBench accuracies range from 19.5% to 63% (a spread of 43.5 percentage points, representing an increase of approximately 223% from the minimum observed score to the maximum). In contrast, their LMArena ELO scores range from 1257 to 1470 (a spread of 213 ELO points, representing an increase of approximately 17% from the minimum observed ELO score to the maximum). The significantly larger proportional range in ManaBench allows for a more granular distinction between models.</p><div style="width:90%;margin:20px auto"><canvas id=correlationChart></canvas></div><script>document.addEventListener("DOMContentLoaded",()=>{const d=document.getElementById("correlationChart").getContext("2d"),h=["o3 (low)","Gemini 2.5 Pro 03-25","Claude 3.7 Sonnet (no thinking)","o4 Mini (low)","Deepseek R1","GPT-4o 08-06","Qwen3 235B A22B (thinking)","Grok 3 Mini (low)","Gemini 2.0 Flash","Mistral 3 Medium","Llama 4 Maverick","GPT-4.1 Nano","Llama 3.3 70B","Gemini 1.5 Flash","Deepseek V3 03-24","Gemini 2.5 Pro 06-05","Deepseek R1 05-28"],u=[63,53,49.5,45,43.5,41,37,37,35,31.5,26.5,19.5,19.5,22.5,37.5,57.5,43],s={"Gemini 2.5 Pro 03-25":1448,"o3 (low)":1411,"Claude 3.7 Sonnet (no thinking)":1291,"o4 Mini (low)":1351,"Deepseek R1":1359,"GPT-4o 08-06":1265,"Qwen3 235B A22B (thinking)":1342,"Gemini 2.0 Flash":1355,"Llama 4 Maverick":1269,"GPT-4.1 Nano":1270,"Llama 3.3 70B":1257,"Gemini 1.5 Flash":1271,"Deepseek V3 03-24":1372,"Gemini 2.5 Pro 06-05":1470},t=["rgba(75, 192, 192, 0.8)","rgba(0, 200, 255, 0.8)","rgba(54, 162, 235, 0.8)","rgba(255, 206, 86, 0.8)","rgba(255, 99, 132, 0.8)","rgba(153, 102, 255, 0.8)","rgba(255, 159, 64, 0.8)","rgba(101, 143, 74, 0.8)","rgba(210, 105, 30, 0.8)","rgba(0, 128, 128, 0.8)","rgba(165, 42, 42, 0.8)","rgba(70, 130, 180, 0.8)","rgba(128, 0, 128, 0.8)","rgba(255, 105, 180, 0.8)","rgba(128, 128, 0, 0.8)","rgba(0, 220, 220, 0.8)","rgba(255, 99, 100, 0.8)"],o=t.map(e=>e.replace("0.8","1")),i=[],a=[],r=[],e=[],c=[];h.forEach((n,l)=>{if(s[n]!==void 0){const d={x:s[n],y:u[l]};i.push(d),e.push(d),c.push(n),a.push(t[l%t.length]||"rgba(150, 150, 150, 0.8)"),r.push(o[l%o.length]||"rgba(150, 150, 150, 1)")}});let n={};if(e.length>=2){let t=0,s=0,a=0,r=0;const o=e.length;e.forEach(e=>{t+=e.x,s+=e.y,a+=e.x*e.y,r+=e.x*e.x});const i=(o*a-t*s)/(o*r-t*t),c=(s-i*t)/o,l=Math.min(...e.map(e=>e.x)),d=Math.max(...e.map(e=>e.x));n={type:"line",label:"Trendline",data:[{x:l,y:i*l+c},{x:d,y:i*d+c}],borderColor:"rgba(255, 0, 0, 0.7)",borderWidth:2,fill:!1,pointRadius:0,tension:0}}const l=[{label:"Models",type:"scatter",data:i,backgroundColor:a,borderColor:r,pointRadius:8,pointHoverRadius:10}];Object.keys(n).length>0&&l.push(n),new Chart(d,{type:"scatter",data:{datasets:l},options:{responsive:!0,maintainAspectRatio:!0,plugins:{legend:{display:!1},title:{display:!0,text:"ManaBench Accuracy vs. LMArena ELO",font:{size:18}},tooltip:{filter:function(e){return e.datasetIndex===0},callbacks:{label:function(e){const n=c[e.dataIndex];let t=n||"";return t&&(t+=": "),e.parsed.x!==null&&e.parsed.y!==null&&(t+=`(LMArena ELO: ${e.parsed.x}, ManaBench: ${e.parsed.y}%)`),t}}}},scales:{x:{type:"linear",position:"bottom",title:{display:!0,text:"LMArena ELO (May 2025)"}},y:{beginAtZero:!0,title:{display:!0,text:"ManaBench Accuracy (%)"}}}}})})</script></li><li><p><strong>Challenge for Frontier Models:</strong> With the exception of o3, most of the models are far from matching human performance, let alone exceeding it. There is a 5.5 point gap between the top models (o3 at 63% and Gemini 2.5 Pro 06-05 at 57.5%) and no model reaches the human baseline, showing that the benchmark remains unsaturated and clearly presents a significant challenge for frontier models.</p></li><li><p><strong>Test of Generalization vs. Benchmark Overfitting:</strong> The complexity of MTG deck construction, the private nature of the benchmark questions, and the fact that MTG strategy is unlikely to be a direct optimization target for most LLM labs, collectively make ManaBench a strong test of generalized reasoning. Performance on this benchmark may reveal whether models are truly capable of applying reasoning to novel, complex systems, or if their high scores on common academic benchmarks (like MMLU or MATH) are partly due to overfitting or memorization of those specific test distributions. For example, a model series like Llama 4, which demonstrated strong performance on many standard benchmarks, gave a much weaker showing here, highlighting the value of diverse, specialized evaluations like ManaBench in assessing true generalization. This aligns with the experiences of many users who reported that Llama 4 struggled with real tasks and underperformed expectations.</p></li><li><p><strong>Cost-Effectiveness and Efficiency:</strong> With 200 questions, the benchmark is relatively concise compared to some larger evaluation suites. This allows for more rapid and cost-effective evaluation cycles, making it feasible to test a wider array of models or fine-tuned variants without incurring prohibitive API costs or excessive computation time, while still providing strong differentiating signals as seen in the results.</p></li></ul><h2 id=benchmark-integrity>Benchmark Integrity<a hidden class=anchor aria-hidden=true href=#benchmark-integrity>#</a></h2><p>To maintain the integrity and long-term utility of ManaBench as an evaluation tool, the specific benchmark questions and code are <strong>not being publicly released at this time.</strong> This measure is taken to prevent the benchmark from being inadvertently included in the training data of future LLMs, which would compromise its validity as an unseen test set. If you are a researcher and would like private access, please reach out.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>ManaBench offers a novel approach to evaluating the sophisticated reasoning capabilities of Large Language Models by leveraging the strategic depth of Magic: The Gathering. The benchmark&rsquo;s core &ldquo;deck completion task&rdquo; – choosing the optimal 60th card for a 59-card deck - demands an understanding of strategic coherence, system-wide optimization, and complex interactions.</p><p>The initial results demonstrate ManaBench&rsquo;s potential as a strong differentiator of LLM reasoning abilities, with even frontier models finding the task challenging. If you would me to add other models to the leaderboard or just think it&rsquo;s a cool project, consider checking out my other related work or following me on <a href=https://x.com/JakeABoggs>Twitter</a></p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://boggs.tech/>Jake Boggs</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>