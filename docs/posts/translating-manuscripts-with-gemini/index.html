<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Translating Historical Manuscripts with Gemini 3 | Jake Boggs</title><meta name=keywords content><meta name=description content="I&rsquo;ve spent a lot of time working on document understanding for products like automated order entry and so I&rsquo;m always looking for new ways to evaluate the visual capabilities of LLMs. About a month ago, I stumbled across this post by Mark Humphries about Gemini 3 Pro&rsquo;s impressive ability to transcribe historical texts. This seemed interesting enough to spend an evening building an app around (and I wanted to impress my girlfriend, who majors in anthropology and is interested in medieval medicine)."><meta name=author content="Jake Boggs"><link rel=canonical href=https://boggs.tech/posts/translating-manuscripts-with-gemini/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://boggs.tech/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://boggs.tech/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://boggs.tech/favicon-32x32.png><link rel=apple-touch-icon href=https://boggs.tech/apple-touch-icon.png><link rel=mask-icon href=https://boggs.tech/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://boggs.tech/posts/translating-manuscripts-with-gemini/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><script>!function(e){if(window.reb2b)return;window.reb2b={loaded:!0};var t=document.createElement("script");t.async=!0,t.src="https://ddwl4m2hdecbv.cloudfront.net/b/"+e+"/"+e+".js.gz",document.getElementsByTagName("script")[0].parentNode.insertBefore(t,document.getElementsByTagName("script")[0])}("GOYPYHQMLEOX")</script><meta property="og:url" content="https://boggs.tech/posts/translating-manuscripts-with-gemini/"><meta property="og:site_name" content="Jake Boggs"><meta property="og:title" content="Translating Historical Manuscripts with Gemini 3"><meta property="og:description" content="I’ve spent a lot of time working on document understanding for products like automated order entry and so I’m always looking for new ways to evaluate the visual capabilities of LLMs. About a month ago, I stumbled across this post by Mark Humphries about Gemini 3 Pro’s impressive ability to transcribe historical texts. This seemed interesting enough to spend an evening building an app around (and I wanted to impress my girlfriend, who majors in anthropology and is interested in medieval medicine)."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-19T00:00:00+00:00"><meta property="article:modified_time" content="2026-01-19T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Translating Historical Manuscripts with Gemini 3"><meta name=twitter:description content="I&rsquo;ve spent a lot of time working on document understanding for products like automated order entry and so I&rsquo;m always looking for new ways to evaluate the visual capabilities of LLMs. About a month ago, I stumbled across this post by Mark Humphries about Gemini 3 Pro&rsquo;s impressive ability to transcribe historical texts. This seemed interesting enough to spend an evening building an app around (and I wanted to impress my girlfriend, who majors in anthropology and is interested in medieval medicine)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://boggs.tech/posts/"},{"@type":"ListItem","position":2,"name":"Translating Historical Manuscripts with Gemini 3","item":"https://boggs.tech/posts/translating-manuscripts-with-gemini/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Translating Historical Manuscripts with Gemini 3","name":"Translating Historical Manuscripts with Gemini 3","description":"I\u0026rsquo;ve spent a lot of time working on document understanding for products like automated order entry and so I\u0026rsquo;m always looking for new ways to evaluate the visual capabilities of LLMs. About a month ago, I stumbled across this post by Mark Humphries about Gemini 3 Pro\u0026rsquo;s impressive ability to transcribe historical texts. This seemed interesting enough to spend an evening building an app around (and I wanted to impress my girlfriend, who majors in anthropology and is interested in medieval medicine).\n","keywords":[],"articleBody":"I’ve spent a lot of time working on document understanding for products like automated order entry and so I’m always looking for new ways to evaluate the visual capabilities of LLMs. About a month ago, I stumbled across this post by Mark Humphries about Gemini 3 Pro’s impressive ability to transcribe historical texts. This seemed interesting enough to spend an evening building an app around (and I wanted to impress my girlfriend, who majors in anthropology and is interested in medieval medicine).\nI had never worked with manuscripts before, but I wanted something special: documents without public translations. After some quick research, we found the Digital Scriptorium, which seemed to have what I needed.\nFor example: Sutro Collection MS 04, Medical recipes\nWhile it's certainly possible this has been translated before, we couldn't locate one online.\rMany of these manuscripts have dozens to hundreds of pages, which is too much for current models to handle well all at once. Even when the full document fits inside the context window, performance degrades as the length increases. To prevent this, my initial attempt processed each page in parallel and asked Gemini to translate directly. This worked decently, but it was inconsistent and difficult to tell if issues were due to incorrectly reading the characters or misunderstanding the meaning. My next iteration split the transcription into a separate step before the translation, which improved the quality but made another issue apparent: accurately interpreting the meaning of one page often requires context from the rest of the document. Missing context was the main culprit behind a lot of the poor translations, so I decided to adopt a hybrid approach. Each page is now transcribed in parallel, but all of the transcripts are concatenated for translation. Although this does bloat the context, the transcripts typically don’t consume as many tokens as the raw images and this method was more reliable than my prior attempts.\nTranscribed page.\rAfter testing both Gemini 3 Pro and Gemini 3 Flash, I went with the Flash model for transcription. It is much cheaper and faster (Pro sometimes cost dozens of cents per page), while still getting close to the same level of quality. To improve the consistency, the app samples multiple transcripts in parallel, then passes them all + the image back to Gemini to have it produce a final result.\nThe last problem was handling pages where the scans were taken from too far away. Although the images are typically very high resolution (e.g. 6000x4000), they are scaled down to a standard input shape by the model provider before processing. This causes a lot of the details to be lost for pages that are zoomed out. My solution was to add a preprocessing step which asks Gemini 3 Flash to output a bounding box around the main area of text, then cropping to that section.\nAdditionally, it runs a blended Sauvola filter, a sharpness filter, converts to the CIELAB color space, and applies CLAHE to the L channel. These filters aren’t strictly necessary, but they made it easier for me to read the text and slightly helped the model too. I won’t go into too much detail here, as image processing is not focus of this post, but the basic idea is that they separate the text from the background and even the lighting and contrast across the image.\nEnhanced text.\rFor translation, I tried both Gemini 3 Pro and GPT-5.2, choosing the latter due to better adherence to the output format. To align the translation with the pages, the model is instructed to wrap the output corresponding to each page in XML tags, which can be parsed out easily. Another strategy I want to try at some point is to sample outputs from multiple models and have them debate to produce a translation collectively.\nResulting translation.\rAs a final touch, I added a button to download the full document with the original images, transcripts, and translations side-by-side as a PDF.\nSide-by-side PDF layout.\rAnthropology is certainly not my main area of interest, but I do enjoy seeing the progress frontier models are making in fields outside of programming and basic knowledge work. I’m far from an industry veteran (just 22 years old), but I do remember learning about CNNs back when people still cared about MNIST / CIFAR and YOLO was a new thing. We live in extraordinary times and I’m incredibly optimistic about the potential for frontier models in all types of research. Even if the promises of automated researchers or “superintelligence” do not come to pass, the existing models still have so much untapped potential and I expect we’ll see many breakthroughs as adoption increases.\nIf you want to try it out, the code is available on Github\n","wordCount":"793","inLanguage":"en","datePublished":"2026-01-19T00:00:00Z","dateModified":"2026-01-19T00:00:00Z","author":{"@type":"Person","name":"Jake Boggs"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://boggs.tech/posts/translating-manuscripts-with-gemini/"},"publisher":{"@type":"Organization","name":"Jake Boggs","logo":{"@type":"ImageObject","url":"https://boggs.tech/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://boggs.tech/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://boggs.tech/about title="About me"><span>About me</span></a></li><li><a href=https://boggs.tech/startups title=Startups><span>Startups</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Translating Historical Manuscripts with Gemini 3</h1><div class=post-meta><span title='2026-01-19 00:00:00 +0000 UTC'>January 19, 2026</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>793 words</span>&nbsp;·&nbsp;<span>Jake Boggs</span></div></header><div class=post-content><p>I&rsquo;ve spent a lot of time working on document understanding for products like <a href=https://www.endeavor.ai/order-entry-automation>automated order entry</a> and so I&rsquo;m always looking for new ways to evaluate the visual capabilities of LLMs. About a month ago, I stumbled across <a href=https://generativehistory.substack.com/p/the-sugar-loaf-test-how-an-18th-century>this post</a> by Mark Humphries about Gemini 3 Pro&rsquo;s impressive ability to transcribe historical texts. This seemed interesting enough to spend an evening building an app around (and I wanted to impress my girlfriend, who majors in anthropology and is interested in medieval medicine).</p><p>I had never worked with manuscripts before, but I wanted something special: documents without public translations. After some quick research, we found the <a href=https://digital-scriptorium.org/>Digital Scriptorium</a>, which seemed to have what I needed.</p><p>For example: <a href=https://archive.org/details/images_SutroCollectionMS04_12>Sutro Collection MS 04, Medical recipes</a></p><div style=display:flex;flex-direction:column;align-items:center><img src=/images/medical_recipes.png alt="Medical Recipes Book" style=max-width:80%><p><small>While it's certainly possible this has been translated before, we couldn't locate one online.</small></p></div><p>Many of these manuscripts have dozens to hundreds of pages, which is too much for current models to handle well all at once. Even when the full document fits inside the context window, performance degrades as the length increases. To prevent this, my initial attempt processed each page in parallel and asked Gemini to translate directly. This worked decently, but it was inconsistent and difficult to tell if issues were due to incorrectly reading the characters or misunderstanding the meaning. My next iteration split the transcription into a separate step before the translation, which improved the quality but made another issue apparent: accurately interpreting the meaning of one page often requires context from the rest of the document. Missing context was the main culprit behind a lot of the poor translations, so I decided to adopt a hybrid approach. Each page is now transcribed in parallel, but all of the transcripts are concatenated for translation. Although this does bloat the context, the transcripts typically don&rsquo;t consume as many tokens as the raw images and this method was more reliable than my prior attempts.</p><div style=display:flex;flex-direction:column;align-items:center><img src=/images/manuscript_transcript.png alt="Transcribed page" style=max-width:80%><p><small>Transcribed page.</small></p></div><p>After testing both Gemini 3 Pro and Gemini 3 Flash, I went with the Flash model for transcription. It is much cheaper and faster (Pro sometimes cost dozens of cents per page), while still getting close to the same level of quality. To improve the consistency, the app samples multiple transcripts in parallel, then passes them all + the image back to Gemini to have it produce a final result.</p><p>The last problem was handling pages where the scans were taken from too far away. Although the images are typically very high resolution (e.g. 6000x4000), they are scaled down to a standard input shape by the model provider before processing. This causes a lot of the details to be lost for pages that are zoomed out. My solution was to add a preprocessing step which asks Gemini 3 Flash to output a bounding box around the main area of text, then cropping to that section.</p><p>Additionally, it runs a blended Sauvola filter, a sharpness filter, converts to the CIELAB color space, and applies CLAHE to the L channel. These filters aren&rsquo;t strictly necessary, but they made it easier for me to read the text and slightly helped the model too. I won&rsquo;t go into too much detail here, as image processing is not focus of this post, but the basic idea is that they separate the text from the background and even the lighting and contrast across the image.</p><div style=display:flex;flex-direction:column;align-items:center><img src=/images/manuscript_enhanced.png alt="Enhanced text" style=max-width:80%><p><small>Enhanced text.</small></p></div><p>For translation, I tried both Gemini 3 Pro and GPT-5.2, choosing the latter due to better adherence to the output format. To align the translation with the pages, the model is instructed to wrap the output corresponding to each page in <code>&lt;page_n>&lt;/page_n></code> XML tags, which can be parsed out easily. Another strategy I want to try at some point is to sample outputs from multiple models and have them debate to produce a translation collectively.</p><div style=display:flex;flex-direction:column;align-items:center><img src=/images/manuscript_translation.png alt="Translated page" style=max-width:80%><p><small>Resulting translation.</small></p></div><p>As a final touch, I added a button to download the full document with the original images, transcripts, and translations side-by-side as a PDF.</p><div style=display:flex;flex-direction:column;align-items:center><img src=/images/manuscript_pdf.png alt="Translated page" style=max-width:80%><p><small>Side-by-side PDF layout.</small></p></div><p>Anthropology is certainly not my main area of interest, but I do enjoy seeing the progress frontier models are making in fields outside of programming and basic knowledge work. I&rsquo;m far from an industry veteran (just 22 years old), but I do remember learning about CNNs back when people still cared about MNIST / CIFAR and YOLO was a new thing. We live in extraordinary times and I&rsquo;m incredibly optimistic about the potential for frontier models in all types of research. Even if the promises of automated researchers or &ldquo;superintelligence&rdquo; do not come to pass, the existing models still have so much untapped potential and I expect we&rsquo;ll see many breakthroughs as adoption increases.</p><p>If you want to try it out, the code is <a href=https://github.com/JakeBoggs/Manuscript-Translator>available on Github</a></p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://boggs.tech/>Jake Boggs</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>