<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=author content="Jake Boggs"><title>Gait Analysis for Physical Therapy with YOLOv11 | Jake Boggs</title><link rel=stylesheet href=/css/main.min.css><link rel=icon href=/favicon.png><link rel=canonical href=https://boggs.tech/posts/gait-analysis-physical-therapy-research/></head><body><header><div class=container><a href=https://boggs.tech/ class=site-title>Jake Boggs</a><nav><a href=/about>About me</a><a href=/startups>Startups</a></nav></div></header><main><div class=container><article><h1>Gait Analysis for Physical Therapy with YOLOv11</h1><div class=article-meta><span>April 21, 2025</span>
<span>716 words</span>
<span>4 min read</span></div><div class=article-content><p>Analyzing how people walk using video is common in research and clinical settings, but getting accurate joint angles usually means either expensive equipment or annotating frames, which is slow and tedious. After seeing my mother spend a lot of time doing this manually over Easter weekend, I built a Python tool to help her speed it up. It uses <a href=https://www.ultralytics.com/blog/how-to-use-ultralytics-yolo11-for-pose-estimation>YOLOv11-pose</a> for automatic detection and adds an interactive interface for manual adjustments.</p><p><strong>Demo</strong></p><video width=100% controls>
<source src=/videos/gait-analyzer.mp4 type=video/mp4>Your browser does not support the video tag.</video><p><strong>How it Works</strong></p><p>The script lets you load a video file (specifically a side-view of someone walking) and capture joint angles at various points. Here&rsquo;s the workflow:</p><ol><li><strong>Load & Play:</strong> Open the video. You can play/pause with <code>Space</code> and move frame-by-frame using the <code>Left</code>/<code>Right</code> arrow keys or <code>p</code>/<code>n</code>.</li><li><strong>Automatic Detection:</strong> When the video is paused or you step to a new frame, the script runs the YOLOv11-pose model on the frame.</li><li><strong>Key Points:</strong> The model identifies the most prominent person and then 5 key points relevant for side-view gait are calculated from the resulting keypoints. For example, calculating the hip center:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># kps holds the keypoints for the detected person</span>
</span></span><span style=display:flex><span>Lh <span style=color:#f92672>=</span> kps[KP[<span style=color:#e6db74>&#39;left_hip&#39;</span>]][:<span style=color:#ae81ff>2</span>] <span style=color:#75715e># Get x,y for left hip</span>
</span></span><span style=display:flex><span>Rh <span style=color:#f92672>=</span> kps[KP[<span style=color:#e6db74>&#39;right_hip&#39;</span>]][:<span style=color:#ae81ff>2</span>] <span style=color:#75715e># Get x,y for right hip</span>
</span></span><span style=display:flex><span>H <span style=color:#f92672>=</span> ((Lh<span style=color:#f92672>+</span>Rh)<span style=color:#f92672>/</span><span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>tolist() <span style=color:#75715e># Calculate the midpoint</span>
</span></span></code></pre></div>And estimating the toe position based on the knee (Kpt) and ankle (Apt):<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Simplified Toe Calculation</span>
</span></span><span style=display:flex><span>Kpt_np, Apt_np <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(Kpt), np<span style=color:#f92672>.</span>array(Apt)
</span></span><span style=display:flex><span>leg_vector_ka <span style=color:#f92672>=</span> Apt_np <span style=color:#f92672>-</span> Kpt_np
</span></span><span style=display:flex><span><span style=color:#75715e># Calculate a perpendicular vector based on side</span>
</span></span><span style=display:flex><span>perp_vector <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#f92672>-</span>leg_vector_ka[<span style=color:#ae81ff>1</span>], leg_vector_ka[<span style=color:#ae81ff>0</span>]]) <span style=color:#75715e># Example for left side</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Offset from ankle, half the length of the knee-ankle segment</span>
</span></span><span style=display:flex><span>T_np <span style=color:#f92672>=</span> Apt_np <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> perp_vector
</span></span><span style=display:flex><span>T <span style=color:#f92672>=</span> T_np<span style=color:#f92672>.</span>tolist()
</span></span></code></pre></div>These points (S, H, K, A, T) are drawn on the video frame.</li><li><strong>Angle Calculation:</strong> It also computes and displays three angles: Shoulder-Hip-Knee, Hip-Knee-Ankle, and Knee-Ankle-Toe.<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Simplified angle calculation within recompute_angles()</span>
</span></span><span style=display:flex><span>S, H, K, A, T <span style=color:#f92672>=</span> [np<span style=color:#f92672>.</span>array(p) <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> points]
</span></span><span style=display:flex><span><span style=color:#75715e># Calculate vectors (e.g., from Hip to Shoulder, Hip to Knee)</span>
</span></span><span style=display:flex><span>vec_HS <span style=color:#f92672>=</span> S <span style=color:#f92672>-</span> H
</span></span><span style=display:flex><span>vec_HK <span style=color:#f92672>=</span> K <span style=color:#f92672>-</span> H
</span></span><span style=display:flex><span><span style=color:#75715e># Use dot product formula (via angle_between helper) for angle</span>
</span></span><span style=display:flex><span>angle_SHK_internal <span style=color:#f92672>=</span> angle_between(vec_HS, vec_HK) <span style=color:#75715e># Custom helper function</span>
</span></span><span style=display:flex><span>angles[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>180.0</span> <span style=color:#f92672>-</span> angle_SHK_internal <span style=color:#75715e># Store final angle</span>
</span></span><span style=display:flex><span><span style=color:#75715e># ... similar calculations for HKA and KAT ...</span>
</span></span></code></pre></div></li><li><strong>Interactive Correction:</strong> If the automatically detected joints aren&rsquo;t quite right, you can <strong>click and drag</strong> them directly on the video frame.</li><li><strong>Save Data:</strong> Once you&rsquo;re satisfied with the point placement on a given frame (while paused), press the <strong>&rsquo;s&rsquo;</strong> key. This triggers the saving process detailed below.</li></ol><p><strong>Output Data</strong></p><p>When you run the script, you can specify an output directory (or it defaults to one named <code>output</code>). Inside that directory, the script automatically creates a <strong>timestamped subfolder</strong> for each video analysis session. For example, analyzing <code>my_gait_video.mp4</code> might create a folder like:</p><p><code>output/my_gait_video_20231028_163045/</code></p><p>This keeps results from different runs or videos organized. Inside this folder, you&rsquo;ll find:</p><ul><li><strong>Cropped Images (PNGs):</strong> Every time you press &rsquo;s&rsquo;, a PNG file is saved, named like <code>my_gait_video_0123.png</code> (where <code>0123</code> is the frame number). This isn&rsquo;t the whole video frame, but a cropped image focused on the detected person&rsquo;s bounding box. The 5 points (S, H, K, A, T) and the calculated angles are drawn directly onto this image. These are useful for visual checks, qualitative assessments, or including specific examples in reports.</li><li><strong>Angle Data (CSV):</strong> A single CSV file, named after the original video (e.g., <code>my_gait_video.csv</code>), is created for the session. Each time you press &rsquo;s&rsquo;, a new row is added to this file. The columns are:
<code>Frame, S-H-K, H-K-A, K-A-T</code>
So, a row might look like: <code>123, 165.23, 170.51, 85.90</code></li></ul><p><strong>Important Note on Privacy:</strong> The YOLO model needs to be downloaded on first use, but after that, all video processing and analysis happens <strong>locally on your machine</strong>. No video data is sent anywhere.</p><p><strong>How to Use It</strong></p><ol><li><strong>Get the code:</strong> It&rsquo;s available on GitHub:
<a href=https://github.com/JakeBoggs/Gait-Analyzer>https://github.com/JakeBoggs/Gait-Analyzer</a></li><li><strong>Install dependencies:</strong> Download <a href=https://www.python.org/downloads/>Python</a> if you do not already have it, then install the required libraries with <code>pip install opencv-python numpy ultralytics</code></li><li><strong>Run from the terminal:</strong> Navigate to the repository directory and run:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python track.py --video &lt;path_to_your_video.mp4&gt; --output &lt;your_results_folder&gt;
</span></span></code></pre></div>Replace <code>&lt;path_to_your_video.mp4></code> with the actual file path and <code>&lt;your_results_folder></code> with where you want the main output directory created. Check the README on GitHub for other options.</li></ol></div></article></div></main><footer><div class=container><p>&copy; 2026 Jake Boggs</p></div></footer></body></html>