<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Gait Analysis for Physical Therapy with YOLOv11 | Jake Boggs</title>
<meta name=keywords content><meta name=description content="Analyzing how people walk using video is common in research and clinical settings, but getting accurate joint angles usually means either expensive equipment or manually annotating frames, which is slow and tedious.
Over Easter weekend, I built a Python tool to help my mother with her research study analyzing patient videos. It uses YOLOv11-pose for automatic detection and adds an interactive interface for manual adjustments.
What it Does
The script (track."><meta name=author content="Jake Boggs"><link rel=canonical href=https://boggs.tech/posts/gait-analysis-physical-therapy-research/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://boggs.tech/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://boggs.tech/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://boggs.tech/favicon-32x32.png><link rel=apple-touch-icon href=https://boggs.tech/apple-touch-icon.png><link rel=mask-icon href=https://boggs.tech/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://boggs.tech/posts/gait-analysis-physical-therapy-research/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Gait Analysis for Physical Therapy with YOLOv11"><meta property="og:description" content="Analyzing how people walk using video is common in research and clinical settings, but getting accurate joint angles usually means either expensive equipment or manually annotating frames, which is slow and tedious.
Over Easter weekend, I built a Python tool to help my mother with her research study analyzing patient videos. It uses YOLOv11-pose for automatic detection and adds an interactive interface for manual adjustments.
What it Does
The script (track."><meta property="og:type" content="article"><meta property="og:url" content="https://boggs.tech/posts/gait-analysis-physical-therapy-research/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-21T00:00:00+00:00"><meta property="article:modified_time" content="2025-04-21T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Gait Analysis for Physical Therapy with YOLOv11"><meta name=twitter:description content="Analyzing how people walk using video is common in research and clinical settings, but getting accurate joint angles usually means either expensive equipment or manually annotating frames, which is slow and tedious.
Over Easter weekend, I built a Python tool to help my mother with her research study analyzing patient videos. It uses YOLOv11-pose for automatic detection and adds an interactive interface for manual adjustments.
What it Does
The script (track."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://boggs.tech/posts/"},{"@type":"ListItem","position":2,"name":"Gait Analysis for Physical Therapy with YOLOv11","item":"https://boggs.tech/posts/gait-analysis-physical-therapy-research/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Gait Analysis for Physical Therapy with YOLOv11","name":"Gait Analysis for Physical Therapy with YOLOv11","description":"Analyzing how people walk using video is common in research and clinical settings, but getting accurate joint angles usually means either expensive equipment or manually annotating frames, which is slow and tedious.\nOver Easter weekend, I built a Python tool to help my mother with her research study analyzing patient videos. It uses YOLOv11-pose for automatic detection and adds an interactive interface for manual adjustments.\nWhat it Does\nThe script (track.","keywords":[],"articleBody":"Analyzing how people walk using video is common in research and clinical settings, but getting accurate joint angles usually means either expensive equipment or manually annotating frames, which is slow and tedious.\nOver Easter weekend, I built a Python tool to help my mother with her research study analyzing patient videos. It uses YOLOv11-pose for automatic detection and adds an interactive interface for manual adjustments.\nWhat it Does\nThe script (track.py) lets you load a video file (ideally a side-view of someone walking). Here’s the workflow:\nLoad \u0026 Play: Open the video. You can play/pause with Space and move frame-by-frame using the Left/Right arrow keys or p/n. Automatic Detection: When the video is paused or you step to a new frame, the script runs YOLOv11-pose on the frame using the ultralytics library. # Simplified: Loading the model model = YOLO(args.model) # e.g., YOLO(\"yolov11l-pose.pt\") # Simplified: Running detection on a frame (or crop) results = model(current_frame_or_crop, verbose=False)[0] keypoints_data = results.keypoints.data Key Points: It identifies the main person and calculates 5 key points relevant for side-view gait from the keypoints_data. For example, calculating the hip center: # kps holds the keypoints for the detected person Lh = kps[KP['left_hip']][:2] # Get x,y for left hip Rh = kps[KP['right_hip']][:2] # Get x,y for right hip H = ((Lh+Rh)/2).tolist() # Calculate the midpoint And estimating the toe position based on the knee (Kpt) and ankle (Apt): # Simplified Toe Calculation (vector math) Kpt_np, Apt_np = np.array(Kpt), np.array(Apt) leg_vector_ka = Apt_np - Kpt_np # Calculate a perpendicular vector based on side perp_vector = np.array([-leg_vector_ka[1], leg_vector_ka[0]]) # Example for left side # Offset from ankle, half the length of the knee-ankle segment T_np = Apt_np + 0.5 * perp_vector T = T_np.tolist() These points (S, H, K, A, T) are drawn on the video frame. Angle Calculation: It also calculates and displays three angles using vector math with numpy. # Simplified angle calculation within recompute_angles() S, H, K, A, T = [np.array(p) for p in points] # Calculate vectors (e.g., from Hip to Shoulder, Hip to Knee) vec_HS = S - H vec_HK = K - H # Use dot product formula (via angle_between helper) for angle angle_SHK_internal = angle_between(vec_HS, vec_HK) # Custom helper function angles[0] = 180.0 - angle_SHK_internal # Store final angle # ... similar calculations for HKA and KAT ... Interactive Correction: This is the key part. If the automatically detected points aren’t quite right, you can click and drag them directly on the video frame. This uses an OpenCV mouse callback: # Simplified mouse callback logic def mouse_cb(event, x, y, flags, param): global points, dragging, drag_idx if event == cv2.EVENT_LBUTTONDOWN: # Check if click is near a point, set dragging=True, drag_idx=i ... elif event == cv2.EVENT_MOUSEMOVE and dragging: points[drag_idx] = (x,y) # Update point position recompute_angles() # Recalculate angles draw_main_window() # Redraw the display elif event == cv2.EVENT_LBUTTONUP: dragging = False ... cv2.setMouseCallback(window_name, mouse_cb) # Register the callback Save Data: When you’re satisfied with the point placement on a given frame (while paused), press the ’s’ key. This triggers the saving process detailed below. Demo\nYour browser does not support the video tag.\rOutput: Data Ready for Analysis\nWhen you run the script, you specify an output directory (or it defaults to one named output). Inside that directory, the script automatically creates a timestamped subfolder for each video analysis session. For example, analyzing my_gait_video.mp4 might create a folder like:\noutput/my_gait_video_20231028_163045/\nThis keeps results from different runs or videos organized. Inside this timestamped folder, you’ll find:\nCropped Images (PNGs): Every time you press ’s’, a PNG file is saved, named like my_gait_video_0123.png (where 0123 is the frame number). This isn’t the whole video frame, but a cropped image focused on the detected person’s bounding box. The 5 points (S, H, K, A, T) and the calculated angles are drawn directly onto this image. These are useful for visual checks, qualitative assessments, or including specific examples in reports. Angle Data (CSV): A single CSV file, named after the original video (e.g., my_gait_video.csv), is created for the session. Each time you press ’s’, a new row is added to this file. The columns are: Frame, S-H-K, H-K-A, K-A-T So, a row might look like: 123, 165.23, 170.51, 85.90 Using the Output for Research\nThe key output for quantitative research is the CSV file. Because it’s a standard format:\nSpreadsheet Friendly: You can directly open the .csv file in software like Microsoft Excel, Google Sheets, LibreOffice Calc, etc. The data will appear in columns (Frame, S-H-K angle, H-K-A angle, K-A-T angle). Easy Analysis: Once in a spreadsheet, you can easily sort by frame, calculate average angles, standard deviations, min/max values, or create plots of angles over time. Statistical Software: The CSV format is readily imported into statistical packages like R, SPSS, or using Python libraries like Pandas. This allows for more advanced statistical analysis, comparing different subjects or conditions, and generating publication-quality graphs. The combination of visual confirmation (PNGs) and structured numerical data (CSV) makes it easier to process video data for research purposes and link back to the source for verification.\nImportant Note on Privacy: The YOLO model needs to be downloaded on first use, but after that, all video processing and analysis happens locally on your machine. No video data is sent anywhere.\nHow to Use It\nGet the code: It’s available on GitHub: https://github.com/JakeBoggs/Gait-Analyzer Run from terminal: Assuming you have Python and the necessary libraries set up, navigate to the code directory and run: python track.py --video --output Replace with the actual file path and with where you want the main output directory created. Check the README on GitHub for other options (like choosing the YOLO model). ","wordCount":"947","inLanguage":"en","datePublished":"2025-04-21T00:00:00Z","dateModified":"2025-04-21T00:00:00Z","author":{"@type":"Person","name":"Jake Boggs"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://boggs.tech/posts/gait-analysis-physical-therapy-research/"},"publisher":{"@type":"Organization","name":"Jake Boggs","logo":{"@type":"ImageObject","url":"https://boggs.tech/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://boggs.tech/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://boggs.tech/about title="About me"><span>About me</span></a></li><li><a href=https://boggs.tech/startups title=Startups><span>Startups</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Gait Analysis for Physical Therapy with YOLOv11</h1><div class=post-meta><span title='2025-04-21 00:00:00 +0000 UTC'>April 21, 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;947 words&nbsp;·&nbsp;Jake Boggs</div></header><div class=post-content><p>Analyzing how people walk using video is common in research and clinical settings, but getting accurate joint angles usually means either expensive equipment or manually annotating frames, which is slow and tedious.</p><p>Over Easter weekend, I built a Python tool to help my mother with her research study analyzing patient videos. It uses YOLOv11-pose for automatic detection and adds an interactive interface for manual adjustments.</p><p><strong>What it Does</strong></p><p>The script (<code>track.py</code>) lets you load a video file (ideally a side-view of someone walking). Here’s the workflow:</p><ol><li><strong>Load & Play:</strong> Open the video. You can play/pause with <code>Space</code> and move frame-by-frame using the <code>Left</code>/<code>Right</code> arrow keys or <code>p</code>/<code>n</code>.</li><li><strong>Automatic Detection:</strong> When the video is paused or you step to a new frame, the script runs YOLOv11-pose on the frame using the <code>ultralytics</code> library.<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Simplified: Loading the model</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> YOLO(args<span style=color:#f92672>.</span>model) <span style=color:#75715e># e.g., YOLO(&#34;yolov11l-pose.pt&#34;)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Simplified: Running detection on a frame (or crop)</span>
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> model(current_frame_or_crop, verbose<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>keypoints_data <span style=color:#f92672>=</span> results<span style=color:#f92672>.</span>keypoints<span style=color:#f92672>.</span>data
</span></span></code></pre></div></li><li><strong>Key Points:</strong> It identifies the main person and calculates 5 key points relevant for side-view gait from the <code>keypoints_data</code>. For example, calculating the hip center:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># kps holds the keypoints for the detected person</span>
</span></span><span style=display:flex><span>Lh <span style=color:#f92672>=</span> kps[KP[<span style=color:#e6db74>&#39;left_hip&#39;</span>]][:<span style=color:#ae81ff>2</span>] <span style=color:#75715e># Get x,y for left hip</span>
</span></span><span style=display:flex><span>Rh <span style=color:#f92672>=</span> kps[KP[<span style=color:#e6db74>&#39;right_hip&#39;</span>]][:<span style=color:#ae81ff>2</span>] <span style=color:#75715e># Get x,y for right hip</span>
</span></span><span style=display:flex><span>H <span style=color:#f92672>=</span> ((Lh<span style=color:#f92672>+</span>Rh)<span style=color:#f92672>/</span><span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>tolist() <span style=color:#75715e># Calculate the midpoint</span>
</span></span></code></pre></div>And estimating the toe position based on the knee (Kpt) and ankle (Apt):<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Simplified Toe Calculation (vector math)</span>
</span></span><span style=display:flex><span>Kpt_np, Apt_np <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(Kpt), np<span style=color:#f92672>.</span>array(Apt)
</span></span><span style=display:flex><span>leg_vector_ka <span style=color:#f92672>=</span> Apt_np <span style=color:#f92672>-</span> Kpt_np
</span></span><span style=display:flex><span><span style=color:#75715e># Calculate a perpendicular vector based on side</span>
</span></span><span style=display:flex><span>perp_vector <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#f92672>-</span>leg_vector_ka[<span style=color:#ae81ff>1</span>], leg_vector_ka[<span style=color:#ae81ff>0</span>]]) <span style=color:#75715e># Example for left side</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Offset from ankle, half the length of the knee-ankle segment</span>
</span></span><span style=display:flex><span>T_np <span style=color:#f92672>=</span> Apt_np <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> perp_vector
</span></span><span style=display:flex><span>T <span style=color:#f92672>=</span> T_np<span style=color:#f92672>.</span>tolist()
</span></span></code></pre></div>These points (S, H, K, A, T) are drawn on the video frame.</li><li><strong>Angle Calculation:</strong> It also calculates and displays three angles using vector math with <code>numpy</code>.<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Simplified angle calculation within recompute_angles()</span>
</span></span><span style=display:flex><span>S, H, K, A, T <span style=color:#f92672>=</span> [np<span style=color:#f92672>.</span>array(p) <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> points]
</span></span><span style=display:flex><span><span style=color:#75715e># Calculate vectors (e.g., from Hip to Shoulder, Hip to Knee)</span>
</span></span><span style=display:flex><span>vec_HS <span style=color:#f92672>=</span> S <span style=color:#f92672>-</span> H
</span></span><span style=display:flex><span>vec_HK <span style=color:#f92672>=</span> K <span style=color:#f92672>-</span> H
</span></span><span style=display:flex><span><span style=color:#75715e># Use dot product formula (via angle_between helper) for angle</span>
</span></span><span style=display:flex><span>angle_SHK_internal <span style=color:#f92672>=</span> angle_between(vec_HS, vec_HK) <span style=color:#75715e># Custom helper function</span>
</span></span><span style=display:flex><span>angles[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>180.0</span> <span style=color:#f92672>-</span> angle_SHK_internal <span style=color:#75715e># Store final angle</span>
</span></span><span style=display:flex><span><span style=color:#75715e># ... similar calculations for HKA and KAT ...</span>
</span></span></code></pre></div></li><li><strong>Interactive Correction:</strong> This is the key part. If the automatically detected points aren&rsquo;t quite right, you can <strong>click and drag</strong> them directly on the video frame. This uses an OpenCV mouse callback:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Simplified mouse callback logic</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>mouse_cb</span>(event, x, y, flags, param):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>global</span> points, dragging, drag_idx
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> event <span style=color:#f92672>==</span> cv2<span style=color:#f92672>.</span>EVENT_LBUTTONDOWN:
</span></span><span style=display:flex><span>        <span style=color:#75715e># Check if click is near a point, set dragging=True, drag_idx=i</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>elif</span> event <span style=color:#f92672>==</span> cv2<span style=color:#f92672>.</span>EVENT_MOUSEMOVE <span style=color:#f92672>and</span> dragging:
</span></span><span style=display:flex><span>        points[drag_idx] <span style=color:#f92672>=</span> (x,y) <span style=color:#75715e># Update point position</span>
</span></span><span style=display:flex><span>        recompute_angles()      <span style=color:#75715e># Recalculate angles</span>
</span></span><span style=display:flex><span>        draw_main_window()      <span style=color:#75715e># Redraw the display</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>elif</span> event <span style=color:#f92672>==</span> cv2<span style=color:#f92672>.</span>EVENT_LBUTTONUP:
</span></span><span style=display:flex><span>        dragging <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>cv2<span style=color:#f92672>.</span>setMouseCallback(window_name, mouse_cb) <span style=color:#75715e># Register the callback</span>
</span></span></code></pre></div></li><li><strong>Save Data:</strong> When you&rsquo;re satisfied with the point placement on a given frame (while paused), press the <strong>&rsquo;s&rsquo;</strong> key. This triggers the saving process detailed below.</li></ol><p><strong>Demo</strong></p><video width=100% controls>
<source src=/videos/gait-analyzer.mp4 type=video/mp4>Your browser does not support the video tag.</video><p><strong>Output: Data Ready for Analysis</strong></p><p>When you run the script, you specify an output directory (or it defaults to one named <code>output</code>). Inside that directory, the script automatically creates a <strong>timestamped subfolder</strong> for each video analysis session. For example, analyzing <code>my_gait_video.mp4</code> might create a folder like:</p><p><code>output/my_gait_video_20231028_163045/</code></p><p>This keeps results from different runs or videos organized. Inside this timestamped folder, you&rsquo;ll find:</p><ul><li><strong>Cropped Images (PNGs):</strong> Every time you press &rsquo;s&rsquo;, a PNG file is saved, named like <code>my_gait_video_0123.png</code> (where <code>0123</code> is the frame number). This isn&rsquo;t the whole video frame, but a cropped image focused on the detected person&rsquo;s bounding box. The 5 points (S, H, K, A, T) and the calculated angles are drawn directly onto this image. These are useful for visual checks, qualitative assessments, or including specific examples in reports.</li><li><strong>Angle Data (CSV):</strong> A single CSV file, named after the original video (e.g., <code>my_gait_video.csv</code>), is created for the session. Each time you press &rsquo;s&rsquo;, a new row is added to this file. The columns are:
<code>Frame, S-H-K, H-K-A, K-A-T</code>
So, a row might look like: <code>123, 165.23, 170.51, 85.90</code></li></ul><p><strong>Using the Output for Research</strong></p><p>The key output for quantitative research is the <strong>CSV file</strong>. Because it&rsquo;s a standard format:</p><ul><li><strong>Spreadsheet Friendly:</strong> You can directly open the <code>.csv</code> file in software like Microsoft Excel, Google Sheets, LibreOffice Calc, etc. The data will appear in columns (Frame, S-H-K angle, H-K-A angle, K-A-T angle).</li><li><strong>Easy Analysis:</strong> Once in a spreadsheet, you can easily sort by frame, calculate average angles, standard deviations, min/max values, or create plots of angles over time.</li><li><strong>Statistical Software:</strong> The CSV format is readily imported into statistical packages like R, SPSS, or using Python libraries like Pandas. This allows for more advanced statistical analysis, comparing different subjects or conditions, and generating publication-quality graphs.</li></ul><p>The combination of visual confirmation (PNGs) and structured numerical data (CSV) makes it easier to process video data for research purposes and link back to the source for verification.</p><p><strong>Important Note on Privacy:</strong> The YOLO model needs to be downloaded on first use, but after that, all video processing and analysis happens <strong>locally on your machine</strong>. No video data is sent anywhere.</p><p><strong>How to Use It</strong></p><ol><li><strong>Get the code:</strong> It&rsquo;s available on GitHub:
<a href=https://github.com/JakeBoggs/Gait-Analyzer>https://github.com/JakeBoggs/Gait-Analyzer</a></li><li><strong>Run from terminal:</strong> Assuming you have Python and the necessary libraries set up, navigate to the code directory and run:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python track.py --video &lt;path_to_your_video.mp4&gt; --output &lt;your_results_folder&gt;
</span></span></code></pre></div>Replace <code>&lt;path_to_your_video.mp4></code> with the actual file path and <code>&lt;your_results_folder></code> with where you want the main output directory created. Check the README on GitHub for other options (like choosing the YOLO model).</li></ol></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://boggs.tech/>Jake Boggs</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>