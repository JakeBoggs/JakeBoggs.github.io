<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=author content="Jake Boggs"><title>Large Language Models for Magic: the Gathering | Jake Boggs</title><link rel=stylesheet href=/css/main.min.css><link rel=icon href=/favicon.png><link rel=canonical href=https://boggs.tech/posts/large-language-models-for-magic-the-gathering/></head><body><header><div class=container><a href=https://boggs.tech/ class=site-title>Jake Boggs</a><nav><a href=/about>About me</a><a href=/startups>Startups</a></nav></div></header><main><div class=container><article><h1>Large Language Models for Magic: the Gathering</h1><div class=article-meta><span>May 24, 2024</span>
<span>1482 words</span>
<span>7 min read</span></div><div class=article-content><p>Context: this was a project from one of my classes. I dumped the content from my final paper here with some slight tweaks to make a blog post.</p><h2 id=introduction>Introduction</h2><p>Magic: The Gathering (MTG) has always fascinated me with its complexity and depth, thanks to its extensive rulebook and vast array of unique cards. Despite the game&rsquo;s popularity, AI systems specifically designed for MTG have been few and far between, often falling short due to their inability to accurately interpret the intricate rules and interactions between cards. This blog post chronicles my recent endeavor to bridge this gap with large language models (LLMs) by creating a specialized dataset and evaluation metric to improve AI performance in MTG-related tasks.</p><p>MTG presents two primary challenges for players: deck construction and in-game decision-making. With over 27,000 unique cards and a rulebook nearing 300 pages, understanding card interactions and making optimal plays can be daunting. Current AI models often struggle with these aspects, leading to frequent hallucinations and misunderstandings.</p><p>To address these challenges, I created a custom dataset of MTG-related question-answer pairs, along with an evaluation metric named MTG-Eval. This dataset aims to train and assess language models on their understanding of MTG rules and card interactions. The dataset is divided into three categories:</p><ol><li><p><strong>Card Descriptions</strong>: Questions like &ldquo;What does card X do?&rdquo; with answers formatted to mimic card rules text. This helps the model reduce hallucinations by familiarizing it with the text of each card. The card information was structured programmatically like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-javascript data-lang=javascript><span style=display:flex><span><span style=color:#75715e>// From generateDescriptions.js
</span></span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> <span style=color:#a6e22e>createDescription</span>(<span style=color:#a6e22e>card</span>) {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> <span style=color:#a6e22e>description</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>`Name: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>name</span><span style=color:#e6db74>}</span><span style=color:#e6db74>`</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>manaCost</span>)
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>description</span> <span style=color:#f92672>+=</span> <span style=color:#e6db74>`Mana Cost: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>manaCost</span><span style=color:#e6db74>}</span><span style=color:#e6db74>`</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>description</span> <span style=color:#f92672>+=</span> <span style=color:#e6db74>`Type: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>type</span><span style=color:#e6db74>}</span><span style=color:#e6db74>`</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>power</span> <span style=color:#f92672>!==</span> <span style=color:#66d9ef>undefined</span>)
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>description</span> <span style=color:#f92672>+=</span> <span style=color:#e6db74>`Power/Toughness: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>power</span><span style=color:#e6db74>}</span><span style=color:#e6db74>/</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>toughness</span><span style=color:#e6db74>}</span><span style=color:#e6db74>`</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>loyalty</span>)
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>description</span> <span style=color:#f92672>+=</span> <span style=color:#e6db74>`Loyalty: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>loyalty</span><span style=color:#e6db74>}</span><span style=color:#e6db74>`</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>text</span>)
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>description</span> <span style=color:#f92672>+=</span> <span style=color:#e6db74>`Abilities: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>text</span><span style=color:#e6db74>}</span><span style=color:#e6db74>`</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Replace em dashes with standard hyphens for consistency
</span></span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#a6e22e>description</span>.<span style=color:#a6e22e>replace</span>(<span style=color:#e6db74>/—/g</span>, <span style=color:#e6db74>&#39;-&#39;</span>);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></li><li><p><strong>Rules Questions</strong>: Derived from rulings by the MTG Rules Committee, these questions clarify niche interactions and game scenarios. The official rulings serve as ground-truth answers. GPT-3.5 was prompted to reformat these rulings into questions:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-javascript data-lang=javascript><span style=display:flex><span><span style=color:#75715e>// From generateRulings.js - Inside the generation loop
</span></span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>completion</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>openai</span>.<span style=color:#a6e22e>chat</span>.<span style=color:#a6e22e>completions</span>.<span style=color:#a6e22e>create</span>({
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>messages</span><span style=color:#f92672>:</span> [
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>role</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;system&#39;</span>,
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>content</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;You are a helpful assistant designed to output JSON.&#39;</span>,
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>role</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;user&#39;</span>,
</span></span><span style=display:flex><span>            <span style=color:#75715e>// Dynamically insert card description and ruling
</span></span></span><span style=display:flex><span>            <span style=color:#a6e22e>content</span><span style=color:#f92672>:</span> <span style=color:#e6db74>`Below is a Magic: the Gathering card and an official ruling associated with it. Reformat the ruling into a simple question.\n</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>createDescription</span>(<span style=color:#a6e22e>cards</span>[<span style=color:#a6e22e>rulings</span>[<span style=color:#a6e22e>ruling</span>].<span style=color:#a6e22e>name</span>][<span style=color:#ae81ff>0</span>])<span style=color:#e6db74>}</span><span style=color:#e6db74>\nRuling: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>rulings</span>[<span style=color:#a6e22e>ruling</span>].<span style=color:#a6e22e>ruling</span><span style=color:#e6db74>}</span><span style=color:#e6db74>\nRespond in the following JSON format: {&#34;question&#34;: &#34;INSERT QUESTION HERE&#34;}`</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>model</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;gpt-3.5-turbo-0125&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>response_format</span><span style=color:#f92672>:</span> { <span style=color:#a6e22e>type</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;json_object&#39;</span> }
</span></span><span style=display:flex><span>});
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>data</span>.<span style=color:#a6e22e>push</span>({
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>instruction</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>JSON</span>.<span style=color:#a6e22e>parse</span>(<span style=color:#a6e22e>completion</span>.<span style=color:#a6e22e>choices</span>[<span style=color:#ae81ff>0</span>].<span style=color:#a6e22e>message</span>.<span style=color:#a6e22e>content</span>).<span style=color:#a6e22e>question</span>,
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>response</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>ruling</span>, <span style=color:#75715e>// The original ruling text serves as the answer
</span></span></span><span style=display:flex><span>    <span style=color:#a6e22e>category</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;ruling&#39;</span>
</span></span><span style=display:flex><span>});
</span></span></code></pre></div></li><li><p><strong>Card Interactions</strong>: Involves questions about combos and card synergies, such as &ldquo;What is a combo with card X?&rdquo; or &ldquo;How can I achieve Y?&rdquo; The data for this category comes from <a href=https://commanderspellbook.com/>Commander Spellbook</a>, a comprehensive MTG combo database. The prompt guided the LLM to create conversational questions and detailed answers based on the combo data:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-javascript data-lang=javascript><span style=display:flex><span><span style=color:#75715e>// From generateInteractions.js - Inside the generation loop
</span></span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>completion</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>openai</span>.<span style=color:#a6e22e>chat</span>.<span style=color:#a6e22e>completions</span>.<span style=color:#a6e22e>create</span>({
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>messages</span><span style=color:#f92672>:</span> [
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>role</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;system&#39;</span>,
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>content</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;You are a helpful assistant designed to output JSON.&#39;</span>,
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>role</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;user&#39;</span>,
</span></span><span style=display:flex><span>            <span style=color:#75715e>// Provide combo details to the LLM
</span></span></span><span style=display:flex><span>            <span style=color:#a6e22e>content</span><span style=color:#f92672>:</span> <span style=color:#e6db74>`I will give you the steps of Magic: the Gathering combo and you will create a question asking about a combo that can be performed with one of the required cards, along with the corresponding answer. You don&#39;t need to specify that the combo is about Magic in the question. When answering, use conversational language and first explain what other cards are required, then describe all of the steps needed to perform the combo, as well as the results. Be concise, but don&#39;t leave out any steps. Respond in JSON in the following format: {&#34;question&#34;: &#34;[QUESTION]&#34;, &#34;answer&#34;: &#34;[ANSWER]&#34;}\nCards required: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>combo</span>.<span style=color:#a6e22e>uses</span>.<span style=color:#a6e22e>map</span>(<span style=color:#a6e22e>x</span> =&gt; <span style=color:#a6e22e>x</span>.<span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>name</span>).<span style=color:#a6e22e>join</span>(<span style=color:#e6db74>&#39;, &#39;</span>)<span style=color:#e6db74>}</span><span style=color:#e6db74>\n</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>combo</span>.<span style=color:#a6e22e>otherPrerequisites</span>.<span style=color:#a6e22e>length</span> <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>?</span> <span style=color:#e6db74>&#39;Prerequisites: &#39;</span> <span style=color:#f92672>+</span> <span style=color:#a6e22e>combo</span>.<span style=color:#a6e22e>otherPrerequisites</span> <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;\n&#39;</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;&#39;</span><span style=color:#e6db74>}</span><span style=color:#e6db74>Combo steps:\n</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>combo</span>.<span style=color:#a6e22e>description</span><span style=color:#e6db74>}</span><span style=color:#e6db74>\nResults:\n</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>combo</span>.<span style=color:#a6e22e>produces</span>.<span style=color:#a6e22e>map</span>(<span style=color:#a6e22e>x</span> =&gt; <span style=color:#a6e22e>x</span>.<span style=color:#a6e22e>name</span>).<span style=color:#a6e22e>join</span>(<span style=color:#e6db74>&#39;, &#39;</span>)<span style=color:#e6db74>}</span><span style=color:#e6db74>`</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>model</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;gpt-3.5-turbo-0125&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>response_format</span><span style=color:#f92672>:</span> { <span style=color:#a6e22e>type</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;json_object&#39;</span> }
</span></span><span style=display:flex><span>});
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>data</span>.<span style=color:#a6e22e>push</span>({
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>instruction</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>JSON</span>.<span style=color:#a6e22e>parse</span>(<span style=color:#a6e22e>completion</span>.<span style=color:#a6e22e>choices</span>[<span style=color:#ae81ff>0</span>].<span style=color:#a6e22e>message</span>.<span style=color:#a6e22e>content</span>).<span style=color:#a6e22e>question</span>,
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>response</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>JSON</span>.<span style=color:#a6e22e>parse</span>(<span style=color:#a6e22e>completion</span>.<span style=color:#a6e22e>choices</span>[<span style=color:#ae81ff>0</span>].<span style=color:#a6e22e>message</span>.<span style=color:#a6e22e>content</span>).<span style=color:#a6e22e>answer</span>,
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>category</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;combo&#39;</span>
</span></span><span style=display:flex><span>});
</span></span></code></pre></div></li></ol><h2 id=methodology>Methodology</h2><h3 id=data-generation>Data Generation</h3><p>Data from <a href=https://mtgjson.com/>MTGJSON</a> and Commander Spellbook was used to generate over 80,000 question-answer pairs. The generation process involved using ChatGPT 3.5 to reformat existing data into conversational questions and answers, as shown in the snippets above. This synthetic dataset covers a wide range of possible game states and interactions, providing a robust foundation for training.</p><h3 id=training-process>Training Process</h3><p>I fine-tuned Llama 3 8B Instruct, an open-source conversational LLM from Meta, using the custom dataset. The training employed QLoRA (Quantized Low-Rank Adaptation) to minimize computational requirements. Here&rsquo;s the configuration used:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># From train.py - QLoRA and PEFT configuration</span>
</span></span><span style=display:flex><span>bnb_config <span style=color:#f92672>=</span> BitsAndBytesConfig(
</span></span><span style=display:flex><span>    load_in_4bit<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    bnb_4bit_quant_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;nf4&#34;</span>,
</span></span><span style=display:flex><span>    bnb_4bit_compute_dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float16,
</span></span><span style=display:flex><span>    bnb_4bit_use_double_quant<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>peft_config <span style=color:#f92672>=</span> LoraConfig(
</span></span><span style=display:flex><span>    r<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>,
</span></span><span style=display:flex><span>    lora_alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>,
</span></span><span style=display:flex><span>    lora_dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0.05</span>,
</span></span><span style=display:flex><span>    bias<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;none&#34;</span>,
</span></span><span style=display:flex><span>    task_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;CAUSAL_LM&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#75715e># Target specific layers for LoRA adaptation</span>
</span></span><span style=display:flex><span>    target_modules<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;up_proj&#39;</span>, <span style=color:#e6db74>&#39;down_proj&#39;</span>, <span style=color:#e6db74>&#39;gate_proj&#39;</span>, <span style=color:#e6db74>&#39;k_proj&#39;</span>, <span style=color:#e6db74>&#39;q_proj&#39;</span>, <span style=color:#e6db74>&#39;v_proj&#39;</span>, <span style=color:#e6db74>&#39;o_proj&#39;</span>]
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>The dataset was formatted into the required chat structure using the Hugging Face <code>transformers</code> library:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># From train.py - Formatting data for chat model</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>format_chat_template</span>(example):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Structure the data into user/assistant turns</span>
</span></span><span style=display:flex><span>    convos <span style=color:#f92672>=</span> [{
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;role&#39;</span>: <span style=color:#e6db74>&#39;user&#39;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;content&#39;</span>: example[<span style=color:#e6db74>&#39;instruction&#39;</span>]
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;role&#39;</span>: <span style=color:#e6db74>&#39;assistant&#39;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;content&#39;</span>: example[<span style=color:#e6db74>&#39;response&#39;</span>]
</span></span><span style=display:flex><span>    }]
</span></span><span style=display:flex><span>    <span style=color:#75715e># Apply the tokenizer&#39;s chat template</span>
</span></span><span style=display:flex><span>    texts <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>apply_chat_template(convos, tokenize<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, add_generation_prompt<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> { <span style=color:#e6db74>&#39;text&#39;</span> : texts }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dataset <span style=color:#f92672>=</span> dataset<span style=color:#f92672>.</span>map(format_chat_template)
</span></span></code></pre></div><p>The model was trained using the <code>SFTTrainer</code> from the <code>trl</code> library with the following arguments over 75 steps:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># From train.py - SFTTrainer setup</span>
</span></span><span style=display:flex><span>trainer <span style=color:#f92672>=</span> SFTTrainer(
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> model,
</span></span><span style=display:flex><span>    tokenizer <span style=color:#f92672>=</span> tokenizer,
</span></span><span style=display:flex><span>    train_dataset <span style=color:#f92672>=</span> dataset[<span style=color:#e6db74>&#39;train&#39;</span>],
</span></span><span style=display:flex><span>    eval_dataset <span style=color:#f92672>=</span> dataset[<span style=color:#e6db74>&#39;test&#39;</span>],
</span></span><span style=display:flex><span>    dataset_text_field <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;text&#39;</span>, <span style=color:#75715e># Field containing formatted chat data</span>
</span></span><span style=display:flex><span>    max_seq_length <span style=color:#f92672>=</span> <span style=color:#ae81ff>2048</span>,
</span></span><span style=display:flex><span>    dataset_num_proc <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>    packing <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>, <span style=color:#75715e># Important for chat format</span>
</span></span><span style=display:flex><span>    peft_config<span style=color:#f92672>=</span>peft_config,
</span></span><span style=display:flex><span>    args <span style=color:#f92672>=</span> TrainingArguments(
</span></span><span style=display:flex><span>        per_device_train_batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>        gradient_accumulation_steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>        warmup_steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>,
</span></span><span style=display:flex><span>        max_steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>75</span>, <span style=color:#75715e># Relatively short fine-tuning run</span>
</span></span><span style=display:flex><span>        learning_rate <span style=color:#f92672>=</span> <span style=color:#ae81ff>2e-4</span>,
</span></span><span style=display:flex><span>        fp16 <span style=color:#f92672>=</span> <span style=color:#f92672>not</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_bf16_supported(),
</span></span><span style=display:flex><span>        bf16 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_bf16_supported(),
</span></span><span style=display:flex><span>        logging_steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        optim <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;adamw_8bit&#39;</span>,
</span></span><span style=display:flex><span>        weight_decay <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.01</span>,
</span></span><span style=display:flex><span>        lr_scheduler_type <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;linear&#39;</span>,
</span></span><span style=display:flex><span>        output_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;outputs&#39;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>trainer<span style=color:#f92672>.</span>train()
</span></span><span style=display:flex><span>trainer<span style=color:#f92672>.</span>save_model(new_model)
</span></span></code></pre></div><h2 id=evaluation>Evaluation</h2><p>I evaluated the model&rsquo;s performance using a subset of the dataset reserved for testing. Both the base and fine-tuned models were assessed on their ability to answer questions from the card interactions and rules categories, which require deeper comprehension.</p><p>First, the model generated an answer for a given instruction:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># From evaluate.py - Generating model response</span>
</span></span><span style=display:flex><span>prompt <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>apply_chat_template([{
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;role&#39;</span>: <span style=color:#e6db74>&#39;user&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;content&#39;</span>: example[<span style=color:#e6db74>&#39;instruction&#39;</span>]
</span></span><span style=display:flex><span>}], tokenize<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, add_generation_prompt<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model_inputs <span style=color:#f92672>=</span> tokenizer(prompt, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>, add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>to(<span style=color:#e6db74>&#39;cuda&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>generated_ids <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>generate(<span style=color:#f92672>**</span>model_inputs, max_new_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>, do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, pad_token_id<span style=color:#f92672>=</span>tokenizer<span style=color:#f92672>.</span>eos_token_id, eos_token_id<span style=color:#f92672>=</span>[tokenizer<span style=color:#f92672>.</span>eos_token_id, tokenizer<span style=color:#f92672>.</span>convert_tokens_to_ids(<span style=color:#e6db74>&#34;&lt;|eot_id|&gt;&#34;</span>)])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>decoded <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>decode(generated_ids[<span style=color:#ae81ff>0</span>], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>student_answer <span style=color:#f92672>=</span> decoded<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;assistant&#34;</span>)[<span style=color:#ae81ff>1</span>] <span style=color:#75715e># Extract the generated answer</span>
</span></span></code></pre></div><p>Then, I used GPT-4 to score the answers on a scale of 1-5, providing consistent and efficient evaluations. GPT-4 was prompted with the original question, the ground-truth answer, and the model&rsquo;s generated answer:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># From evaluate.py - Payload for GPT-4 evaluation</span>
</span></span><span style=display:flex><span>payload <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;model&#34;</span>: <span style=color:#e6db74>&#34;gpt-4-turbo&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;response_format&#34;</span>: { <span style=color:#e6db74>&#34;type&#34;</span>: <span style=color:#e6db74>&#39;json_object&#39;</span> },
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;messages&#34;</span>: [
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;content&#34;</span>: [
</span></span><span style=display:flex><span>                {
</span></span><span style=display:flex><span>                    <span style=color:#e6db74>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;text&#34;</span>,
</span></span><span style=display:flex><span>                    <span style=color:#75715e># The prompt instructing GPT-4 how to grade</span>
</span></span><span style=display:flex><span>                    <span style=color:#e6db74>&#34;text&#34;</span>: <span style=color:#e6db74>&#34;You are an expert grader who will be given a Magic: the Gathering related question and correct answer, along with a students response. You rate the student&#39;s response on a scale of 1-5, with 5 being entirely correct and 1 being entirely incorrect. Accurately identify any issues with the answer and explain why the students response is entirely correct, partially correct, or entirely incorrect. Reply in the following JSON format: {</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>explanation</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>: </span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>[EXPLANATION]</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>, </span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>score</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>: SCORE}</span><span style=color:#ae81ff>\n\&#34;</span><span style=color:#e6db74>Question: &#34;</span><span style=color:#f92672>+</span> example[<span style=color:#e6db74>&#39;instruction&#39;</span>] <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Correct answer: &#34;</span><span style=color:#f92672>+</span> example[<span style=color:#e6db74>&#39;response&#39;</span>] <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Student&#39;s answer: &#34;</span> <span style=color:#f92672>+</span> student_answer
</span></span><span style=display:flex><span>                }
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>response <span style=color:#f92672>=</span> json<span style=color:#f92672>.</span>loads(requests<span style=color:#f92672>.</span>post(<span style=color:#e6db74>&#34;https://api.openai.com/v1/chat/completions&#34;</span>, headers<span style=color:#f92672>=</span>headers, json<span style=color:#f92672>=</span>payload)<span style=color:#f92672>.</span>json()[<span style=color:#e6db74>&#39;choices&#39;</span>][<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#39;message&#39;</span>][<span style=color:#e6db74>&#39;content&#39;</span>])
</span></span><span style=display:flex><span>score <span style=color:#f92672>=</span> float(response[<span style=color:#e6db74>&#39;score&#39;</span>])
</span></span></code></pre></div><h2 id=results>Results</h2><p>The fine-tuned model demonstrated a 10.5% improvement over the base model, with an average score increase from 1.62 to 1.79 based on the GPT-4 evaluation. This indicates a slight improvement in the model&rsquo;s understanding of MTG rules and interactions, but both models still have tremendous room to learn.</p><h2 id=future-directions>Future Directions</h2><p>Future research could explore the addition of custom tokens for special game symbols like mana and tapping, which are underrepresented in pre-training data and maybe not be appropriately tokenized. Additionally, expanding the dataset to include more diverse game scenarios and interactions could further refine the model&rsquo;s capabilities.</p><h2 id=conclusion>Conclusion</h2><p>This project showcases the potential of LLMs to enhance the MTG playing experience and the challenges that still need to be overcome to get there. I hope other people will find this dataset useful for training future models and build off my work. There&rsquo;s certainly much more that could be done and I can&rsquo;t wait for the day when AI systems can build good decks with my janky pet cards.</p><h2 id=acknowledgments>Acknowledgments</h2><p>Thanks to the team at Commander Spellbook for generously sharing their dataset, without which this project would not have been possible. All generated data is unofficial Fan Content permitted under the Fan Content Policy. Not approved/endorsed by Wizards. Portions of the materials used are property of Wizards of the Coast. ©Wizards of the Coast LLC.</p><p>For more details and access to the dataset and model, visit the following links:</p><ul><li><a href=https://huggingface.co/datasets/jakeboggs/MTG-Eval>MTG-Eval Dataset</a></li><li><a href=https://huggingface.co/jakeboggs/MTG-Llama>Fine-Tuned Model</a></li><li><a href=https://github.com/JakeBoggs/Large-Language-Models-for-Magic-the-Gathering>Training Code on GitHub</a></li></ul></div></article></div></main><footer><div class=container><p>&copy; 2026 Jake Boggs</p></div></footer></body></html>