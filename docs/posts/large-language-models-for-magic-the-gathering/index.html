<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Large Language Models for Magic: the Gathering | Jake Boggs</title>
<meta name=keywords content><meta name=description content="Magic: The Gathering (MTG) has always fascinated me with its complexity and strategic depth, thanks to its extensive rulebook and vast array of unique cards. Despite the game&rsquo;s popularity, AI systems specifically designed for MTG have been few and far between, often falling short due to their inability to accurately interpret the intricate rules and interactions between cards. This blog post chronicles my recent endeavor to bridge this gap with large language models (LLMs) by creating a specialized dataset and evaluation metric to improve AI performance in MTG-related tasks."><meta name=author content="Jake Boggs"><link rel=canonical href=https://boggs.tech/posts/large-language-models-for-magic-the-gathering/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://boggs.tech/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://boggs.tech/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://boggs.tech/favicon-32x32.png><link rel=apple-touch-icon href=https://boggs.tech/apple-touch-icon.png><link rel=mask-icon href=https://boggs.tech/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://boggs.tech/posts/large-language-models-for-magic-the-gathering/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Large Language Models for Magic: the Gathering"><meta property="og:description" content="Magic: The Gathering (MTG) has always fascinated me with its complexity and strategic depth, thanks to its extensive rulebook and vast array of unique cards. Despite the game&rsquo;s popularity, AI systems specifically designed for MTG have been few and far between, often falling short due to their inability to accurately interpret the intricate rules and interactions between cards. This blog post chronicles my recent endeavor to bridge this gap with large language models (LLMs) by creating a specialized dataset and evaluation metric to improve AI performance in MTG-related tasks."><meta property="og:type" content="article"><meta property="og:url" content="https://boggs.tech/posts/large-language-models-for-magic-the-gathering/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-24T00:00:00+00:00"><meta property="article:modified_time" content="2024-05-24T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Large Language Models for Magic: the Gathering"><meta name=twitter:description content="Magic: The Gathering (MTG) has always fascinated me with its complexity and strategic depth, thanks to its extensive rulebook and vast array of unique cards. Despite the game&rsquo;s popularity, AI systems specifically designed for MTG have been few and far between, often falling short due to their inability to accurately interpret the intricate rules and interactions between cards. This blog post chronicles my recent endeavor to bridge this gap with large language models (LLMs) by creating a specialized dataset and evaluation metric to improve AI performance in MTG-related tasks."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://boggs.tech/posts/"},{"@type":"ListItem","position":2,"name":"Large Language Models for Magic: the Gathering","item":"https://boggs.tech/posts/large-language-models-for-magic-the-gathering/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Large Language Models for Magic: the Gathering","name":"Large Language Models for Magic: the Gathering","description":"Magic: The Gathering (MTG) has always fascinated me with its complexity and strategic depth, thanks to its extensive rulebook and vast array of unique cards. Despite the game\u0026rsquo;s popularity, AI systems specifically designed for MTG have been few and far between, often falling short due to their inability to accurately interpret the intricate rules and interactions between cards. This blog post chronicles my recent endeavor to bridge this gap with large language models (LLMs) by creating a specialized dataset and evaluation metric to improve AI performance in MTG-related tasks.","keywords":[],"articleBody":"Magic: The Gathering (MTG) has always fascinated me with its complexity and strategic depth, thanks to its extensive rulebook and vast array of unique cards. Despite the game’s popularity, AI systems specifically designed for MTG have been few and far between, often falling short due to their inability to accurately interpret the intricate rules and interactions between cards. This blog post chronicles my recent endeavor to bridge this gap with large language models (LLMs) by creating a specialized dataset and evaluation metric to improve AI performance in MTG-related tasks.\nThe Challenge of MTG for AI MTG presents two primary challenges for players: deck construction and in-game decision-making. With over 27,000 unique cards and a rulebook nearing 300 pages, understanding card interactions and making optimal plays can be daunting. Current AI models often struggle with these aspects, leading to frequent hallucinations and misunderstandings.\nCustom Dataset and MTG-Eval Metric To address these challenges, I created a custom dataset of MTG-related question-answer pairs, along with an evaluation metric named MTG-Eval. This dataset aims to train and assess language models on their understanding of MTG rules and card interactions. The dataset is divided into three categories:\nCard Descriptions: Questions like “What does card X do?” with answers formatted to mimic card rules text. This helps the model reduce hallucinations by familiarizing it with the text of each card. The card information was structured programmatically like this:\n// From generateDescriptions.js function createDescription(card) { let description = `Name: ${card.name}`; if (card.manaCost) description += `Mana Cost: ${card.manaCost}`; description += `Type: ${card.type}`; if (card.power !== undefined) description += `Power/Toughness: ${card.power}/${card.toughness}`; if (card.loyalty) description += `Loyalty: ${card.loyalty}`; if (card.text) description += `Abilities: ${card.text}`; // Replace em dashes with standard hyphens for consistency return description.replace(/—/g, '-'); } Rules Questions: Derived from rulings by the MTG Rules Committee, these questions clarify niche interactions and game scenarios. The official rulings serve as ground-truth answers. GPT-3.5 was prompted to reformat these rulings into questions:\n// From generateRulings.js - Inside the generation loop const completion = await openai.chat.completions.create({ messages: [ { role: 'system', content: 'You are a helpful assistant designed to output JSON.', }, { role: 'user', // Dynamically insert card description and ruling content: `Below is a Magic: the Gathering card and an official ruling associated with it. Reformat the ruling into a simple question.\\n${createDescription(cards[rulings[ruling].name][0])}\\nRuling: ${rulings[ruling].ruling}\\nRespond in the following JSON format: {\"question\": \"INSERT QUESTION HERE\"}` } ], model: 'gpt-3.5-turbo-0125', response_format: { type: 'json_object' } }); data.push({ instruction: JSON.parse(completion.choices[0].message.content).question, response: ruling, // The original ruling text serves as the answer category: 'ruling' }); Card Interactions: Involves questions about combos and card synergies, such as “What is a combo with card X?” or “How can I achieve Y?” The data for this category comes from Commander Spellbook, a comprehensive MTG combo database. The prompt guided the LLM to create conversational questions and detailed answers based on the combo data:\n// From generateInteractions.js - Inside the generation loop const completion = await openai.chat.completions.create({ messages: [ { role: 'system', content: 'You are a helpful assistant designed to output JSON.', }, { role: 'user', // Provide combo details to the LLM content: `I will give you the steps of Magic: the Gathering combo and you will create a question asking about a combo that can be performed with one of the required cards, along with the corresponding answer. You don't need to specify that the combo is about Magic in the question. When answering, use conversational language and first explain what other cards are required, then describe all of the steps needed to perform the combo, as well as the results. Be concise, but don't leave out any steps. Respond in JSON in the following format: {\"question\": \"[QUESTION]\", \"answer\": \"[ANSWER]\"}\\nCards required: ${combo.uses.map(x =\u003e x.card.name).join(', ')}\\n${combo.otherPrerequisites.length \u003e 0 ? 'Prerequisites: ' + combo.otherPrerequisites + '\\n': ''}Combo steps:\\n${combo.description}\\nResults:\\n${combo.produces.map(x =\u003e x.name).join(', ')}` } ], model: 'gpt-3.5-turbo-0125', response_format: { type: 'json_object' } }); data.push({ instruction: JSON.parse(completion.choices[0].message.content).question, response: JSON.parse(completion.choices[0].message.content).answer, category: 'combo' }); Methodology Data Generation Data from MTGJSON and Commander Spellbook was used to generate over 80,000 question-answer pairs. The generation process involved using ChatGPT 3.5 to reformat existing data into conversational questions and answers, as shown in the snippets above. This synthetic dataset covers a wide range of possible game states and interactions, providing a robust foundation for training.\nTraining Process I fine-tuned Llama 3 8B Instruct, an open-source conversational LLM from Meta, using the custom dataset. The training employed QLoRA (Quantized Low-Rank Adaptation) to minimize computational requirements. Here’s the configuration used:\n# From train.py - QLoRA and PEFT configuration bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True, ) peft_config = LoraConfig( r=64, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\", # Target specific layers for LoRA adaptation target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj'] ) The dataset was formatted into the required chat structure using the Hugging Face transformers library:\n# From train.py - Formatting data for chat model def format_chat_template(example): # Structure the data into user/assistant turns convos = [{ 'role': 'user', 'content': example['instruction'] }, { 'role': 'assistant', 'content': example['response'] }] # Apply the tokenizer's chat template texts = tokenizer.apply_chat_template(convos, tokenize=False, add_generation_prompt=False) return { 'text' : texts } dataset = dataset.map(format_chat_template) The model was trained using the SFTTrainer from the trl library with the following arguments over 75 steps:\n# From train.py - SFTTrainer setup trainer = SFTTrainer( model = model, tokenizer = tokenizer, train_dataset = dataset['train'], eval_dataset = dataset['test'], dataset_text_field = 'text', # Field containing formatted chat data max_seq_length = 2048, dataset_num_proc = 2, packing = False, # Important for chat format peft_config=peft_config, args = TrainingArguments( per_device_train_batch_size = 2, gradient_accumulation_steps = 4, warmup_steps = 5, max_steps = 75, # Relatively short fine-tuning run learning_rate = 2e-4, fp16 = not torch.cuda.is_bf16_supported(), bf16 = torch.cuda.is_bf16_supported(), logging_steps = 1, optim = 'adamw_8bit', weight_decay = 0.01, lr_scheduler_type = 'linear', output_dir='outputs' ) ) trainer.train() trainer.save_model(new_model) Evaluation I evaluated the model’s performance using a subset of the dataset reserved for testing. Both the base and fine-tuned models were assessed on their ability to answer questions from the card interactions and rules categories, which require deeper comprehension.\nFirst, the model generated an answer for a given instruction:\n# From evaluate.py - Generating model response prompt = tokenizer.apply_chat_template([{ 'role': 'user', 'content': example['instruction'] }], tokenize=False, add_generation_prompt=True) model_inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).to('cuda') generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id, eos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"\u003c|eot_id|\u003e\")]) decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True) student_answer = decoded.split(\"assistant\")[1] # Extract the generated answer Then, I used GPT-4 to score the answers on a scale of 1-5, providing consistent and efficient evaluations. GPT-4 was prompted with the original question, the ground-truth answer, and the model’s generated answer:\n# From evaluate.py - Payload for GPT-4 evaluation payload = { \"model\": \"gpt-4-turbo\", \"response_format\": { \"type\": 'json_object' }, \"messages\": [ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", # The prompt instructing GPT-4 how to grade \"text\": \"You are an expert grader who will be given a Magic: the Gathering related question and correct answer, along with a students response. You rate the student's response on a scale of 1-5, with 5 being entirely correct and 1 being entirely incorrect. Accurately identify any issues with the answer and explain why the students response is entirely correct, partially correct, or entirely incorrect. Reply in the following JSON format: {\\\"explanation\\\": \\\"[EXPLANATION]\\\", \\\"score\\\": SCORE}\\n\\\"Question: \"+ example['instruction'] + \"\\nCorrect answer: \"+ example['response'] + \"\\nStudent's answer: \" + student_answer } ] } ] } response = json.loads(requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload).json()['choices'][0]['message']['content']) score = float(response['score']) Results and Impact The fine-tuned model demonstrated a 10.5% improvement over the base model, with an average score increase from 1.62 to 1.79 based on the GPT-4 evaluation. This indicates a moderate improvement in the model’s understanding of MTG rules and interactions, but both models still have tremendous room to learn.\nFuture Directions Future research could explore the addition of custom tokens for special game symbols like mana and tapping, which are underrepresented in pre-training data and maybe not be appropriately tokenized. Additionally, expanding the dataset to include more diverse game scenarios and interactions could further refine the model’s capabilities.\nConclusion This project showcases the potential of LLMs to enhance the MTG playing experience and the challenges that still need to be overcome to get there. I hope other people will find this dataset useful for training future models and build off my work. There’s certainly much more that could be done and I can’t wait for the day when AI systems can build good decks with my janky pet cards.\nAcknowledgments Thanks to the team at Commander Spellbook for generously sharing their dataset, without which this project would not have been possible. All generated data is unofficial Fan Content permitted under the Fan Content Policy. Not approved/endorsed by Wizards. Portions of the materials used are property of Wizards of the Coast. ©Wizards of the Coast LLC.\nFor more details and access to the dataset and model, visit the following links:\nMTG-Eval Dataset Fine-Tuned Model Training Code on GitHub ","wordCount":"1467","inLanguage":"en","datePublished":"2024-05-24T00:00:00Z","dateModified":"2024-05-24T00:00:00Z","author":{"@type":"Person","name":"Jake Boggs"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://boggs.tech/posts/large-language-models-for-magic-the-gathering/"},"publisher":{"@type":"Organization","name":"Jake Boggs","logo":{"@type":"ImageObject","url":"https://boggs.tech/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://boggs.tech/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://boggs.tech/about title="About me"><span>About me</span></a></li><li><a href=https://boggs.tech/startups title=Startups><span>Startups</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Large Language Models for Magic: the Gathering</h1><div class=post-meta><span title='2024-05-24 00:00:00 +0000 UTC'>May 24, 2024</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1467 words&nbsp;·&nbsp;Jake Boggs</div></header><div class=post-content><p>Magic: The Gathering (MTG) has always fascinated me with its complexity and strategic depth, thanks to its extensive rulebook and vast array of unique cards. Despite the game&rsquo;s popularity, AI systems specifically designed for MTG have been few and far between, often falling short due to their inability to accurately interpret the intricate rules and interactions between cards. This blog post chronicles my recent endeavor to bridge this gap with large language models (LLMs) by creating a specialized dataset and evaluation metric to improve AI performance in MTG-related tasks.</p><h2 id=the-challenge-of-mtg-for-ai>The Challenge of MTG for AI<a hidden class=anchor aria-hidden=true href=#the-challenge-of-mtg-for-ai>#</a></h2><p>MTG presents two primary challenges for players: deck construction and in-game decision-making. With over 27,000 unique cards and a rulebook nearing 300 pages, understanding card interactions and making optimal plays can be daunting. Current AI models often struggle with these aspects, leading to frequent hallucinations and misunderstandings.</p><h2 id=custom-dataset-and-mtg-eval-metric>Custom Dataset and MTG-Eval Metric<a hidden class=anchor aria-hidden=true href=#custom-dataset-and-mtg-eval-metric>#</a></h2><p>To address these challenges, I created a custom dataset of MTG-related question-answer pairs, along with an evaluation metric named MTG-Eval. This dataset aims to train and assess language models on their understanding of MTG rules and card interactions. The dataset is divided into three categories:</p><ol><li><p><strong>Card Descriptions</strong>: Questions like &ldquo;What does card X do?&rdquo; with answers formatted to mimic card rules text. This helps the model reduce hallucinations by familiarizing it with the text of each card. The card information was structured programmatically like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-javascript data-lang=javascript><span style=display:flex><span><span style=color:#75715e>// From generateDescriptions.js
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>function</span> <span style=color:#a6e22e>createDescription</span>(<span style=color:#a6e22e>card</span>) {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> <span style=color:#a6e22e>description</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>`Name: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>name</span><span style=color:#e6db74>}</span><span style=color:#e6db74>`</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>manaCost</span>)
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>description</span> <span style=color:#f92672>+=</span> <span style=color:#e6db74>`Mana Cost: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>manaCost</span><span style=color:#e6db74>}</span><span style=color:#e6db74>`</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>description</span> <span style=color:#f92672>+=</span> <span style=color:#e6db74>`Type: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>type</span><span style=color:#e6db74>}</span><span style=color:#e6db74>`</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>power</span> <span style=color:#f92672>!==</span> <span style=color:#66d9ef>undefined</span>)
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>description</span> <span style=color:#f92672>+=</span> <span style=color:#e6db74>`Power/Toughness: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>power</span><span style=color:#e6db74>}</span><span style=color:#e6db74>/</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>toughness</span><span style=color:#e6db74>}</span><span style=color:#e6db74>`</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>loyalty</span>)
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>description</span> <span style=color:#f92672>+=</span> <span style=color:#e6db74>`Loyalty: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>loyalty</span><span style=color:#e6db74>}</span><span style=color:#e6db74>`</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>text</span>)
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>description</span> <span style=color:#f92672>+=</span> <span style=color:#e6db74>`Abilities: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>text</span><span style=color:#e6db74>}</span><span style=color:#e6db74>`</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Replace em dashes with standard hyphens for consistency
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>return</span> <span style=color:#a6e22e>description</span>.<span style=color:#a6e22e>replace</span>(<span style=color:#e6db74>/—/g</span>, <span style=color:#e6db74>&#39;-&#39;</span>);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></li><li><p><strong>Rules Questions</strong>: Derived from rulings by the MTG Rules Committee, these questions clarify niche interactions and game scenarios. The official rulings serve as ground-truth answers. GPT-3.5 was prompted to reformat these rulings into questions:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-javascript data-lang=javascript><span style=display:flex><span><span style=color:#75715e>// From generateRulings.js - Inside the generation loop
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>completion</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>openai</span>.<span style=color:#a6e22e>chat</span>.<span style=color:#a6e22e>completions</span>.<span style=color:#a6e22e>create</span>({
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>messages</span><span style=color:#f92672>:</span> [
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>role</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;system&#39;</span>,
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>content</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;You are a helpful assistant designed to output JSON.&#39;</span>,
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>role</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;user&#39;</span>,
</span></span><span style=display:flex><span>            <span style=color:#75715e>// Dynamically insert card description and ruling
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>            <span style=color:#a6e22e>content</span><span style=color:#f92672>:</span> <span style=color:#e6db74>`Below is a Magic: the Gathering card and an official ruling associated with it. Reformat the ruling into a simple question.\n</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>createDescription</span>(<span style=color:#a6e22e>cards</span>[<span style=color:#a6e22e>rulings</span>[<span style=color:#a6e22e>ruling</span>].<span style=color:#a6e22e>name</span>][<span style=color:#ae81ff>0</span>])<span style=color:#e6db74>}</span><span style=color:#e6db74>\nRuling: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>rulings</span>[<span style=color:#a6e22e>ruling</span>].<span style=color:#a6e22e>ruling</span><span style=color:#e6db74>}</span><span style=color:#e6db74>\nRespond in the following JSON format: {&#34;question&#34;: &#34;INSERT QUESTION HERE&#34;}`</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>model</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;gpt-3.5-turbo-0125&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>response_format</span><span style=color:#f92672>:</span> { <span style=color:#a6e22e>type</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;json_object&#39;</span> }
</span></span><span style=display:flex><span>});
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>data</span>.<span style=color:#a6e22e>push</span>({
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>instruction</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>JSON</span>.<span style=color:#a6e22e>parse</span>(<span style=color:#a6e22e>completion</span>.<span style=color:#a6e22e>choices</span>[<span style=color:#ae81ff>0</span>].<span style=color:#a6e22e>message</span>.<span style=color:#a6e22e>content</span>).<span style=color:#a6e22e>question</span>,
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>response</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>ruling</span>, <span style=color:#75715e>// The original ruling text serves as the answer
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#a6e22e>category</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;ruling&#39;</span>
</span></span><span style=display:flex><span>});
</span></span></code></pre></div></li><li><p><strong>Card Interactions</strong>: Involves questions about combos and card synergies, such as &ldquo;What is a combo with card X?&rdquo; or &ldquo;How can I achieve Y?&rdquo; The data for this category comes from <a href=https://commanderspellbook.com/>Commander Spellbook</a>, a comprehensive MTG combo database. The prompt guided the LLM to create conversational questions and detailed answers based on the combo data:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-javascript data-lang=javascript><span style=display:flex><span><span style=color:#75715e>// From generateInteractions.js - Inside the generation loop
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>completion</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>openai</span>.<span style=color:#a6e22e>chat</span>.<span style=color:#a6e22e>completions</span>.<span style=color:#a6e22e>create</span>({
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>messages</span><span style=color:#f92672>:</span> [
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>role</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;system&#39;</span>,
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>content</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;You are a helpful assistant designed to output JSON.&#39;</span>,
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>role</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;user&#39;</span>,
</span></span><span style=display:flex><span>            <span style=color:#75715e>// Provide combo details to the LLM
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>            <span style=color:#a6e22e>content</span><span style=color:#f92672>:</span> <span style=color:#e6db74>`I will give you the steps of Magic: the Gathering combo and you will create a question asking about a combo that can be performed with one of the required cards, along with the corresponding answer. You don&#39;t need to specify that the combo is about Magic in the question. When answering, use conversational language and first explain what other cards are required, then describe all of the steps needed to perform the combo, as well as the results. Be concise, but don&#39;t leave out any steps. Respond in JSON in the following format: {&#34;question&#34;: &#34;[QUESTION]&#34;, &#34;answer&#34;: &#34;[ANSWER]&#34;}\nCards required: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>combo</span>.<span style=color:#a6e22e>uses</span>.<span style=color:#a6e22e>map</span>(<span style=color:#a6e22e>x</span> =&gt; <span style=color:#a6e22e>x</span>.<span style=color:#a6e22e>card</span>.<span style=color:#a6e22e>name</span>).<span style=color:#a6e22e>join</span>(<span style=color:#e6db74>&#39;, &#39;</span>)<span style=color:#e6db74>}</span><span style=color:#e6db74>\n</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>combo</span>.<span style=color:#a6e22e>otherPrerequisites</span>.<span style=color:#a6e22e>length</span> <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>?</span> <span style=color:#e6db74>&#39;Prerequisites: &#39;</span> <span style=color:#f92672>+</span> <span style=color:#a6e22e>combo</span>.<span style=color:#a6e22e>otherPrerequisites</span> <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;\n&#39;</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;&#39;</span><span style=color:#e6db74>}</span><span style=color:#e6db74>Combo steps:\n</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>combo</span>.<span style=color:#a6e22e>description</span><span style=color:#e6db74>}</span><span style=color:#e6db74>\nResults:\n</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>combo</span>.<span style=color:#a6e22e>produces</span>.<span style=color:#a6e22e>map</span>(<span style=color:#a6e22e>x</span> =&gt; <span style=color:#a6e22e>x</span>.<span style=color:#a6e22e>name</span>).<span style=color:#a6e22e>join</span>(<span style=color:#e6db74>&#39;, &#39;</span>)<span style=color:#e6db74>}</span><span style=color:#e6db74>`</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>model</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;gpt-3.5-turbo-0125&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>response_format</span><span style=color:#f92672>:</span> { <span style=color:#a6e22e>type</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;json_object&#39;</span> }
</span></span><span style=display:flex><span>});
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>data</span>.<span style=color:#a6e22e>push</span>({
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>instruction</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>JSON</span>.<span style=color:#a6e22e>parse</span>(<span style=color:#a6e22e>completion</span>.<span style=color:#a6e22e>choices</span>[<span style=color:#ae81ff>0</span>].<span style=color:#a6e22e>message</span>.<span style=color:#a6e22e>content</span>).<span style=color:#a6e22e>question</span>,
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>response</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>JSON</span>.<span style=color:#a6e22e>parse</span>(<span style=color:#a6e22e>completion</span>.<span style=color:#a6e22e>choices</span>[<span style=color:#ae81ff>0</span>].<span style=color:#a6e22e>message</span>.<span style=color:#a6e22e>content</span>).<span style=color:#a6e22e>answer</span>,
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>category</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;combo&#39;</span>
</span></span><span style=display:flex><span>});
</span></span></code></pre></div></li></ol><h2 id=methodology>Methodology<a hidden class=anchor aria-hidden=true href=#methodology>#</a></h2><h3 id=data-generation>Data Generation<a hidden class=anchor aria-hidden=true href=#data-generation>#</a></h3><p>Data from <a href=https://mtgjson.com/>MTGJSON</a> and Commander Spellbook was used to generate over 80,000 question-answer pairs. The generation process involved using ChatGPT 3.5 to reformat existing data into conversational questions and answers, as shown in the snippets above. This synthetic dataset covers a wide range of possible game states and interactions, providing a robust foundation for training.</p><h3 id=training-process>Training Process<a hidden class=anchor aria-hidden=true href=#training-process>#</a></h3><p>I fine-tuned Llama 3 8B Instruct, an open-source conversational LLM from Meta, using the custom dataset. The training employed QLoRA (Quantized Low-Rank Adaptation) to minimize computational requirements. Here&rsquo;s the configuration used:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># From train.py - QLoRA and PEFT configuration</span>
</span></span><span style=display:flex><span>bnb_config <span style=color:#f92672>=</span> BitsAndBytesConfig(
</span></span><span style=display:flex><span>    load_in_4bit<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    bnb_4bit_quant_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;nf4&#34;</span>,
</span></span><span style=display:flex><span>    bnb_4bit_compute_dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float16,
</span></span><span style=display:flex><span>    bnb_4bit_use_double_quant<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>peft_config <span style=color:#f92672>=</span> LoraConfig(
</span></span><span style=display:flex><span>    r<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>,
</span></span><span style=display:flex><span>    lora_alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>,
</span></span><span style=display:flex><span>    lora_dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0.05</span>,
</span></span><span style=display:flex><span>    bias<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;none&#34;</span>,
</span></span><span style=display:flex><span>    task_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;CAUSAL_LM&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#75715e># Target specific layers for LoRA adaptation</span>
</span></span><span style=display:flex><span>    target_modules<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;up_proj&#39;</span>, <span style=color:#e6db74>&#39;down_proj&#39;</span>, <span style=color:#e6db74>&#39;gate_proj&#39;</span>, <span style=color:#e6db74>&#39;k_proj&#39;</span>, <span style=color:#e6db74>&#39;q_proj&#39;</span>, <span style=color:#e6db74>&#39;v_proj&#39;</span>, <span style=color:#e6db74>&#39;o_proj&#39;</span>]
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>The dataset was formatted into the required chat structure using the Hugging Face <code>transformers</code> library:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># From train.py - Formatting data for chat model</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>format_chat_template</span>(example):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Structure the data into user/assistant turns</span>
</span></span><span style=display:flex><span>    convos <span style=color:#f92672>=</span> [{
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;role&#39;</span>: <span style=color:#e6db74>&#39;user&#39;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;content&#39;</span>: example[<span style=color:#e6db74>&#39;instruction&#39;</span>]
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;role&#39;</span>: <span style=color:#e6db74>&#39;assistant&#39;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;content&#39;</span>: example[<span style=color:#e6db74>&#39;response&#39;</span>]
</span></span><span style=display:flex><span>    }]
</span></span><span style=display:flex><span>    <span style=color:#75715e># Apply the tokenizer&#39;s chat template</span>
</span></span><span style=display:flex><span>    texts <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>apply_chat_template(convos, tokenize<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, add_generation_prompt<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> { <span style=color:#e6db74>&#39;text&#39;</span> : texts }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dataset <span style=color:#f92672>=</span> dataset<span style=color:#f92672>.</span>map(format_chat_template)
</span></span></code></pre></div><p>The model was trained using the <code>SFTTrainer</code> from the <code>trl</code> library with the following arguments over 75 steps:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># From train.py - SFTTrainer setup</span>
</span></span><span style=display:flex><span>trainer <span style=color:#f92672>=</span> SFTTrainer(
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> model,
</span></span><span style=display:flex><span>    tokenizer <span style=color:#f92672>=</span> tokenizer,
</span></span><span style=display:flex><span>    train_dataset <span style=color:#f92672>=</span> dataset[<span style=color:#e6db74>&#39;train&#39;</span>],
</span></span><span style=display:flex><span>    eval_dataset <span style=color:#f92672>=</span> dataset[<span style=color:#e6db74>&#39;test&#39;</span>],
</span></span><span style=display:flex><span>    dataset_text_field <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;text&#39;</span>, <span style=color:#75715e># Field containing formatted chat data</span>
</span></span><span style=display:flex><span>    max_seq_length <span style=color:#f92672>=</span> <span style=color:#ae81ff>2048</span>,
</span></span><span style=display:flex><span>    dataset_num_proc <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>    packing <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>, <span style=color:#75715e># Important for chat format</span>
</span></span><span style=display:flex><span>    peft_config<span style=color:#f92672>=</span>peft_config,
</span></span><span style=display:flex><span>    args <span style=color:#f92672>=</span> TrainingArguments(
</span></span><span style=display:flex><span>        per_device_train_batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>        gradient_accumulation_steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>        warmup_steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>,
</span></span><span style=display:flex><span>        max_steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>75</span>, <span style=color:#75715e># Relatively short fine-tuning run</span>
</span></span><span style=display:flex><span>        learning_rate <span style=color:#f92672>=</span> <span style=color:#ae81ff>2e-4</span>,
</span></span><span style=display:flex><span>        fp16 <span style=color:#f92672>=</span> <span style=color:#f92672>not</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_bf16_supported(),
</span></span><span style=display:flex><span>        bf16 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_bf16_supported(),
</span></span><span style=display:flex><span>        logging_steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        optim <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;adamw_8bit&#39;</span>,
</span></span><span style=display:flex><span>        weight_decay <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.01</span>,
</span></span><span style=display:flex><span>        lr_scheduler_type <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;linear&#39;</span>,
</span></span><span style=display:flex><span>        output_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;outputs&#39;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>trainer<span style=color:#f92672>.</span>train()
</span></span><span style=display:flex><span>trainer<span style=color:#f92672>.</span>save_model(new_model)
</span></span></code></pre></div><h2 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h2><p>I evaluated the model&rsquo;s performance using a subset of the dataset reserved for testing. Both the base and fine-tuned models were assessed on their ability to answer questions from the card interactions and rules categories, which require deeper comprehension.</p><p>First, the model generated an answer for a given instruction:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># From evaluate.py - Generating model response</span>
</span></span><span style=display:flex><span>prompt <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>apply_chat_template([{
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;role&#39;</span>: <span style=color:#e6db74>&#39;user&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;content&#39;</span>: example[<span style=color:#e6db74>&#39;instruction&#39;</span>]
</span></span><span style=display:flex><span>}], tokenize<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, add_generation_prompt<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model_inputs <span style=color:#f92672>=</span> tokenizer(prompt, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>, add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>to(<span style=color:#e6db74>&#39;cuda&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>generated_ids <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>generate(<span style=color:#f92672>**</span>model_inputs, max_new_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>, do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, pad_token_id<span style=color:#f92672>=</span>tokenizer<span style=color:#f92672>.</span>eos_token_id, eos_token_id<span style=color:#f92672>=</span>[tokenizer<span style=color:#f92672>.</span>eos_token_id, tokenizer<span style=color:#f92672>.</span>convert_tokens_to_ids(<span style=color:#e6db74>&#34;&lt;|eot_id|&gt;&#34;</span>)])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>decoded <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>decode(generated_ids[<span style=color:#ae81ff>0</span>], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>student_answer <span style=color:#f92672>=</span> decoded<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;assistant&#34;</span>)[<span style=color:#ae81ff>1</span>] <span style=color:#75715e># Extract the generated answer</span>
</span></span></code></pre></div><p>Then, I used GPT-4 to score the answers on a scale of 1-5, providing consistent and efficient evaluations. GPT-4 was prompted with the original question, the ground-truth answer, and the model&rsquo;s generated answer:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># From evaluate.py - Payload for GPT-4 evaluation</span>
</span></span><span style=display:flex><span>payload <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;model&#34;</span>: <span style=color:#e6db74>&#34;gpt-4-turbo&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;response_format&#34;</span>: { <span style=color:#e6db74>&#34;type&#34;</span>: <span style=color:#e6db74>&#39;json_object&#39;</span> },
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;messages&#34;</span>: [
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;content&#34;</span>: [
</span></span><span style=display:flex><span>                {
</span></span><span style=display:flex><span>                    <span style=color:#e6db74>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;text&#34;</span>,
</span></span><span style=display:flex><span>                    <span style=color:#75715e># The prompt instructing GPT-4 how to grade</span>
</span></span><span style=display:flex><span>                    <span style=color:#e6db74>&#34;text&#34;</span>: <span style=color:#e6db74>&#34;You are an expert grader who will be given a Magic: the Gathering related question and correct answer, along with a students response. You rate the student&#39;s response on a scale of 1-5, with 5 being entirely correct and 1 being entirely incorrect. Accurately identify any issues with the answer and explain why the students response is entirely correct, partially correct, or entirely incorrect. Reply in the following JSON format: {</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>explanation</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>: </span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>[EXPLANATION]</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>, </span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>score</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>: SCORE}</span><span style=color:#ae81ff>\n\&#34;</span><span style=color:#e6db74>Question: &#34;</span><span style=color:#f92672>+</span> example[<span style=color:#e6db74>&#39;instruction&#39;</span>] <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Correct answer: &#34;</span><span style=color:#f92672>+</span> example[<span style=color:#e6db74>&#39;response&#39;</span>] <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Student&#39;s answer: &#34;</span> <span style=color:#f92672>+</span> student_answer
</span></span><span style=display:flex><span>                }
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>response <span style=color:#f92672>=</span> json<span style=color:#f92672>.</span>loads(requests<span style=color:#f92672>.</span>post(<span style=color:#e6db74>&#34;https://api.openai.com/v1/chat/completions&#34;</span>, headers<span style=color:#f92672>=</span>headers, json<span style=color:#f92672>=</span>payload)<span style=color:#f92672>.</span>json()[<span style=color:#e6db74>&#39;choices&#39;</span>][<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#39;message&#39;</span>][<span style=color:#e6db74>&#39;content&#39;</span>])
</span></span><span style=display:flex><span>score <span style=color:#f92672>=</span> float(response[<span style=color:#e6db74>&#39;score&#39;</span>])
</span></span></code></pre></div><h2 id=results-and-impact>Results and Impact<a hidden class=anchor aria-hidden=true href=#results-and-impact>#</a></h2><p>The fine-tuned model demonstrated a 10.5% improvement over the base model, with an average score increase from 1.62 to 1.79 based on the GPT-4 evaluation. This indicates a moderate improvement in the model&rsquo;s understanding of MTG rules and interactions, but both models still have tremendous room to learn.</p><h2 id=future-directions>Future Directions<a hidden class=anchor aria-hidden=true href=#future-directions>#</a></h2><p>Future research could explore the addition of custom tokens for special game symbols like mana and tapping, which are underrepresented in pre-training data and maybe not be appropriately tokenized. Additionally, expanding the dataset to include more diverse game scenarios and interactions could further refine the model&rsquo;s capabilities.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>This project showcases the potential of LLMs to enhance the MTG playing experience and the challenges that still need to be overcome to get there. I hope other people will find this dataset useful for training future models and build off my work. There&rsquo;s certainly much more that could be done and I can&rsquo;t wait for the day when AI systems can build good decks with my janky pet cards.</p><h2 id=acknowledgments>Acknowledgments<a hidden class=anchor aria-hidden=true href=#acknowledgments>#</a></h2><p>Thanks to the team at Commander Spellbook for generously sharing their dataset, without which this project would not have been possible. All generated data is unofficial Fan Content permitted under the Fan Content Policy. Not approved/endorsed by Wizards. Portions of the materials used are property of Wizards of the Coast. ©Wizards of the Coast LLC.</p><p>For more details and access to the dataset and model, visit the following links:</p><ul><li><a href=https://huggingface.co/datasets/jakeboggs/MTG-Eval>MTG-Eval Dataset</a></li><li><a href=https://huggingface.co/jakeboggs/MTG-Llama>Fine-Tuned Model</a></li><li><a href=https://github.com/JakeBoggs/Large-Language-Models-for-Magic-the-Gathering>Training Code on GitHub</a></li></ul></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://boggs.tech/>Jake Boggs</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>