<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=author content="Jake Boggs"><title>About me | Jake Boggs</title><link rel=stylesheet href=/css/main.min.css><link rel=icon href=/favicon.png><link rel=canonical href=https://boggs.tech/about/></head><body><header><div class=container><a href=https://boggs.tech/ class=site-title>Jake Boggs</a><nav><a href=/about>About me</a><a href=/startups>Startups</a></nav></div></header><main><div class=container><article><h1>About me</h1><div class=article-content><p>My name is Jake and I have a snake. Currently building environments and benchmarks full-time at <a href=https://endeavorai.com/>Endeavor</a>. Previously studied computer science and math at NC State, before leaving my senior year to move to San Francisco. I post here occassionally to share thoughts and projects.</p><h3 id=research-interests>Research Interests</h3><ul><li><strong>EVALS EVALS EVALS.</strong> Specifically, designing metrics and datasets to measure the ability of models to achieve useful outcomes. As a concrete example, suppose you are training an LSTM to trade a stock given its last 30 days of historical price data. A naive approach would be to minimize the MSE of the next day&rsquo;s closing price and use this prediction to decide whether to go long or short. If you try this, you might get excited as you see the test loss drop sharply. However, attempting to trade with this model is unlikely to be successful due to your target being poorly formulated. Stocks generally move very little from day to day, so you can easily get your error to within a couple percent. You need to optimize for something where an improvement in the loss directly improves your profitability. As another example, at Endeavor we encounter many documents that use handwritten diagrams to convey meaning (like a drawing of a pipe with dimensions on it). There are many correct ways you could extract these diagrams as text (&ldquo;10 ft steel pipe&rdquo; and &ldquo;pipe: 10 feet, material: steel&rdquo; are semantically equivalent), so you must go beyond metrics like word or character error rates when comparing against the ground-truth.</li><li><strong>Data consistency.</strong> Large Language Models are great at modeling. This might seem obvious, as &ldquo;Model&rdquo; is in the name, but I think people forget what this means: LLMs construct an internal representation that is consistent with their training data. If your data is inconsistent with reality, your model will be too. Garbage in, garbage out. This is also very important for safety, because consistency creates implications. I believe this is the culprit behind <a href=https://arxiv.org/abs/2512.09742>&ldquo;weird generalization&rdquo;</a> and <a href=https://arxiv.org/abs/2502.17424>&ldquo;emergent misaligment&rdquo;</a>. These are some of the classic examples where the authors find that fine-tuning on outdated bird names makes the models behave as though they are in the 19th century and fine-tuning on insecure code causes them to act maliciously in other contexts. While unintended, these behaviors are perfectly consistent with the data. A helpful assistant would not write vulnerable code, so the model becomes unaligned. If the assistant was the in 21st century, it would not use 19th century names, so it assumes it is in the 1800s. I think these are great examples of how current architectures are already sufficently generalizable and also how careful we need to be with our training data.</li><li><strong>My current pet project:</strong> an attempt to genetically engineer a real-life <a href=/about#lotus>Black Lotus</a>. This is really just a way for me to teach myself about genetics and how AI can be applied to the space. My initial initial approach combines a couple of genes to boost anthocyanin production in the petals and I&rsquo;m paying a lab to run the experiment. I&rsquo;ll post more about this as results arrive over the summer.</li></ul><h3 id=gallery>Gallery</h3><div style=display:flex;flex-wrap:wrap;justify-content:center;gap:1rem;align-items:flex-start><div style=width:300px;min-width:250px;max-width:90%><img src=/images/jake.png alt=Jake style=width:100%;margin:0;display:block><div style=text-align:center;font-size:.8rem;color:#888;margin-top:.25rem>Jake</div></div><div style=width:300px;min-width:250px;max-width:90%><img src=/images/snake.jpg alt=Snake style=width:100%;margin:0;display:block><div style=text-align:center;font-size:.8rem;color:#888;margin-top:.25rem>Snake</div></div></div><span id=crocs></span><div style=margin-top:2rem>If you see me in the wild, chances are that I'll be rocking these:<div style=display:flex;flex-wrap:wrap;justify-content:center;gap:1rem;align-items:flex-start><img src=/images/crocs.jpg alt=Crocs style=max-width:min(500px,90%);margin:0;display:block></div></div><span id=lotus></span><div style=margin-top:2rem>I'm also skilled at Magic: the Gathering üòè<div style=display:flex;flex-wrap:wrap;justify-content:center;gap:1rem;align-items:flex-start><img src=/images/lotus.jpg alt="Black Lotus" style=max-width:min(400px,90%);margin:0;display:block></div></div></div></article></div></main><footer><div class=container><p>&copy; 2026 Jake Boggs</p></div></footer></body></html>