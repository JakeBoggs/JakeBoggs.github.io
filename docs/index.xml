<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jake Boggs</title>
    <link>https://boggs.tech/</link>
    <description>Recent content on Jake Boggs</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Mon, 19 Jan 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://boggs.tech/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Translating Historical Manuscripts with Gemini 3</title>
      <link>https://boggs.tech/posts/translating-manuscripts-with-gemini/</link>
      <pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://boggs.tech/posts/translating-manuscripts-with-gemini/</guid>
      <description>An app for transcribing and translating historical manuscripts using Gemini 3 and GPT-5.2. Built as an early Christmas present for my girlfriend.</description>
    </item>
    <item>
      <title>Scalable Reinforcement Learning with LLMs - Atropos Guide</title>
      <link>https://boggs.tech/posts/atropos-guide-reinforcement-learning-with-llms/</link>
      <pubDate>Fri, 16 May 2025 00:00:00 +0000</pubDate>
      <guid>https://boggs.tech/posts/atropos-guide-reinforcement-learning-with-llms/</guid>
      <description>Quick intro to Atropos, the RL framework from Nous Research. Written while prepping for their hackathon, where I got 2nd place with an environment for teaching LLMs how to write jokes.</description>
    </item>
    <item>
      <title>Evaluating Reasoning in LLMs Through MTG Deck Building</title>
      <link>https://boggs.tech/posts/evaluating-llm-reasoning-with-mtg-deck-building/</link>
      <pubDate>Fri, 09 May 2025 00:00:00 +0000</pubDate>
      <guid>https://boggs.tech/posts/evaluating-llm-reasoning-with-mtg-deck-building/</guid>
      <description>ManaBench is my benchmark for testing LLM reasoning using Magic: The Gathering deck building. Models are tasked with picking the best card from 6 options to complete a deck.</description>
    </item>
    <item>
      <title>Introducing Manamorphosis: A Diffusion Model for MTG Deck Generation</title>
      <link>https://boggs.tech/posts/manamorphosis/</link>
      <pubDate>Mon, 05 May 2025 00:00:00 +0000</pubDate>
      <guid>https://boggs.tech/posts/manamorphosis/</guid>
      <description>I trained a diffusion model to complete Magic: The Gathering decklists. Give it some cards, it fills in the rest. Trained on 47k tournament decks.</description>
    </item>
    <item>
      <title>Gait Analysis for Physical Therapy with YOLOv11</title>
      <link>https://boggs.tech/posts/gait-analysis-physical-therapy-research/</link>
      <pubDate>Mon, 21 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://boggs.tech/posts/gait-analysis-physical-therapy-research/</guid>
      <description>Built a video analysis tool for my mom&amp;rsquo;s physical therapy research. Uses YOLOv11-pose for automatic joint detection with a drag-to-correct interface.</description>
    </item>
    <item>
      <title>AccountaBuddy: An AI Accountability Partner - HackNC 2024</title>
      <link>https://boggs.tech/posts/accountabuddy-hacknc-2024/</link>
      <pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://boggs.tech/posts/accountabuddy-hacknc-2024/</guid>
      <description>Weekend hackathon project that calls your phone to check on your tasks.</description>
    </item>
    <item>
      <title>Daily Yap: A Synthetically Generated Conversational Audio Dataset</title>
      <link>https://boggs.tech/posts/synthetic-conversational-audio-generation-daily-yap/</link>
      <pubDate>Sun, 23 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://boggs.tech/posts/synthetic-conversational-audio-generation-daily-yap/</guid>
      <description>90-hour conversational audio dataset. Used GPT-4o to clean up Daily Dialog transcripts and XTTS v2 to synthesize speech with 8 different voices. Available on HuggingFace.</description>
    </item>
    <item>
      <title>Large Language Models for Magic: the Gathering</title>
      <link>https://boggs.tech/posts/large-language-models-for-magic-the-gathering/</link>
      <pubDate>Fri, 24 May 2024 00:00:00 +0000</pubDate>
      <guid>https://boggs.tech/posts/large-language-models-for-magic-the-gathering/</guid>
      <description>One of my early attempts at teaching LLMs to understand MTG. Created a dataset of 80k question-answer pairs covering card text, rulings, and combos, then fine-tuned Llama 3 8B with QLoRA.</description>
    </item>
    <item>
      <title>About me</title>
      <link>https://boggs.tech/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://boggs.tech/about/</guid>
      <description>&lt;p&gt;My name is Jake and I have a snake. Currently building environments and benchmarks full-time at &lt;a href=&#34;https://endeavorai.com/&#34;&gt;Endeavor&lt;/a&gt;. Previously studied computer science and math at NC State, before leaving my senior year to move to San Francisco. I post here occassionally to share thoughts and projects.&lt;/p&gt;&#xA;&lt;h3 id=&#34;research-interests&#34;&gt;Research Interests&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Training objectives &amp;amp; evals.&lt;/strong&gt; Specifically, designing metrics and datasets to measure the ability of models to achieve outcomes. To give a concrete example, suppose you are training an LSTM to trade a stock given its last 30 days of historical price data. A naive method would be to minimize the MSE of the next day&amp;rsquo;s closing price and use this prediction to decide whether to go long or short. If you try this, you might get excited as you see the test loss drop sharply. However, attempting to trade with this model is unlikely to be successful due to your objective being poorly formulated. Stocks typically move little from day to day, so you can easily reduce your error to within a couple percent, but this isn&amp;rsquo;t enough to gain a meaningful signal. You need to optimize something with a stronger correlation to your profitability. As another example, in my current role we encounter many documents that use handwritten diagrams or annotations to convey meaning (like a drawing of a pipe with dimensions on it). There are many correct ways you could interpret these diagrams as text (&amp;ldquo;10 ft steel pipe&amp;rdquo; and &amp;ldquo;pipe: 10 feet, material: steel&amp;rdquo; are semantically equivalent), so a good eval must go beyond metrics like word or character error rates.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Data consistency.&lt;/strong&gt; Large Language Models are great at modeling. This might seem obvious, as &amp;ldquo;Model&amp;rdquo; is in the name, but I think people forget what this means: LLMs construct internal representations that are consistent with their training data. If your data is inconsistent with reality, your model&amp;rsquo;s representation of reality will be too. Garbage in, garbage out. Reality is important, but not the only aspect of consistency you should consider. In order to reason about new data points, models must make assumptions and they are extremely sensitive to subtle implications in their training set. Some of the classic examples are &lt;a href=&#34;https://arxiv.org/abs/2512.09742&#34;&gt;&amp;ldquo;weird generalization&amp;rdquo;&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2502.17424&#34;&gt;&amp;ldquo;emergent misaligment&amp;rdquo;&lt;/a&gt;, where the authors find that fine-tuning on outdated bird names makes the models behave as though they are in the 19th century or fine-tuning on insecure code causes them to act maliciously in other contexts. While unintended, these behaviors are consistent with the data. A helpful assistant would not write vulnerable code, so the model becomes unaligned. If the assistant was the in 21st century, it would not use 19th century names, so it assumes it is in the 1800s. I think this shows how current architectures are already sufficently generalizable and also how careful we need to be to ensure that our data is consistent with the behaviors we desire.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;My current pet project:&lt;/strong&gt; genetically engineering a real-life &lt;a href=&#34;https://boggs.tech/about#lotus&#34;&gt;Black Lotus&lt;/a&gt;. This is really just a way for me to teach myself about genetics and how AI can be applied to the space. My initial approach combines a couple of genes to boost anthocyanin production in the petals and I&amp;rsquo;m paying a lab to carry out the experiment. I&amp;rsquo;ll post more about this as results arrive over the summer.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;gallery&#34;&gt;Gallery&lt;/h3&gt;&#xA;&lt;div style=&#34;display: flex; flex-wrap: wrap; justify-content: center; gap: 1rem; align-items: flex-start;&#34;&gt;&#xD;&#xA;    &lt;div style=&#34;width: 300px; min-width: 250px; max-width: 90%;&#34;&gt;&#xD;&#xA;        &lt;img src=&#34;https://boggs.tech/images/jake.png&#34; alt=&#34;Jake&#34; style=&#34;width: 100%; margin: 0; display: block;&#34;&gt;&#xD;&#xA;        &lt;div style=&#34;text-align: center; font-size: 0.8rem; color: #888; margin-top: 0.25rem;&#34;&gt;Jake&lt;/div&gt;&#xD;&#xA;    &lt;/div&gt;&#xD;&#xA;    &lt;div style=&#34;width: 300px; min-width: 250px; max-width: 90%;&#34;&gt;&#xD;&#xA;        &lt;img src=&#34;https://boggs.tech/images/snake.jpg&#34; alt=&#34;Snake&#34; style=&#34;width: 100%; margin: 0; display: block;&#34;&gt;&#xD;&#xA;        &lt;div style=&#34;text-align: center; font-size: 0.8rem; color: #888; margin-top: 0.25rem;&#34;&gt;Snake&lt;/div&gt;&#xD;&#xA;    &lt;/div&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;span id=&#34;crocs&#34;&gt;&lt;/span&gt;&#xD;&#xA;&lt;div style=&#34;margin-top: 2rem;&#34;&gt;&#xD;&#xA;    If you see me in the wild, chances are that I&#39;ll be rocking these:&#xD;&#xA;    &lt;div style=&#34;display: flex; flex-wrap: wrap; justify-content: center; gap: 1rem; align-items: flex-start;&#34;&gt;&#xD;&#xA;        &lt;img src=&#34;https://boggs.tech/images/crocs.jpg&#34; alt=&#34;Crocs&#34; style=&#34;max-width: min(500px, 90%); margin: 0; display: block;&#34;&gt;&#xD;&#xA;    &lt;/div&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;span id=&#34;lotus&#34;&gt;&lt;/span&gt;&#xD;&#xA;&lt;div style=&#34;margin-top: 2rem;&#34;&gt;&#xD;&#xA;    I&#39;m also skilled at Magic: the Gathering üòè&#xD;&#xA;    &lt;div style=&#34;display: flex; flex-wrap: wrap; justify-content: center; gap: 1rem; align-items: flex-start;&#34;&gt;&#xD;&#xA;        &lt;img src=&#34;https://boggs.tech/images/lotus.jpg&#34; alt=&#34;Black Lotus&#34; style=&#34;max-width: min(400px, 90%); margin: 0; display: block;&#34;&gt;&#xD;&#xA;    &lt;/div&gt;&#xD;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Startups</title>
      <link>https://boggs.tech/startups/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://boggs.tech/startups/</guid>
      <description>&lt;p&gt;This page has been up for a while but I recently noticed there wasn&amp;rsquo;t an explanation for why it exists: I&amp;rsquo;ve always been afraid of getting stuck. I never want to look back and realize I haven&amp;rsquo;t progressed in the last month / year / decade or that I could have achieved more. This is partially why I&amp;rsquo;ve been drawn to startups, as &amp;ldquo;If you&amp;rsquo;re not growing, you&amp;rsquo;re dying&amp;rdquo; holds true here more than anywhere. There are many other aspects I enjoy too of course (building things, solving problems, ownership etc are great), but the desire to continually learn has been the biggest driver. Couple that with a strong belief following conventional wisdom produces conventional outcomes, and I was motivated to drop out of college with a semester left to join a company I believe is going to win.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
